{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba8867ab-aab3-4f92-886e-6c26edfb5a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[텍스트 블럭] 과제 평가기준 대응 요약\n",
    "\n",
    "1) \"Transformer(Encoder-Decoder) → GPT(Decoder-only)\" 변경사항\n",
    "- 인코더 삭제: 질문(Q)과 답변(A)을 하나의 시퀀스로 합쳐 한 개의 디코더 스택만 사용합니다.\n",
    "- 자기회귀(causal) 마스킹: 미래 토큰을 가리는 상삼각 마스크를 전 블록에 적용합니다.\n",
    "- 포지션 인코딩: GPT 논문 방식에 맞춰 \"학습 가능한 위치 임베딩\"으로 교체했습니다.\n",
    "- 입력 포맷: [BOS] + Q + [SEP] + A + [EOS] 로 단일 시퀀스 구성, 다음 토큰 예측으로 학습.\n",
    "- 손실 계산: 기본은 전체 토큰에 대해 LM loss(다음 토큰 예측)를 적용하되,\n",
    "LOSS_ON_ANSWER_ONLY=True으로 Q 구간 손실을 무시하고 A 구간만 학습하도록 선택 가능하게 했습니다.\n",
    "\n",
    "2) 전처리 및 데이터 변형\n",
    "- 기존 전처리(normalize_text) 유지: 학습/추론 동일 규칙을 강제합니다.\n",
    "- SentencePiece 공통 어휘: Q/A를 합친 코퍼스로 학습. GPT 입력을 위해 [SEP] 심볼을 추가했습니다.\n",
    "- Dataset: (input_ids, labels) 한 쌍을 반환합니다. labels는 input_ids를 1칸 오른쪽으로 민 형태이며,\n",
    "패딩 Q+SEP 구간은 IGNORE_INDEX(-100)로 채워 손실에서 제외합니다.\n",
    "\n",
    "3) GPT 입력 블럭\n",
    "- TokenEmbedding + PositionEmbedding + Dropout →\n",
    "[Decoder 블록 × L]: (Pre-LN) LayerNorm → Masked MHA → Residual → LayerNorm → MLP(GELU) → Residual →\n",
    "Final LayerNorm → Vocab Linear\n",
    "- 위치 정보는 각 시퀀스 길이만큼 0..L-1 인덱스를 부여해 학습 가능한 임베딩으로 더합니다.\n",
    "\n",
    "4) 모델 구성/확인 (summary, fit)\n",
    "- torchinfo가 있으면 summary를 출력합니다(없으면 파라미터 수/텐서 크기만 출력).\n",
    "- 학습 루프(run_epoch)를 제공하여 model.fit에 준하는 에폭별 loss/ppl을 출력합니다.\n",
    "- 체크포인트(best valid loss) 저장/로딩 로직 포함.\n",
    "\n",
    "5) 추론 및 동작 확인\n",
    "- 입력: \"[BOS] + Q + [SEP]\"를 넣고, EOS 또는 최대 길이까지 오토리그레시브로 생성합니다.\n",
    "- 출력: [SEP] 이후부터 EOS 전까지의 토큰을 디코드하여 답변 문자열로 반환합니다.\n",
    "\"\"\"\n",
    "\n",
    "!pip install sentencepiece torch torchvision scikit-learn pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c547edd-4822-4c5d-b478-0f4fcfdcfb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 1) 패키지 임포트\n",
    "# ========================\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import math\n",
    "import unicodedata\n",
    "import random\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6000d7f7-2412-4969-8105-7076b9559361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# 2) 재현성(시드 고정)\n",
    "# ========================\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bde0f16-f9e9-4a4a-92dc-cfe715e933f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로딩 중...\n",
      "              Q            A  label\n",
      "0        12시 땡!   하루가 또 가네요.      0\n",
      "1   1지망 학교 떨어졌어    위로해 드립니다.      0\n",
      "2  3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "원본 샘플 수: 11823\n",
      "전처리 후 샘플 수: 11750\n",
      "train: 11162 | valid: 588\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# 3) 데이터 로드 & 전처리\n",
    "# ========================\n",
    "URL = \"https://github.com/songys/Chatbot_data/raw/master/ChatbotData.csv\"\n",
    "print(\"데이터 로딩 중...\")\n",
    "df = pd.read_csv(URL)\n",
    "print(df.head(3))\n",
    "print(\"원본 샘플 수:\", len(df))\n",
    "\n",
    "# 정규화: 학습/추론 동일 규칙 유지(중요)\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s))\n",
    "    s = re.sub(r\"[^0-9A-Za-z가-힣ㄱ-ㅎㅏ-ㅣ\\s.,?!~’'\\\"()\\-\\:;@/]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# 결측/중복/길이 필터링\n",
    "MIN_CHARS, MAX_CHARS = 1, 128\n",
    "df = df.dropna(subset=[\"Q\", \"A\"]).copy()\n",
    "df[\"Q\"] = df[\"Q\"].astype(str).map(normalize_text)\n",
    "df[\"A\"] = df[\"A\"].astype(str).map(normalize_text)\n",
    "\n",
    "mask = (\n",
    "    df[\"Q\"].str.len().between(MIN_CHARS, MAX_CHARS) &\n",
    "    df[\"A\"].str.len().between(MIN_CHARS, MAX_CHARS)\n",
    ")\n",
    "df = df[mask].drop_duplicates(subset=[\"Q\", \"A\"]).reset_index(drop=True)\n",
    "print(\"전처리 후 샘플 수:\", len(df))\n",
    "\n",
    "train_df, valid_df = train_test_split(\n",
    "    df[[\"Q\", \"A\"]],\n",
    "    test_size=0.05,\n",
    "    random_state=SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "print(f\"train: {len(train_df)} | valid: {len(valid_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "984cd74f-6ebc-4998-abc7-7fb2cd52d79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 SentencePiece 모델 발견 → 재사용.\n",
      "Vocab size: 8000 | PAD/BOS/EOS/SEP: 0 2 3 4\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# 4) SentencePiece 학습(+ [SEP])\n",
    "# ========================\n",
    "ARTIFACTS = Path(\"artifacts\")\n",
    "ARTIFACTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "corpus_path = ARTIFACTS / \"spm_corpus.txt\"\n",
    "with io.open(corpus_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for s in pd.concat([train_df[\"Q\"], train_df[\"A\"]], axis=0).astype(str):\n",
    "        if s:\n",
    "            f.write(s + \"\\n\")\n",
    "\n",
    "spm_model_prefix = str(ARTIFACTS / \"spm_ko_gpt\")\n",
    "VOCAB_SIZE = 8000\n",
    "\n",
    "need_train_spm = True\n",
    "if Path(spm_model_prefix + \".model\").exists():\n",
    "    # 이미 있으면 [SEP] 존재 여부 체크 → 없으면 재학습\n",
    "    sp_tmp = spm.SentencePieceProcessor()\n",
    "    sp_tmp.load(spm_model_prefix + \".model\")\n",
    "    need_train_spm = (sp_tmp.piece_to_id(\"[SEP]\") == -1)\n",
    "\n",
    "if need_train_spm:\n",
    "    print(\"SentencePiece 학습 시작([SEP] 추가 포함)...\")\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        input=str(corpus_path),\n",
    "        model_prefix=spm_model_prefix,\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        model_type=\"unigram\",\n",
    "        character_coverage=0.9995,\n",
    "        pad_id=0, unk_id=1, bos_id=2, eos_id=3,\n",
    "        pad_piece=\"[PAD]\", unk_piece=\"[UNK]\", bos_piece=\"<s>\", eos_piece=\"</s>\",\n",
    "        user_defined_symbols=[\"[SEP]\"], # ★ 추가: GPT 입력을 위한 구분자\n",
    "        input_sentence_size=2000000,\n",
    "        shuffle_input_sentence=True,\n",
    "        train_extremely_large_corpus=False\n",
    "    )\n",
    "    print(\"SentencePiece 학습 완료.\")\n",
    "else:\n",
    "    print(\"기존 SentencePiece 모델 발견 → 재사용.\")\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(spm_model_prefix + \".model\")\n",
    "\n",
    "PAD_ID = sp.pad_id()   # 0\n",
    "UNK_ID = sp.unk_id()   # 1\n",
    "BOS_ID = sp.bos_id()   # 2\n",
    "EOS_ID = sp.eos_id()   # 3\n",
    "VOCAB  = sp.get_piece_size()\n",
    "\n",
    "SEP_ID = sp.piece_to_id(\"[SEP]\")\n",
    "if SEP_ID == -1:\n",
    "    raise ValueError(\"SentencePiece vocab에 [SEP]이 없습니다. user_defined_symbols=['[SEP]']로 재학습 필요.\")\n",
    "\n",
    "print(\"Vocab size:\", VOCAB, \"| PAD/BOS/EOS/SEP:\", PAD_ID, BOS_ID, EOS_ID, SEP_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b7efd34-41d4-4a7d-8007-4c60103878ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 36])\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# 5) Dataset / DataLoader (Decoder-only용)\n",
    "# ========================\n",
    "MAX_TOKENS = 128\n",
    "IGNORE_INDEX = -100\n",
    "LOSS_ON_ANSWER_ONLY = True  # Q+SEP 구간 손실 무시\n",
    "\n",
    "def encode_text(s: str, spm_processor, max_len=MAX_TOKENS):\n",
    "    ids = spm_processor.encode(s, out_type=int)\n",
    "    return ids[: max_len - 2]\n",
    "\n",
    "class GPTChatDataset(Dataset):\n",
    "    \"\"\"Q, A → [BOS] Q [SEP] A [EOS] 단일 시퀀스\n",
    "    input_ids = seq[:-1], labels = seq[1:]\n",
    "    (옵션) Q+SEP 구간은 IGNORE_INDEX로 손실 제외\n",
    "    \"\"\"\n",
    "    def __init__(self, frame: pd.DataFrame, spm_processor, max_len=MAX_TOKENS):\n",
    "        self.q = frame[\"Q\"].tolist()\n",
    "        self.a = frame[\"A\"].tolist()\n",
    "        self.sp = spm_processor\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.q)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        q_ids = encode_text(self.q[i], self.sp, self.max_len)\n",
    "        a_ids = encode_text(self.a[i], self.sp, self.max_len)\n",
    "\n",
    "        seq = [BOS_ID] + q_ids + [SEP_ID] + a_ids + [EOS_ID]\n",
    "        seq = seq[: max(2, self.max_len)]  # 최소 2토큰 유지\n",
    "\n",
    "        input_ids = seq[:-1]\n",
    "        labels    = seq[1:]\n",
    "\n",
    "        if LOSS_ON_ANSWER_ONLY and (SEP_ID in seq):\n",
    "            sep_pos = seq.index(SEP_ID)\n",
    "            for t in range(0, min(sep_pos, len(labels))):  # ← 여기서 끝! (a₁은 학습에 포함)\n",
    "                labels[t] = IGNORE_INDEX\n",
    "\n",
    "\n",
    "        return (\n",
    "            torch.tensor(input_ids, dtype=torch.long),\n",
    "            torch.tensor(labels,    dtype=torch.long),\n",
    "        )\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_seqs, label_seqs = zip(*batch)\n",
    "    maxlen = max(x.size(0) for x in input_seqs)\n",
    "\n",
    "    inputs = torch.full((len(batch), maxlen), PAD_ID,       dtype=torch.long)\n",
    "    labels = torch.full((len(batch), maxlen), IGNORE_INDEX, dtype=torch.long)\n",
    "\n",
    "    for i, (inp, lab) in enumerate(zip(input_seqs, label_seqs)):\n",
    "        L = inp.size(0)\n",
    "        inputs[i, :L] = inp\n",
    "        labels[i, :L] = lab\n",
    "    return inputs, labels  # (B, T)\n",
    "\n",
    "train_ds = GPTChatDataset(train_df, sp, MAX_TOKENS)\n",
    "valid_ds = GPTChatDataset(valid_df, sp, MAX_TOKENS)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=128, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "print(next(iter(train_loader))[0].shape)  # (B, T) 확인용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d31de7-e775-4aa4-896d-afd349ef8443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 준비 완료\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# 6) GPT 블록/모델 (Decoder-only)\n",
    "# ========================\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int\n",
    "    max_seq_len: int = 128\n",
    "    d_model: int = 256\n",
    "    n_heads: int = 8\n",
    "    n_layers: int = 6\n",
    "    attn_pdrop: float = 0.1\n",
    "    resid_pdrop: float = 0.1\n",
    "    emb_pdrop: float = 0.1\n",
    "    mlp_ratio: int = 4\n",
    "\n",
    "class GPTBlock(nn.Module):\n",
    "    \"\"\"Pre-LN: LN → Masked MHA → Residual, LN → MLP(GELU) → Residual\"\"\"\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.d_model)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=config.d_model,\n",
    "            num_heads=config.n_heads,\n",
    "            dropout=config.attn_pdrop,\n",
    "            batch_first=False,\n",
    "        )\n",
    "        self.drop1 = nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(config.d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.d_model, config.mlp_ratio * config.d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.mlp_ratio * config.d_model, config.d_model),\n",
    "            nn.Dropout(config.resid_pdrop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, attn_mask, key_padding_mask):\n",
    "        # x: (T, B, C)\n",
    "        x_norm = self.ln1(x)\n",
    "        attn_out, _ = self.attn(\n",
    "            x_norm, x_norm, x_norm,\n",
    "            attn_mask=attn_mask,              # (T, T) True=mask\n",
    "            key_padding_mask=key_padding_mask,# (B, T) True=ignore\n",
    "            need_weights=False,\n",
    "        )\n",
    "        x = x + self.drop1(attn_out)\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.d_model, padding_idx=PAD_ID)\n",
    "        self.pos_emb = nn.Embedding(config.max_seq_len, config.d_model)\n",
    "        self.drop = nn.Dropout(config.emb_pdrop)\n",
    "\n",
    "        self.blocks = nn.ModuleList([GPTBlock(config) for _ in range(config.n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(config.d_model)\n",
    "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        # weight tying\n",
    "        self.head.weight = self.tok_emb.weight\n",
    "\n",
    "    def forward(self, input_ids, attn_mask=None, key_padding_mask=None):\n",
    "        # input_ids: (B, T)\n",
    "        B, T = input_ids.shape\n",
    "        if T > self.config.max_seq_len:\n",
    "            raise ValueError(f\"시퀀스 길이 {T} > max_seq_len {self.config.max_seq_len}\")\n",
    "\n",
    "        pos = torch.arange(0, T, device=input_ids.device).unsqueeze(0)  # (1, T)\n",
    "        x = self.tok_emb(input_ids) + self.pos_emb(pos)                 # (B, T, C)\n",
    "        x = self.drop(x)\n",
    "        x = x.transpose(0, 1).contiguous()                              # (T, B, C)\n",
    "\n",
    "        if attn_mask is None:\n",
    "            attn_mask = torch.triu(torch.ones(T, T, device=input_ids.device, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, attn_mask=attn_mask, key_padding_mask=key_padding_mask)\n",
    "\n",
    "        x = self.ln_f(x)                                                # (T, B, C)\n",
    "        logits = self.head(x)                                           # (T, B, V)\n",
    "        return logits\n",
    "\n",
    "config = GPTConfig(vocab_size=VOCAB, max_seq_len=MAX_TOKENS, d_model=256, n_heads=8, n_layers=6)\n",
    "model = GPTLanguageModel(config).to(DEVICE)\n",
    "print(\"모델 준비 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6cfcd9d-6a68-4181-8764-aca05057cc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchinfo 요약 실패: No module named 'torchinfo'\n",
      "모델 파라미터: total=6,819,840 trainable=6,819,840\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# 7) summary\n",
    "# ========================\n",
    "\n",
    "def print_model_summary():\n",
    "    try:\n",
    "        from torchinfo import summary\n",
    "        summary(model, input_size=(2, 32), dtypes=[torch.long], device=DEVICE)\n",
    "    except Exception as e:\n",
    "        n_params = sum(p.numel() for p in model.parameters())\n",
    "        n_train = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(\"torchinfo 요약 실패:\", e)\n",
    "        print(f\"모델 파라미터: total={n_params:,} trainable={n_train:,}\")\n",
    "\n",
    "print_model_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f223f567-f52e-4b9c-9a18-d6a4da906d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 8) 학습 유틸 (fit)\n",
    "# ========================\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX, label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8, weight_decay=0.01)\n",
    "\n",
    "def to_masks(inputs: torch.Tensor):\n",
    "    # inputs: (B, T)\n",
    "    B, T = inputs.shape\n",
    "    attn_mask = torch.triu(torch.ones(T, T, device=inputs.device, dtype=torch.bool), diagonal=1)\n",
    "    key_padding_mask = (inputs == PAD_ID)  # (B, T) True=PAD\n",
    "    return attn_mask, key_padding_mask\n",
    "\n",
    "def run_epoch(dataloader, train=True):\n",
    "    model.train(train)\n",
    "    total_loss, total_tokens = 0.0, 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(DEVICE)   # (B, T)\n",
    "        labels = labels.to(DEVICE)   # (B, T)\n",
    "\n",
    "        attn_mask, key_padding_mask = to_masks(inputs)\n",
    "        logits = model(inputs, attn_mask=attn_mask, key_padding_mask=key_padding_mask)  # (T, B, V)\n",
    "        \n",
    "\n",
    "        loss = criterion(\n",
    "            logits.view(-1, logits.size(-1)),          # (T*B, V)\n",
    "            labels.transpose(0, 1).reshape(-1)         # (T*B,)\n",
    "        )\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        valid_tokens = (labels != IGNORE_INDEX).sum().item()\n",
    "        total_loss += loss.item() * max(1, valid_tokens)\n",
    "        total_tokens += max(1, valid_tokens)\n",
    "\n",
    "    avg_loss = total_loss / max(1, total_tokens)\n",
    "    ppl = math.exp(avg_loss)\n",
    "    return avg_loss, ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32928f6b-f4e8-4c97-9b98-cddb4a56c206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 시작...\n",
      "[01] train loss 31.6494 | ppl 55610144822849.61  ||  valid loss 18.6051 | ppl 120249150.64\n",
      "[02] train loss 16.8403 | ppl 20589909.41  ||  valid loss 13.6607 | ppl 856598.44\n",
      "[03] train loss 12.8820 | ppl 393177.58  ||  valid loss 10.6610 | ppl 42657.99\n",
      "[04] train loss 10.3937 | ppl 32651.65  ||  valid loss 8.7694 | ppl 6434.59\n",
      "[05] train loss 8.9183 | ppl 7467.10  ||  valid loss 7.7646 | ppl 2355.68\n",
      "[06] train loss 8.0260 | ppl 3059.43  ||  valid loss 7.0995 | ppl 1211.41\n",
      "[07] train loss 7.4323 | ppl 1689.70  ||  valid loss 6.6606 | ppl 781.03\n",
      "[08] train loss 7.0243 | ppl 1123.60  ||  valid loss 6.3638 | ppl 580.44\n",
      "[09] train loss 6.7173 | ppl 826.59  ||  valid loss 6.1395 | ppl 463.81\n",
      "[10] train loss 6.4923 | ppl 660.04  ||  valid loss 5.9664 | ppl 390.11\n",
      "[11] train loss 6.3075 | ppl 548.69  ||  valid loss 5.8416 | ppl 344.34\n",
      "[12] train loss 6.1543 | ppl 470.74  ||  valid loss 5.7553 | ppl 315.86\n",
      "[13] train loss 6.0318 | ppl 416.44  ||  valid loss 5.6892 | ppl 295.66\n",
      "[14] train loss 5.9207 | ppl 372.69  ||  valid loss 5.6040 | ppl 271.50\n",
      "[15] train loss 5.8176 | ppl 336.18  ||  valid loss 5.5791 | ppl 264.83\n",
      "[16] train loss 5.7153 | ppl 303.49  ||  valid loss 5.5368 | ppl 253.87\n",
      "[17] train loss 5.6384 | ppl 281.03  ||  valid loss 5.4609 | ppl 235.30\n",
      "[18] train loss 5.5537 | ppl 258.19  ||  valid loss 5.4483 | ppl 232.37\n",
      "[19] train loss 5.4657 | ppl 236.43  ||  valid loss 5.4110 | ppl 223.86\n",
      "[20] train loss 5.3879 | ppl 218.75  ||  valid loss 5.3953 | ppl 220.38\n",
      "[21] train loss 5.3070 | ppl 201.73  ||  valid loss 5.3842 | ppl 217.94\n",
      "[22] train loss 5.2246 | ppl 185.79  ||  valid loss 5.3375 | ppl 207.99\n",
      "[23] train loss 5.1509 | ppl 172.58  ||  valid loss 5.3330 | ppl 207.06\n",
      "[24] train loss 5.0694 | ppl 159.08  ||  valid loss 5.2910 | ppl 198.54\n",
      "[25] train loss 5.0014 | ppl 148.62  ||  valid loss 5.3062 | ppl 201.59\n",
      "[26] train loss 4.9214 | ppl 137.20  ||  valid loss 5.2935 | ppl 199.04\n",
      "[27] train loss 4.8462 | ppl 127.26  ||  valid loss 5.2500 | ppl 190.57\n",
      "[28] train loss 4.7671 | ppl 117.58  ||  valid loss 5.2134 | ppl 183.72\n",
      "[29] train loss 4.6895 | ppl 108.80  ||  valid loss 5.2169 | ppl 184.36\n",
      "[30] train loss 4.6165 | ppl 101.14  ||  valid loss 5.1951 | ppl 180.39\n",
      "[31] train loss 4.5478 | ppl 94.42  ||  valid loss 5.2026 | ppl 181.75\n",
      "[32] train loss 4.4632 | ppl 86.76  ||  valid loss 5.1660 | ppl 175.22\n",
      "[33] train loss 4.3886 | ppl 80.52  ||  valid loss 5.1851 | ppl 178.59\n",
      "[34] train loss 4.3102 | ppl 74.46  ||  valid loss 5.1531 | ppl 172.97\n",
      "[35] train loss 4.2392 | ppl 69.36  ||  valid loss 5.1876 | ppl 179.04\n",
      "[36] train loss 4.1614 | ppl 64.16  ||  valid loss 5.1800 | ppl 177.68\n",
      "[37] train loss 4.0983 | ppl 60.24  ||  valid loss 5.1262 | ppl 168.37\n",
      "[38] train loss 4.0153 | ppl 55.44  ||  valid loss 5.1429 | ppl 171.21\n",
      "[39] train loss 3.9426 | ppl 51.55  ||  valid loss 5.1164 | ppl 166.73\n",
      "[40] train loss 3.8808 | ppl 48.46  ||  valid loss 5.1226 | ppl 167.77\n",
      "[41] train loss 3.8087 | ppl 45.09  ||  valid loss 5.1423 | ppl 171.11\n",
      "[42] train loss 3.7442 | ppl 42.27  ||  valid loss 5.0886 | ppl 162.16\n",
      "[43] train loss 3.6738 | ppl 39.40  ||  valid loss 5.1353 | ppl 169.91\n",
      "[44] train loss 3.6070 | ppl 36.85  ||  valid loss 5.1247 | ppl 168.13\n",
      "[45] train loss 3.5432 | ppl 34.58  ||  valid loss 5.1309 | ppl 169.18\n",
      "[46] train loss 3.4739 | ppl 32.26  ||  valid loss 5.0558 | ppl 156.93\n",
      "[47] train loss 3.4150 | ppl 30.42  ||  valid loss 5.0834 | ppl 161.32\n",
      "[48] train loss 3.3553 | ppl 28.65  ||  valid loss 5.0968 | ppl 163.50\n",
      "[49] train loss 3.2920 | ppl 26.90  ||  valid loss 5.0845 | ppl 161.49\n",
      "[50] train loss 3.2393 | ppl 25.52  ||  valid loss 5.0878 | ppl 162.04\n",
      "[51] train loss 3.1783 | ppl 24.01  ||  valid loss 5.1019 | ppl 164.34\n",
      "[52] train loss 3.1216 | ppl 22.68  ||  valid loss 5.0665 | ppl 158.63\n",
      "[53] train loss 3.0653 | ppl 21.44  ||  valid loss 5.1483 | ppl 172.13\n",
      "[54] train loss 3.0064 | ppl 20.21  ||  valid loss 5.0729 | ppl 159.64\n",
      "[55] train loss 2.9504 | ppl 19.11  ||  valid loss 5.1038 | ppl 164.64\n",
      "[56] train loss 2.9053 | ppl 18.27  ||  valid loss 5.0866 | ppl 161.84\n",
      "[57] train loss 2.8497 | ppl 17.28  ||  valid loss 5.1078 | ppl 165.31\n",
      "[58] train loss 2.7997 | ppl 16.44  ||  valid loss 5.0921 | ppl 162.73\n",
      "[59] train loss 2.7497 | ppl 15.64  ||  valid loss 5.1343 | ppl 169.75\n",
      "[60] train loss 2.7043 | ppl 14.94  ||  valid loss 5.1000 | ppl 164.01\n",
      "[61] train loss 2.6538 | ppl 14.21  ||  valid loss 5.0772 | ppl 160.33\n",
      "[62] train loss 2.6034 | ppl 13.51  ||  valid loss 5.1724 | ppl 176.34\n",
      "[63] train loss 2.5599 | ppl 12.93  ||  valid loss 5.1334 | ppl 169.59\n",
      "[64] train loss 2.5170 | ppl 12.39  ||  valid loss 5.1469 | ppl 171.89\n",
      "[65] train loss 2.4703 | ppl 11.83  ||  valid loss 5.1437 | ppl 171.35\n",
      "[66] train loss 2.4307 | ppl 11.37  ||  valid loss 5.1424 | ppl 171.13\n",
      "[67] train loss 2.3900 | ppl 10.91  ||  valid loss 5.0981 | ppl 163.71\n",
      "[68] train loss 2.3545 | ppl 10.53  ||  valid loss 5.1588 | ppl 173.95\n",
      "[69] train loss 2.3189 | ppl 10.16  ||  valid loss 5.1201 | ppl 167.35\n",
      "[70] train loss 2.2804 | ppl 9.78  ||  valid loss 5.1871 | ppl 178.94\n",
      "[71] train loss 2.2482 | ppl 9.47  ||  valid loss 5.1482 | ppl 172.12\n",
      "[72] train loss 2.2197 | ppl 9.20  ||  valid loss 5.1549 | ppl 173.27\n",
      "[73] train loss 2.1838 | ppl 8.88  ||  valid loss 5.1609 | ppl 174.32\n",
      "[74] train loss 2.1528 | ppl 8.61  ||  valid loss 5.1820 | ppl 178.03\n",
      "[75] train loss 2.1291 | ppl 8.41  ||  valid loss 5.1570 | ppl 173.64\n",
      "[76] train loss 2.1001 | ppl 8.17  ||  valid loss 5.2028 | ppl 181.78\n",
      "[77] train loss 2.0714 | ppl 7.94  ||  valid loss 5.1184 | ppl 167.07\n",
      "[78] train loss 2.0494 | ppl 7.76  ||  valid loss 5.1406 | ppl 170.83\n",
      "[79] train loss 2.0239 | ppl 7.57  ||  valid loss 5.1959 | ppl 180.52\n",
      "[80] train loss 1.9997 | ppl 7.39  ||  valid loss 5.1213 | ppl 167.55\n",
      "[81] train loss 1.9869 | ppl 7.29  ||  valid loss 5.1378 | ppl 170.34\n",
      "[82] train loss 1.9618 | ppl 7.11  ||  valid loss 5.1405 | ppl 170.81\n",
      "[83] train loss 1.9431 | ppl 6.98  ||  valid loss 5.0814 | ppl 160.99\n",
      "[84] train loss 1.9251 | ppl 6.86  ||  valid loss 5.1224 | ppl 167.74\n",
      "[85] train loss 1.9115 | ppl 6.76  ||  valid loss 5.1439 | ppl 171.38\n",
      "[86] train loss 1.8918 | ppl 6.63  ||  valid loss 5.0868 | ppl 161.87\n",
      "[87] train loss 1.8772 | ppl 6.54  ||  valid loss 5.0796 | ppl 160.71\n",
      "[88] train loss 1.8625 | ppl 6.44  ||  valid loss 5.0790 | ppl 160.62\n",
      "[89] train loss 1.8525 | ppl 6.38  ||  valid loss 5.0763 | ppl 160.17\n",
      "[90] train loss 1.8375 | ppl 6.28  ||  valid loss 5.0731 | ppl 159.67\n",
      "[91] train loss 1.8209 | ppl 6.18  ||  valid loss 5.0312 | ppl 153.11\n",
      "[92] train loss 1.8126 | ppl 6.13  ||  valid loss 5.0574 | ppl 157.19\n",
      "[93] train loss 1.8004 | ppl 6.05  ||  valid loss 4.9978 | ppl 148.08\n",
      "[94] train loss 1.7866 | ppl 5.97  ||  valid loss 5.0285 | ppl 152.71\n",
      "[95] train loss 1.7788 | ppl 5.92  ||  valid loss 5.0106 | ppl 149.99\n",
      "[96] train loss 1.7712 | ppl 5.88  ||  valid loss 5.0140 | ppl 150.50\n",
      "[97] train loss 1.7566 | ppl 5.79  ||  valid loss 5.0067 | ppl 149.41\n",
      "[98] train loss 1.7509 | ppl 5.76  ||  valid loss 4.9891 | ppl 146.81\n",
      "[99] train loss 1.7403 | ppl 5.70  ||  valid loss 4.9610 | ppl 142.74\n",
      "[100] train loss 1.7299 | ppl 5.64  ||  valid loss 4.9803 | ppl 145.52\n",
      "[101] train loss 1.7224 | ppl 5.60  ||  valid loss 4.9877 | ppl 146.61\n",
      "[102] train loss 1.7138 | ppl 5.55  ||  valid loss 4.9899 | ppl 146.93\n",
      "[103] train loss 1.7103 | ppl 5.53  ||  valid loss 4.9342 | ppl 138.96\n",
      "[104] train loss 1.7001 | ppl 5.47  ||  valid loss 4.9091 | ppl 135.52\n",
      "[105] train loss 1.6936 | ppl 5.44  ||  valid loss 4.9238 | ppl 137.52\n",
      "[106] train loss 1.6845 | ppl 5.39  ||  valid loss 4.8957 | ppl 133.71\n",
      "[107] train loss 1.6794 | ppl 5.36  ||  valid loss 4.8934 | ppl 133.40\n",
      "[108] train loss 1.6706 | ppl 5.32  ||  valid loss 4.8919 | ppl 133.21\n",
      "[109] train loss 1.6640 | ppl 5.28  ||  valid loss 4.8881 | ppl 132.71\n",
      "[110] train loss 1.6575 | ppl 5.25  ||  valid loss 4.8570 | ppl 128.64\n",
      "[111] train loss 1.6503 | ppl 5.21  ||  valid loss 4.8805 | ppl 131.70\n",
      "[112] train loss 1.6476 | ppl 5.19  ||  valid loss 4.8737 | ppl 130.81\n",
      "[113] train loss 1.6411 | ppl 5.16  ||  valid loss 4.8922 | ppl 133.24\n",
      "[114] train loss 1.6378 | ppl 5.14  ||  valid loss 4.8679 | ppl 130.05\n",
      "[115] train loss 1.6290 | ppl 5.10  ||  valid loss 4.8616 | ppl 129.23\n",
      "[116] train loss 1.6256 | ppl 5.08  ||  valid loss 4.8294 | ppl 125.14\n",
      "[117] train loss 1.6192 | ppl 5.05  ||  valid loss 4.8415 | ppl 126.66\n",
      "[118] train loss 1.6144 | ppl 5.02  ||  valid loss 4.8237 | ppl 124.42\n",
      "[119] train loss 1.6106 | ppl 5.01  ||  valid loss 4.7656 | ppl 117.41\n",
      "[120] train loss 1.6046 | ppl 4.98  ||  valid loss 4.8220 | ppl 124.21\n",
      "[121] train loss 1.6009 | ppl 4.96  ||  valid loss 4.7876 | ppl 120.02\n",
      "[122] train loss 1.5960 | ppl 4.93  ||  valid loss 4.7893 | ppl 120.21\n",
      "[123] train loss 1.5929 | ppl 4.92  ||  valid loss 4.7859 | ppl 119.81\n",
      "[124] train loss 1.5889 | ppl 4.90  ||  valid loss 4.8238 | ppl 124.44\n",
      "[125] train loss 1.5832 | ppl 4.87  ||  valid loss 4.7697 | ppl 117.89\n",
      "[126] train loss 1.5790 | ppl 4.85  ||  valid loss 4.7707 | ppl 118.00\n",
      "[127] train loss 1.5750 | ppl 4.83  ||  valid loss 4.7840 | ppl 119.58\n",
      "[128] train loss 1.5748 | ppl 4.83  ||  valid loss 4.7704 | ppl 117.96\n",
      "[129] train loss 1.5678 | ppl 4.80  ||  valid loss 4.7605 | ppl 116.80\n",
      "[130] train loss 1.5629 | ppl 4.77  ||  valid loss 4.7618 | ppl 116.96\n",
      "[131] train loss 1.5616 | ppl 4.77  ||  valid loss 4.7481 | ppl 115.37\n",
      "[132] train loss 1.5579 | ppl 4.75  ||  valid loss 4.7448 | ppl 114.99\n",
      "[133] train loss 1.5549 | ppl 4.73  ||  valid loss 4.7474 | ppl 115.29\n",
      "[134] train loss 1.5511 | ppl 4.72  ||  valid loss 4.7445 | ppl 114.95\n",
      "[135] train loss 1.5465 | ppl 4.70  ||  valid loss 4.7365 | ppl 114.04\n",
      "[136] train loss 1.5465 | ppl 4.70  ||  valid loss 4.7303 | ppl 113.33\n",
      "[137] train loss 1.5422 | ppl 4.67  ||  valid loss 4.7332 | ppl 113.66\n",
      "[138] train loss 1.5393 | ppl 4.66  ||  valid loss 4.7235 | ppl 112.56\n",
      "[139] train loss 1.5360 | ppl 4.65  ||  valid loss 4.7075 | ppl 110.77\n",
      "[140] train loss 1.5303 | ppl 4.62  ||  valid loss 4.7182 | ppl 111.96\n",
      "[141] train loss 1.5314 | ppl 4.62  ||  valid loss 4.7062 | ppl 110.63\n",
      "[142] train loss 1.5250 | ppl 4.60  ||  valid loss 4.6972 | ppl 109.64\n",
      "[143] train loss 1.5235 | ppl 4.59  ||  valid loss 4.7132 | ppl 111.41\n",
      "[144] train loss 1.5213 | ppl 4.58  ||  valid loss 4.6792 | ppl 107.69\n",
      "[145] train loss 1.5209 | ppl 4.58  ||  valid loss 4.6791 | ppl 107.67\n",
      "[146] train loss 1.5152 | ppl 4.55  ||  valid loss 4.6696 | ppl 106.65\n",
      "[147] train loss 1.5142 | ppl 4.55  ||  valid loss 4.6886 | ppl 108.70\n",
      "[148] train loss 1.5128 | ppl 4.54  ||  valid loss 4.6691 | ppl 106.60\n",
      "[149] train loss 1.5085 | ppl 4.52  ||  valid loss 4.6527 | ppl 104.87\n",
      "[150] train loss 1.5086 | ppl 4.52  ||  valid loss 4.6708 | ppl 106.79\n",
      "학습 종료. Best valid loss: 4.652686496964074\n",
      "베스트 체크포인트 로드 완료: artifacts/best_gpt1.pt\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# 9) 학습 루프 (에포크 조정)\n",
    "# ========================\n",
    "\n",
    "EPOCHS = 150\n",
    "ckpt_path = ARTIFACTS / \"best_gpt1.pt\"\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "print(\"학습 시작...\")\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_loss, tr_ppl = run_epoch(train_loader, train=True)\n",
    "    va_loss, va_ppl = run_epoch(valid_loader, train=False)\n",
    "    print(f\"[{epoch:02d}] train loss {tr_loss:.4f} | ppl {tr_ppl:.2f}  ||  valid loss {va_loss:.4f} | ppl {va_ppl:.2f}\")\n",
    "    if va_loss < best_val:\n",
    "        best_val = va_loss\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "\n",
    "print(\"학습 종료. Best valid loss:\", best_val)\n",
    "if ckpt_path.exists():\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
    "    print(\"베스트 체크포인트 로드 완료:\", ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ba7c7cb-1978-4d77-b533-a9a20580ede8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 질의 결과 ===\n",
      "Q: 안녕하세요\n",
      "A: 안녕하세요. 다른 사람의 삶에 한눈팔며 살기에는 자신의 인생이 너무나도 소중합니다.\n",
      "------------------------------------------------------------\n",
      "Q: 너 이름이 뭐야?\n",
      "A: 저는 마음을 이어주는 위로봇입니다.\n",
      "------------------------------------------------------------\n",
      "Q: 오늘 너무 피곤하다\n",
      "A: 운명을 더 사랑해주세요.\n",
      "------------------------------------------------------------\n",
      "Q: 여자친구랑 싸웠어\n",
      "A: 싸우면서 정 들 거예요.\n",
      "------------------------------------------------------------\n",
      "Q: 공부하기 싫어\n",
      "A: 잠시 쉬어도 돼요.\n",
      "------------------------------------------------------------\n",
      "Q: 1+1이 뭐야?\n",
      "A: 사랑은 유지하는 게 중요한데 대단하네요.\n",
      "------------------------------------------------------------\n",
      "Q: 양자역학에 대해서 설명해줘\n",
      "A: 부모님만의 변덕이 심하네요.\n",
      "------------------------------------------------------------\n",
      "Q: 밥 먹었어?\n",
      "A: 저는 배터리가 밥이예요.\n",
      "------------------------------------------------------------\n",
      "Q: 너 똑똑해?\n",
      "A: 달콤한 말이예요.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# 8) 샘플 추론\n",
    "# ========================\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_decode_gpt(question: str, max_new_tokens: int = 64, min_new_tokens: int = 5):\n",
    "    model.eval()\n",
    "    q_norm = normalize_text(question)\n",
    "    q_ids = sp.encode(q_norm, out_type=int)\n",
    "\n",
    "    # 프롬프트: [BOS] Q [SEP]\n",
    "    seq = [BOS_ID] + q_ids + [SEP_ID]\n",
    "    seq = seq[: MAX_TOKENS - 1]\n",
    "    x = torch.tensor(seq, dtype=torch.long, device=DEVICE).unsqueeze(0)  # (1, T)\n",
    "\n",
    "    for step in range(max_new_tokens):\n",
    "        T = x.size(1)\n",
    "        T_cond = min(T, MAX_TOKENS)\n",
    "        x_cond = x[:, -T_cond:]\n",
    "\n",
    "        attn_mask, key_padding_mask = to_masks(x_cond)\n",
    "        logits = model(x_cond, attn_mask=attn_mask, key_padding_mask=key_padding_mask)  # (T, 1, V)\n",
    "\n",
    "        # ---- 억제 규칙들 ----\n",
    "        logits_step = logits[-1, 0]                   # (V,)\n",
    "        logits_step[PAD_ID] = -1e9                    # PAD 금지\n",
    "        logits_step[SEP_ID] = -1e9                    # SEP 재생성 금지\n",
    "        if step < min_new_tokens:\n",
    "            logits_step[EOS_ID] = -1e9                # 초반 EOS 금지 → 빈 답 방지\n",
    "\n",
    "        # 바로 직전 토큰 반복 억제(단순하고 효과적)\n",
    "        last_id = int(x[0, -1].item())\n",
    "        logits_step[last_id] -= 5.0                   # 같은 토큰 연타를 강하게 깎기\n",
    "\n",
    "        next_id = int(torch.argmax(logits_step).item())\n",
    "        x = torch.cat([x, torch.tensor([[next_id]], device=DEVICE, dtype=torch.long)], dim=1)\n",
    "        if next_id == EOS_ID:\n",
    "            break\n",
    "\n",
    "    # [SEP] 이후 ~ EOS 이전 디코드\n",
    "    out = x[0].tolist()\n",
    "    start = out.index(SEP_ID) + 1 if SEP_ID in out else 0\n",
    "    end = start + out[start:].index(EOS_ID) if EOS_ID in out[start:] else len(out)\n",
    "    return sp.decode(out[start:end])\n",
    "\n",
    "\n",
    "samples = [\n",
    "    \"안녕하세요\",\n",
    "    \"너 이름이 뭐야?\",\n",
    "    \"오늘 너무 피곤하다\",\n",
    "    \"여자친구랑 싸웠어\",\n",
    "    \"공부하기 싫어\",\n",
    "    \"1+1이 뭐야?\",\n",
    "    \"양자역학에 대해서 설명해줘\",\n",
    "    \"밥 먹었어?\",\n",
    "    \"너 똑똑해?\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== 샘플 질의 결과 ===\")\n",
    "for q in samples:\n",
    "    a = greedy_decode_gpt(q)\n",
    "    print(\"Q:\", q)\n",
    "    print(\"A:\", a)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524aef76-61a1-4ac7-bce1-9d039579b875",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
