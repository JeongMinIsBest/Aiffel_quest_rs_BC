{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7242fe26-b919-4c45-91ad-9c05e49ea2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece torch torchvision scikit-learn pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a496468a-d858-4f27-ab98-1f197a9a85a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import math\n",
    "import unicodedata\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97132749-dfd7-4786-870d-160313f64865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) 재현성(재사용) 확보: 시드 고정\n",
    "# ------------------------------------------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52097401-8ae2-453b-af4a-931d66da63c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로딩 중...\n",
      "              Q            A  label\n",
      "0        12시 땡!   하루가 또 가네요.      0\n",
      "1   1지망 학교 떨어졌어    위로해 드립니다.      0\n",
      "2  3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "원본 샘플 수: 11823\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1) 데이터 로드\n",
    "#   - ChatbotData.csv: Q(질문), A(답변), label(의도, 여기선 사용X)\n",
    "#   - 인터넷이 되는 환경에서 URL로 직접 로딩\n",
    "# ------------------------------------------------------------\n",
    "URL = \"https://github.com/songys/Chatbot_data/raw/master/ChatbotData.csv\"\n",
    "\n",
    "print(\"데이터 로딩 중...\")\n",
    "df = pd.read_csv(URL)\n",
    "print(df.head(3))\n",
    "print(\"원본 샘플 수:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a021b430-1dec-4105-a881-fd645993c42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 샘플 수: 11750\n",
      "train: 11162  | valid: 588\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2) 전처리 함수 정의\n",
    "#   - 한국어/영문 혼용 채팅 텍스트에서 잡음을 줄여 토크나이저/모델 학습 안정화\n",
    "#   - 규칙:\n",
    "#       (1) 유니코드 정규화(NFKC): 전각/반각/합성문자 통일\n",
    "#       (2) 허용 문자만 남기기: 한글/영문/숫자/공백/기본문장부호\n",
    "#       (3) 다중 공백 → 단일 공백, 양끝 공백 제거\n",
    "#   - 주의: 학습과 추론에서 \"동일\" 규칙을 써야 함\n",
    "# ------------------------------------------------------------\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s))                           # (1)\n",
    "    s = re.sub(r\"[^0-9A-Za-z가-힣ㄱ-ㅎㅏ-ㅣ\\s.,?!~’'\\\"()\\-\\:;@/]\", \" \", s)  # (2)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()                                   # (3)\n",
    "    return s\n",
    "\n",
    "# 결측치 제거(Q/A 둘 중 하나라도 NaN이면 제거), 원본 백업\n",
    "df = df.dropna(subset=[\"Q\", \"A\"]).copy()\n",
    "df[\"Q_orig\"] = df[\"Q\"]\n",
    "df[\"A_orig\"] = df[\"A\"]\n",
    "\n",
    "# 전처리 적용\n",
    "df[\"Q\"] = df[\"Q\"].map(normalize_text)\n",
    "df[\"A\"] = df[\"A\"].map(normalize_text)\n",
    "\n",
    "# 너무 짧거나(정보 부족) 너무 긴(학습 불안정) 샘플 제거\n",
    "MIN_CHARS, MAX_CHARS = 1, 128\n",
    "mask = (\n",
    "    df[\"Q\"].str.len().between(MIN_CHARS, MAX_CHARS) &\n",
    "    df[\"A\"].str.len().between(MIN_CHARS, MAX_CHARS)\n",
    ")\n",
    "df = df[mask].drop_duplicates(subset=[\"Q\", \"A\"]).reset_index(drop=True)\n",
    "print(\"전처리 후 샘플 수:\", len(df))\n",
    "\n",
    "# 학습/검증 분리 (검증은 과적합 체크 및 체크포인트 기준)\n",
    "train_df, valid_df = train_test_split(\n",
    "    df[[\"Q\", \"A\"]],\n",
    "    test_size=0.05,\n",
    "    random_state=SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "print(f\"train: {len(train_df)}  | valid: {len(valid_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23ab1581-1369-437b-b0d2-f7a32992d0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 SentencePiece 모델 발견 → 재사용.\n",
      "Vocab size: 8000 | PAD/BOS/EOS: 0 2 3\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3) SentencePiece 서브워드 토크나이저 학습\n",
    "#   - 형태소 분석기 대신 서브워드(언어 비의존적) 사용\n",
    "#   - Q/A를 합친 \"공동 코퍼스\"로 1회 학습 → 어휘공간 공유\n",
    "#   - 특수 토큰 ID 고정: PAD=0, UNK=1, BOS=2, EOS=3\n",
    "# ------------------------------------------------------------\n",
    "ARTIFACTS = Path(\"artifacts\")\n",
    "ARTIFACTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "corpus_path = ARTIFACTS / \"spm_corpus.txt\"\n",
    "with io.open(corpus_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    # 질문과 답변을 하나의 코퍼스로 합쳐 학습 → 양 방향 어휘를 모두 커버\n",
    "    for s in pd.concat([train_df[\"Q\"], train_df[\"A\"]], axis=0).astype(str):\n",
    "        if s:\n",
    "            f.write(s + \"\\n\")\n",
    "\n",
    "spm_model_prefix = str(ARTIFACTS / \"spm_ko\")\n",
    "VOCAB_SIZE = 8000  # 데이터 크기/도메인에 따라 4k~16k 탐색 권장\n",
    "\n",
    "# 이미 학습된 모델이 없으면 학습\n",
    "if not Path(spm_model_prefix + \".model\").exists():\n",
    "    print(\"SentencePiece 학습 시작...\")\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        input=str(corpus_path),\n",
    "        model_prefix=spm_model_prefix,\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        model_type=\"unigram\",            # \"bpe\"도 가능\n",
    "        character_coverage=0.9995,       # 한/영 혼용 텍스트에서 적절\n",
    "        pad_id=0, unk_id=1, bos_id=2, eos_id=3,\n",
    "        pad_piece=\"[PAD]\", unk_piece=\"[UNK]\", bos_piece=\"<s>\", eos_piece=\"</s>\",\n",
    "        user_defined_symbols=[],\n",
    "        input_sentence_size=2000000,     # (옵션) 대규모 코퍼스 샘플링 크기\n",
    "        shuffle_input_sentence=True,\n",
    "        train_extremely_large_corpus=False\n",
    "    )\n",
    "    print(\"SentencePiece 학습 완료.\")\n",
    "else:\n",
    "    print(\"기존 SentencePiece 모델 발견 → 재사용.\")\n",
    "\n",
    "# 학습된 토크나이저 로드\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(spm_model_prefix + \".model\")\n",
    "\n",
    "PAD_ID = sp.pad_id()   # 0\n",
    "UNK_ID = sp.unk_id()   # 1\n",
    "BOS_ID = sp.bos_id()   # 2\n",
    "EOS_ID = sp.eos_id()   # 3\n",
    "\n",
    "print(\"Vocab size:\", sp.get_piece_size(), \"| PAD/BOS/EOS:\", PAD_ID, BOS_ID, EOS_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9205418b-210f-46fe-be7c-9adcd8f69f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4) PyTorch Dataset / DataLoader\n",
    "#   - encode_text: 문자열 → 서브워드 IDs (최대 길이 컷오프)\n",
    "#   - ChatDataset: Transformer 입력/출력 쌍 구성\n",
    "#       * src      = [BOS] + Q + [EOS]        → 인코더 입력\n",
    "#       * tgt_in   = [BOS] + A                → 디코더 입력(teacher forcing)\n",
    "#       * tgt_out  =       A + [EOS]          → 디코더 정답\n",
    "#   - collate_fn: 배치 내 최대길이 기준 오른쪽 PAD로 맞춤\n",
    "#     ※ DataLoader는 (batch, seq)의 텐서를 뱉고,\n",
    "#        모델로 넣기 직전에 (seq, batch)로 전치합니다.\n",
    "# ------------------------------------------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "MAX_TOKENS = 64  # 질문/답변 모두 동일한 최대 서브워드 길이(실험으로 조정)\n",
    "\n",
    "def encode_text(s: str, spm_processor, max_len=MAX_TOKENS):\n",
    "    # 주의: 학습과 추론에서 동일 전처리(norm) + 동일 토크나이저 사용\n",
    "    ids = spm_processor.encode(s, out_type=int)\n",
    "    ids = ids[:max_len - 2]  # BOS/EOS 자리를 위해 -2\n",
    "    return ids\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, frame: pd.DataFrame, spm_processor, max_len=MAX_TOKENS):\n",
    "        self.q = frame[\"Q\"].tolist()\n",
    "        self.a = frame[\"A\"].tolist()\n",
    "        self.sp = spm_processor\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.q)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # 문자열 → 서브워드 ID 시퀀스\n",
    "        src_ids = encode_text(self.q[i], self.sp, self.max_len)\n",
    "        tgt_ids = encode_text(self.a[i], self.sp, self.max_len)\n",
    "\n",
    "        # 인코더/디코더 시퀀스 구성(시작/종료 토큰 부착, 1칸 시프트)\n",
    "        src = [BOS_ID] + src_ids + [EOS_ID]\n",
    "        tgt_in  = [BOS_ID] + tgt_ids\n",
    "        tgt_out = tgt_ids + [EOS_ID]\n",
    "\n",
    "        return torch.tensor(src, dtype=torch.long), \\\n",
    "               torch.tensor(tgt_in, dtype=torch.long), \\\n",
    "               torch.tensor(tgt_out, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch: List[ (src, tgt_in, tgt_out) ]\n",
    "    srcs, tgts_in, tgts_out = zip(*batch)\n",
    "\n",
    "    def pad_right(seqs, pad_id=PAD_ID):\n",
    "        # (batch, max_len)로 오른쪽 패딩\n",
    "        maxlen = max(x.size(0) for x in seqs)\n",
    "        out = torch.full((len(seqs), maxlen), pad_id, dtype=torch.long)\n",
    "        for i, s in enumerate(seqs):\n",
    "            out[i, :s.size(0)] = s\n",
    "        return out\n",
    "\n",
    "    return pad_right(srcs), pad_right(tgts_in), pad_right(tgts_out)\n",
    "\n",
    "train_ds = ChatDataset(train_df, sp, MAX_TOKENS)\n",
    "valid_ds = ChatDataset(valid_df, sp, MAX_TOKENS)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=128, shuffle=False, collate_fn=collate_fn, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "272f586e-8c38-4ec3-8e88-a27bde7aa162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5) Positional Encoding (사인/코사인)\n",
    "#   - Transformer는 RNN처럼 순서를 내재적으로 알지 못하므로\n",
    "#     위치 정보를 임베딩에 더해줘야 함\n",
    "#   - 입력/타깃 시퀀스 모두에 적용\n",
    "# ------------------------------------------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 2048, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # pe: (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) *\n",
    "                             (-math.log(10000.0) / d_model))\n",
    "        # 짝수/홀수 차원에 각각 sin/cos\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Buffer로 등록하면 학습 파라미터가 아닌 \"상수\"로 취급됨\n",
    "        self.register_buffer('pe', pe.unsqueeze(1))  # (max_len, 1, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (seq_len, batch, d_model) 이어야 함\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20a33440-e95f-4897-a132-d880bbac4d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 6) Transformer 모델 정의\n",
    "#   - 임베딩 → 포지셔널 인코딩 → (인코더/디코더 스택) → Linear(어휘 크기 로짓)\n",
    "#   - nn.Transformer 기본 규약: (seq, batch, dim), batch_first=False\n",
    "# ------------------------------------------------------------\n",
    "class TransformerChatbot(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=8,\n",
    "                 num_layers=4, dim_ff=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n",
    "        self.pos_enc = PositionalEncoding(d_model, dropout=dropout)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=False  # 입력/출력은 (seq, batch, dim)\n",
    "        )\n",
    "        self.generator = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt_in,\n",
    "                src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask):\n",
    "        \"\"\"\n",
    "        src  : (S, B)  인코더 입력\n",
    "        tgt_in: (T, B) 디코더 입력(teacher forcing)\n",
    "        *_key_padding_mask: (B, S or T), True=가려라(PAD)\n",
    "        반환: logits (T, B, V)\n",
    "        \"\"\"\n",
    "        # 임베딩 + 스케일링(√d_model)\n",
    "        src = self.tok_emb(src) * math.sqrt(self.tok_emb.embedding_dim)  # (S, B, d_model)\n",
    "        tgt = self.tok_emb(tgt_in) * math.sqrt(self.tok_emb.embedding_dim)  # (T, B, d_model)\n",
    "\n",
    "        # 위치 인코딩 부착\n",
    "        src = self.pos_enc(src)\n",
    "        tgt = self.pos_enc(tgt)\n",
    "\n",
    "        # 디코더 미래 가림 마스크 생성: (T, T)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(0)).to(src.device)\n",
    "\n",
    "        # Transformer 전파\n",
    "        out = self.transformer(\n",
    "            src=src,\n",
    "            tgt=tgt,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask\n",
    "        )  # (T, B, d_model)\n",
    "\n",
    "        # 어휘 분포 로짓\n",
    "        logits = self.generator(out)  # (T, B, V)\n",
    "        return logits\n",
    "\n",
    "VOCAB = sp.get_piece_size()\n",
    "model = TransformerChatbot(VOCAB).to(DEVICE)\n",
    "\n",
    "# 손실함수: PAD 무시 + 라벨 스무딩(과신 방지)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID, label_smoothing=0.1)\n",
    "# 옵티마이저: Adam (Transformer 권장 세팅)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5df2ee8-baed-430a-908a-08a397199346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7) 학습 유틸(마스크/전치 등)\n",
    "#   - key_padding_mask: (batch, seq) 형태, 값 True=PAD=가려라\n",
    "#   - Transformer는 (seq, batch, dim)을 쓰므로, DataLoader 출력\n",
    "#     (batch, seq)을 (seq, batch)로 전치해서 넣음\n",
    "# ------------------------------------------------------------\n",
    "def make_padding_mask(batch_ids):  # (batch, seq) → (batch, seq) bool\n",
    "    return (batch_ids == PAD_ID)\n",
    "\n",
    "def to_seq_first(x):  # (batch, seq) → (seq, batch)\n",
    "    return x.transpose(0, 1).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01e730bb-32ac-49ef-8d9b-6d3e00b1e430",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] train loss 6.4373 | ppl 624.70  ||  valid loss 5.8864 | ppl 360.12\n",
      "[02] train loss 5.7244 | ppl 306.26  ||  valid loss 5.5409 | ppl 254.90\n",
      "[03] train loss 5.4169 | ppl 225.18  ||  valid loss 5.2995 | ppl 200.23\n",
      "[04] train loss 5.1649 | ppl 175.02  ||  valid loss 5.1064 | ppl 165.07\n",
      "[05] train loss 4.9516 | ppl 141.40  ||  valid loss 4.9709 | ppl 144.16\n",
      "[06] train loss 4.7600 | ppl 116.74  ||  valid loss 4.8509 | ppl 127.85\n",
      "[07] train loss 4.5966 | ppl 99.15  ||  valid loss 4.7403 | ppl 114.46\n",
      "[08] train loss 4.4435 | ppl 85.07  ||  valid loss 4.6622 | ppl 105.87\n",
      "[09] train loss 4.3039 | ppl 73.98  ||  valid loss 4.5984 | ppl 99.32\n",
      "[10] train loss 4.1674 | ppl 64.55  ||  valid loss 4.5135 | ppl 91.24\n",
      "[11] train loss 4.0434 | ppl 57.02  ||  valid loss 4.4720 | ppl 87.53\n",
      "[12] train loss 3.9223 | ppl 50.52  ||  valid loss 4.4223 | ppl 83.29\n",
      "[13] train loss 3.8125 | ppl 45.26  ||  valid loss 4.3670 | ppl 78.81\n",
      "[14] train loss 3.6980 | ppl 40.37  ||  valid loss 4.3289 | ppl 75.86\n",
      "[15] train loss 3.5951 | ppl 36.42  ||  valid loss 4.2860 | ppl 72.68\n",
      "[16] train loss 3.4929 | ppl 32.88  ||  valid loss 4.2410 | ppl 69.47\n",
      "[17] train loss 3.3919 | ppl 29.72  ||  valid loss 4.2157 | ppl 67.74\n",
      "[18] train loss 3.2951 | ppl 26.98  ||  valid loss 4.1795 | ppl 65.33\n",
      "[19] train loss 3.2035 | ppl 24.62  ||  valid loss 4.1496 | ppl 63.41\n",
      "[20] train loss 3.1175 | ppl 22.59  ||  valid loss 4.1245 | ppl 61.83\n",
      "[21] train loss 3.0282 | ppl 20.66  ||  valid loss 4.0896 | ppl 59.72\n",
      "[22] train loss 2.9415 | ppl 18.94  ||  valid loss 4.0839 | ppl 59.38\n",
      "[23] train loss 2.8573 | ppl 17.41  ||  valid loss 4.0615 | ppl 58.06\n",
      "[24] train loss 2.7780 | ppl 16.09  ||  valid loss 4.0394 | ppl 56.79\n",
      "[25] train loss 2.7001 | ppl 14.88  ||  valid loss 4.0278 | ppl 56.14\n",
      "[26] train loss 2.6240 | ppl 13.79  ||  valid loss 4.0175 | ppl 55.56\n",
      "[27] train loss 2.5513 | ppl 12.82  ||  valid loss 3.9966 | ppl 54.41\n",
      "[28] train loss 2.4737 | ppl 11.87  ||  valid loss 3.9952 | ppl 54.34\n",
      "[29] train loss 2.4083 | ppl 11.11  ||  valid loss 3.9712 | ppl 53.05\n",
      "[30] train loss 2.3388 | ppl 10.37  ||  valid loss 3.9866 | ppl 53.87\n",
      "[31] train loss 2.2789 | ppl 9.77  ||  valid loss 3.9888 | ppl 53.99\n",
      "[32] train loss 2.2129 | ppl 9.14  ||  valid loss 3.9936 | ppl 54.25\n",
      "[33] train loss 2.1561 | ppl 8.64  ||  valid loss 3.9979 | ppl 54.48\n",
      "[34] train loss 2.0994 | ppl 8.16  ||  valid loss 4.0065 | ppl 54.95\n",
      "[35] train loss 2.0489 | ppl 7.76  ||  valid loss 4.0075 | ppl 55.01\n",
      "[36] train loss 2.0008 | ppl 7.40  ||  valid loss 4.0058 | ppl 54.91\n",
      "[37] train loss 1.9540 | ppl 7.06  ||  valid loss 4.0182 | ppl 55.60\n",
      "[38] train loss 1.9075 | ppl 6.74  ||  valid loss 4.0278 | ppl 56.14\n",
      "[39] train loss 1.8696 | ppl 6.49  ||  valid loss 4.0394 | ppl 56.79\n",
      "[40] train loss 1.8314 | ppl 6.24  ||  valid loss 4.0503 | ppl 57.42\n",
      "[41] train loss 1.7987 | ppl 6.04  ||  valid loss 4.0722 | ppl 58.69\n",
      "[42] train loss 1.7606 | ppl 5.82  ||  valid loss 4.0675 | ppl 58.41\n",
      "[43] train loss 1.7333 | ppl 5.66  ||  valid loss 4.0862 | ppl 59.51\n",
      "[44] train loss 1.7071 | ppl 5.51  ||  valid loss 4.0859 | ppl 59.49\n",
      "[45] train loss 1.6794 | ppl 5.36  ||  valid loss 4.1092 | ppl 60.90\n",
      "[46] train loss 1.6621 | ppl 5.27  ||  valid loss 4.1247 | ppl 61.85\n",
      "[47] train loss 1.6408 | ppl 5.16  ||  valid loss 4.1216 | ppl 61.66\n",
      "[48] train loss 1.6240 | ppl 5.07  ||  valid loss 4.1170 | ppl 61.37\n",
      "[49] train loss 1.6081 | ppl 4.99  ||  valid loss 4.1443 | ppl 63.07\n",
      "[50] train loss 1.5920 | ppl 4.91  ||  valid loss 4.1400 | ppl 62.80\n",
      "[51] train loss 1.5780 | ppl 4.85  ||  valid loss 4.1412 | ppl 62.88\n",
      "[52] train loss 1.5635 | ppl 4.78  ||  valid loss 4.1407 | ppl 62.85\n",
      "[53] train loss 1.5489 | ppl 4.71  ||  valid loss 4.1376 | ppl 62.65\n",
      "[54] train loss 1.5442 | ppl 4.68  ||  valid loss 4.1316 | ppl 62.28\n",
      "[55] train loss 1.5327 | ppl 4.63  ||  valid loss 4.1429 | ppl 62.99\n",
      "[56] train loss 1.5247 | ppl 4.59  ||  valid loss 4.1447 | ppl 63.10\n",
      "[57] train loss 1.5170 | ppl 4.56  ||  valid loss 4.1563 | ppl 63.84\n",
      "[58] train loss 1.5101 | ppl 4.53  ||  valid loss 4.1417 | ppl 62.91\n",
      "[59] train loss 1.5006 | ppl 4.48  ||  valid loss 4.1531 | ppl 63.63\n",
      "[60] train loss 1.4953 | ppl 4.46  ||  valid loss 4.1414 | ppl 62.89\n",
      "[61] train loss 1.4903 | ppl 4.44  ||  valid loss 4.1377 | ppl 62.66\n",
      "[62] train loss 1.4851 | ppl 4.42  ||  valid loss 4.1552 | ppl 63.77\n",
      "[63] train loss 1.4781 | ppl 4.38  ||  valid loss 4.1519 | ppl 63.56\n",
      "[64] train loss 1.4758 | ppl 4.37  ||  valid loss 4.1398 | ppl 62.79\n",
      "[65] train loss 1.4696 | ppl 4.35  ||  valid loss 4.1378 | ppl 62.67\n",
      "[66] train loss 1.4664 | ppl 4.33  ||  valid loss 4.1607 | ppl 64.12\n",
      "[67] train loss 1.4624 | ppl 4.32  ||  valid loss 4.1346 | ppl 62.46\n",
      "[68] train loss 1.4555 | ppl 4.29  ||  valid loss 4.1246 | ppl 61.84\n",
      "[69] train loss 1.4538 | ppl 4.28  ||  valid loss 4.1097 | ppl 60.93\n",
      "[70] train loss 1.4480 | ppl 4.25  ||  valid loss 4.1367 | ppl 62.60\n",
      "[71] train loss 1.4452 | ppl 4.24  ||  valid loss 4.1241 | ppl 61.81\n",
      "[72] train loss 1.4419 | ppl 4.23  ||  valid loss 4.1082 | ppl 60.84\n",
      "[73] train loss 1.4405 | ppl 4.22  ||  valid loss 4.1172 | ppl 61.39\n",
      "[74] train loss 1.4377 | ppl 4.21  ||  valid loss 4.1028 | ppl 60.51\n",
      "[75] train loss 1.4328 | ppl 4.19  ||  valid loss 4.1162 | ppl 61.33\n",
      "[76] train loss 1.4291 | ppl 4.18  ||  valid loss 4.1141 | ppl 61.20\n",
      "[77] train loss 1.4262 | ppl 4.16  ||  valid loss 4.0878 | ppl 59.61\n",
      "[78] train loss 1.4253 | ppl 4.16  ||  valid loss 4.0777 | ppl 59.01\n",
      "[79] train loss 1.4220 | ppl 4.15  ||  valid loss 4.0821 | ppl 59.27\n",
      "[80] train loss 1.4177 | ppl 4.13  ||  valid loss 4.0865 | ppl 59.53\n",
      "[81] train loss 1.4169 | ppl 4.12  ||  valid loss 4.0890 | ppl 59.68\n",
      "[82] train loss 1.4144 | ppl 4.11  ||  valid loss 4.0803 | ppl 59.16\n",
      "[83] train loss 1.4128 | ppl 4.11  ||  valid loss 4.1005 | ppl 60.37\n",
      "[84] train loss 1.4087 | ppl 4.09  ||  valid loss 4.0912 | ppl 59.81\n",
      "[85] train loss 1.4075 | ppl 4.09  ||  valid loss 4.0787 | ppl 59.07\n",
      "[86] train loss 1.4059 | ppl 4.08  ||  valid loss 4.0680 | ppl 58.44\n",
      "[87] train loss 1.4031 | ppl 4.07  ||  valid loss 4.0744 | ppl 58.82\n",
      "[88] train loss 1.4003 | ppl 4.06  ||  valid loss 4.0781 | ppl 59.04\n",
      "[89] train loss 1.3983 | ppl 4.05  ||  valid loss 4.0698 | ppl 58.54\n",
      "[90] train loss 1.3990 | ppl 4.05  ||  valid loss 4.0493 | ppl 57.36\n",
      "[91] train loss 1.3967 | ppl 4.04  ||  valid loss 4.0716 | ppl 58.65\n",
      "[92] train loss 1.3946 | ppl 4.03  ||  valid loss 4.0569 | ppl 57.79\n",
      "[93] train loss 1.3919 | ppl 4.02  ||  valid loss 4.0449 | ppl 57.11\n",
      "[94] train loss 1.3917 | ppl 4.02  ||  valid loss 4.0387 | ppl 56.75\n",
      "[95] train loss 1.3877 | ppl 4.01  ||  valid loss 4.0479 | ppl 57.28\n",
      "[96] train loss 1.3877 | ppl 4.01  ||  valid loss 4.0237 | ppl 55.91\n",
      "[97] train loss 1.3864 | ppl 4.00  ||  valid loss 4.0341 | ppl 56.49\n",
      "[98] train loss 1.3852 | ppl 4.00  ||  valid loss 4.0282 | ppl 56.16\n",
      "[99] train loss 1.3847 | ppl 3.99  ||  valid loss 4.0353 | ppl 56.56\n",
      "[100] train loss 1.3824 | ppl 3.98  ||  valid loss 4.0335 | ppl 56.46\n",
      "[101] train loss 1.3803 | ppl 3.98  ||  valid loss 4.0266 | ppl 56.07\n",
      "[102] train loss 1.3800 | ppl 3.97  ||  valid loss 4.0363 | ppl 56.61\n",
      "[103] train loss 1.3785 | ppl 3.97  ||  valid loss 4.0338 | ppl 56.48\n",
      "[104] train loss 1.3756 | ppl 3.96  ||  valid loss 4.0130 | ppl 55.31\n",
      "[105] train loss 1.3756 | ppl 3.96  ||  valid loss 4.0160 | ppl 55.48\n",
      "[106] train loss 1.3750 | ppl 3.96  ||  valid loss 4.0130 | ppl 55.31\n",
      "[107] train loss 1.3732 | ppl 3.95  ||  valid loss 4.0203 | ppl 55.72\n",
      "[108] train loss 1.3712 | ppl 3.94  ||  valid loss 4.0226 | ppl 55.85\n",
      "[109] train loss 1.3710 | ppl 3.94  ||  valid loss 3.9993 | ppl 54.56\n",
      "[110] train loss 1.3703 | ppl 3.94  ||  valid loss 4.0105 | ppl 55.17\n",
      "[111] train loss 1.3687 | ppl 3.93  ||  valid loss 4.0205 | ppl 55.73\n",
      "[112] train loss 1.3679 | ppl 3.93  ||  valid loss 4.0129 | ppl 55.31\n",
      "[113] train loss 1.3670 | ppl 3.92  ||  valid loss 3.9867 | ppl 53.87\n",
      "[114] train loss 1.3658 | ppl 3.92  ||  valid loss 4.0027 | ppl 54.74\n",
      "[115] train loss 1.3652 | ppl 3.92  ||  valid loss 3.9914 | ppl 54.13\n",
      "[116] train loss 1.3638 | ppl 3.91  ||  valid loss 4.0033 | ppl 54.78\n",
      "[117] train loss 1.3636 | ppl 3.91  ||  valid loss 3.9916 | ppl 54.14\n",
      "[118] train loss 1.3617 | ppl 3.90  ||  valid loss 3.9852 | ppl 53.79\n",
      "[119] train loss 1.3593 | ppl 3.89  ||  valid loss 3.9962 | ppl 54.39\n",
      "[120] train loss 1.3599 | ppl 3.90  ||  valid loss 3.9803 | ppl 53.53\n",
      "[121] train loss 1.3585 | ppl 3.89  ||  valid loss 3.9840 | ppl 53.73\n",
      "[122] train loss 1.3579 | ppl 3.89  ||  valid loss 3.9819 | ppl 53.62\n",
      "[123] train loss 1.3571 | ppl 3.88  ||  valid loss 3.9944 | ppl 54.29\n",
      "[124] train loss 1.3564 | ppl 3.88  ||  valid loss 3.9804 | ppl 53.54\n",
      "[125] train loss 1.3556 | ppl 3.88  ||  valid loss 3.9643 | ppl 52.68\n",
      "[126] train loss 1.3531 | ppl 3.87  ||  valid loss 3.9662 | ppl 52.78\n",
      "[127] train loss 1.3524 | ppl 3.87  ||  valid loss 3.9687 | ppl 52.91\n",
      "[128] train loss 1.3531 | ppl 3.87  ||  valid loss 3.9732 | ppl 53.15\n",
      "[129] train loss 1.3515 | ppl 3.86  ||  valid loss 3.9857 | ppl 53.82\n",
      "[130] train loss 1.3519 | ppl 3.86  ||  valid loss 3.9497 | ppl 51.92\n",
      "[131] train loss 1.3510 | ppl 3.86  ||  valid loss 3.9370 | ppl 51.27\n",
      "[132] train loss 1.3505 | ppl 3.86  ||  valid loss 3.9800 | ppl 53.52\n",
      "[133] train loss 1.3479 | ppl 3.85  ||  valid loss 3.9531 | ppl 52.10\n",
      "[134] train loss 1.3490 | ppl 3.85  ||  valid loss 3.9492 | ppl 51.89\n",
      "[135] train loss 1.3476 | ppl 3.85  ||  valid loss 3.9425 | ppl 51.55\n",
      "[136] train loss 1.3471 | ppl 3.85  ||  valid loss 3.9465 | ppl 51.75\n",
      "[137] train loss 1.3463 | ppl 3.84  ||  valid loss 3.9371 | ppl 51.27\n",
      "[138] train loss 1.3452 | ppl 3.84  ||  valid loss 3.9328 | ppl 51.05\n",
      "[139] train loss 1.3459 | ppl 3.84  ||  valid loss 3.9452 | ppl 51.69\n",
      "[140] train loss 1.3436 | ppl 3.83  ||  valid loss 3.9449 | ppl 51.67\n",
      "[141] train loss 1.3440 | ppl 3.83  ||  valid loss 3.9504 | ppl 51.96\n",
      "[142] train loss 1.3429 | ppl 3.83  ||  valid loss 3.9410 | ppl 51.47\n",
      "[143] train loss 1.3427 | ppl 3.83  ||  valid loss 3.9480 | ppl 51.83\n",
      "[144] train loss 1.3419 | ppl 3.83  ||  valid loss 3.9419 | ppl 51.52\n",
      "[145] train loss 1.3410 | ppl 3.82  ||  valid loss 3.9231 | ppl 50.56\n",
      "[146] train loss 1.3402 | ppl 3.82  ||  valid loss 3.9384 | ppl 51.34\n",
      "[147] train loss 1.3405 | ppl 3.82  ||  valid loss 3.9464 | ppl 51.75\n",
      "[148] train loss 1.3401 | ppl 3.82  ||  valid loss 3.9446 | ppl 51.65\n",
      "[149] train loss 1.3395 | ppl 3.82  ||  valid loss 3.9370 | ppl 51.26\n",
      "[150] train loss 1.3382 | ppl 3.81  ||  valid loss 3.9242 | ppl 50.61\n",
      "학습 종료. Best valid loss: 3.9230739011160156\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8) 학습 루프\n",
    "#   - Teacher Forcing: 디코더 입력은 \"정답의 이전 토큰\"\n",
    "#   - PAD 무시, Label Smoothing, Gradient Clipping\n",
    "#   - ppl(Perplexity) = exp(토큰당 평균 손실)\n",
    "# ------------------------------------------------------------\n",
    "def run_epoch(dataloader, train=True):\n",
    "    model.train(train)\n",
    "    total_loss, total_tokens = 0.0, 0\n",
    "\n",
    "    for src, tgt_in, tgt_out in dataloader:\n",
    "        # DataLoader는 (batch, seq) → (seq, batch)로 전치 후 장치로 이동\n",
    "        src = to_seq_first(src).to(DEVICE)      # (S, B)\n",
    "        tgt_in = to_seq_first(tgt_in).to(DEVICE)  # (T, B)\n",
    "        tgt_out = to_seq_first(tgt_out).to(DEVICE)  # (T, B)\n",
    "\n",
    "        # key_padding_mask 생성: (B, seq), True=PAD=가려라\n",
    "        src_kpm = make_padding_mask(src.transpose(0, 1)).to(DEVICE)     # (B, S)\n",
    "        tgt_kpm = make_padding_mask(tgt_in.transpose(0, 1)).to(DEVICE)  # (B, T)\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            logits = model(src, tgt_in, src_kpm, tgt_kpm, src_kpm)  # (T, B, V)\n",
    "\n",
    "            # CE 입력 형태: (N, C) vs (N,)\n",
    "            loss = criterion(\n",
    "                logits.view(-1, logits.size(-1)),  # (T*B, V)\n",
    "                tgt_out.reshape(-1)                # (T*B,)\n",
    "            )\n",
    "            if train:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                # 그래디언트 폭주 방지\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "        # PAD를 제외한 실제 토큰 수로 가중 평균 손실 계산\n",
    "        n_tokens = (tgt_out != PAD_ID).sum().item()\n",
    "        total_loss += loss.item() * n_tokens\n",
    "        total_tokens += n_tokens\n",
    "\n",
    "    avg_loss = total_loss / max(1, total_tokens)\n",
    "    ppl = math.exp(avg_loss)\n",
    "    return avg_loss, ppl\n",
    "\n",
    "EPOCHS = 150\n",
    "# 에폭 조정에 따라 답변 성능이 달라짐\n",
    "# 10 > 기본적인 인사만 할 수 있음 (ex 안녕하세요.)\n",
    "# 50 > 상태의 대한 질문에 공감을 할 수 있음 (ex 싸우면서 정 들 거예요. 아무 생각 하지 말고 푹 주무세요.)\n",
    "# 100 > 채팅봇 본인의 상태에 대한 대답을 할 수 있음 (ex 위로봇이요. 저는 배터리가 밥이예요.)\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "ckpt_path = ARTIFACTS / \"best_transformer.pt\"\n",
    "\n",
    "print(\"학습 시작...\")\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_loss, tr_ppl = run_epoch(train_loader, train=True)\n",
    "    va_loss, va_ppl = run_epoch(valid_loader, train=False)\n",
    "    print(f\"[{epoch:02d}] train loss {tr_loss:.4f} | ppl {tr_ppl:.2f}  ||  valid loss {va_loss:.4f} | ppl {va_ppl:.2f}\")\n",
    "\n",
    "    # 검증 손실이 가장 낮은 시점의 가중치 저장(조기중단 대용)\n",
    "    if va_loss < best_val:\n",
    "        best_val = va_loss\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "\n",
    "print(\"학습 종료. Best valid loss:\", best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bbc4acc-78ef-4d49-be6f-fd986ebc76ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "베스트 체크포인트 로드 완료: artifacts/best_transformer.pt\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 9) 추론(디코딩) - Greedy\n",
    "#   - 인코더: 질문 문맥 벡터화\n",
    "#   - 디코더: [BOS]로 시작 → 한 토큰씩 argmax로 이어 붙임\n",
    "#   - 종료 조건: EOS 생성 or 최대 길이 도달\n",
    "#   - 한계: 단순/빠르지만 반복/짧은 답 위험 → 필요 시 빔/탑-p로 확장\n",
    "# ------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def greedy_decode(question: str, max_len=MAX_TOKENS):\n",
    "    model.eval()\n",
    "\n",
    "    # (1) 전처리/토큰화: 학습과 \"동일\" 규칙 적용\n",
    "    q_norm = normalize_text(question)\n",
    "    src_ids = [BOS_ID] + sp.encode(q_norm, out_type=int)[:max_len-2] + [EOS_ID]\n",
    "    src = torch.tensor(src_ids, dtype=torch.long).unsqueeze(1).to(DEVICE)  # (S, 1)\n",
    "\n",
    "    # key_padding_mask: (B=1, S), True=PAD\n",
    "    src_kpm = make_padding_mask(src.transpose(0, 1)).to(DEVICE)\n",
    "\n",
    "    # (2) 디코더 시작 토큰: BOS (T=1, B=1)\n",
    "    ys = torch.tensor([BOS_ID], dtype=torch.long, device=DEVICE).unsqueeze(1)\n",
    "\n",
    "    # (3) 한 토큰씩 생성\n",
    "    for _ in range(max_len - 1):\n",
    "        tgt_kpm = make_padding_mask(ys.transpose(0, 1)).to(DEVICE)  # (1, T)\n",
    "        logits = model(src, ys, src_kpm, tgt_kpm, src_kpm)          # (T, 1, V)\n",
    "        next_token = logits[-1, 0].argmax(-1).item()                # 마지막 스텝의 분포에서 argmax\n",
    "        ys = torch.cat([ys, torch.tensor([[next_token]], device=DEVICE)], dim=0)\n",
    "        if next_token == EOS_ID:\n",
    "            break\n",
    "\n",
    "    # (4) BOS 제거, EOS 이전까지만 디코드\n",
    "    out_ids = ys.squeeze(1).tolist()[1:]\n",
    "    if EOS_ID in out_ids:\n",
    "        out_ids = out_ids[:out_ids.index(EOS_ID)]\n",
    "    return sp.decode(out_ids)\n",
    "\n",
    "# 학습된 베스트 체크포인트 로드(가능할 때만)\n",
    "if ckpt_path.exists():\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
    "    print(\"베스트 체크포인트 로드 완료:\", ckpt_path)\n",
    "else:\n",
    "    print(\"체크포인트가 없어 현재 가중치로 추론합니다(테스트 목적).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecc56d3d-527c-4ba8-a8c4-9cf7e522aefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 질의 결과 ===\n",
      "Q: 안녕하세요\n",
      "A: 안녕하세요.\n",
      "------------------------------------------------------------\n",
      "Q: 너 이름이 뭐야?\n",
      "A: 저는 위로봇입니다.\n",
      "------------------------------------------------------------\n",
      "Q: 오늘 너무 피곤하다\n",
      "A: 아무 생각 하지 말고 푹 주무세요.\n",
      "------------------------------------------------------------\n",
      "Q: 여자친구랑 싸웠어\n",
      "A: 싸우면서 정 들 거예요.\n",
      "------------------------------------------------------------\n",
      "Q: 공부하기 싫어\n",
      "A: 잠시 쉬어도 돼요.\n",
      "------------------------------------------------------------\n",
      "Q: 1+1이 뭐야?\n",
      "A: 저도 해보고 싶은 알바예요.\n",
      "------------------------------------------------------------\n",
      "Q: 양자역학에 대해서 설명해줘\n",
      "A: 게 대화를 이어나가는게 좋겠어요.\n",
      "------------------------------------------------------------\n",
      "Q: 밥 먹었어?\n",
      "A: 저는 배터리가 밥이예요.\n",
      "------------------------------------------------------------\n",
      "Q: 너 똑똑해?\n",
      "A: 저는 위로해드리는 로봇이에요.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 10) 샘플 질의 테스트\n",
    "# ------------------------------------------------------------\n",
    "samples = [\n",
    "    \"안녕하세요\",\n",
    "    \"너 이름이 뭐야?\",\n",
    "    \"오늘 너무 피곤하다\",\n",
    "    \"여자친구랑 싸웠어\",\n",
    "    \"공부하기 싫어\",\n",
    "    \"1+1이 뭐야?\", # 연산을 학습하지 않았으므로 올바른 답변을 하지 못함\n",
    "    \"양자역학에 대해서 설명해줘\", # 일상 대화가 아닌 전문지식은 올바른 답변을 하지 못함\n",
    "    \"밥 먹었어?\",\n",
    "    \"너 똑똑해?\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== 샘플 질의 결과 ===\")\n",
    "for q in samples:\n",
    "    a = greedy_decode(q)\n",
    "    print(\"Q:\", q)\n",
    "    print(\"A:\", a)\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# ============================================================\n",
    "# [요약]\n",
    "# - 전처리: 학습/추론 동일 규칙 유지(중요)\n",
    "# - SentencePiece: 공동 코퍼스, 특수토큰 ID 고정\n",
    "# - DataLoader: (batch, seq) → (seq, batch) 전치\n",
    "# - key_padding_mask: (batch, seq), True=PAD=무시\n",
    "# - 디코더: tgt_mask(미래 가림) 반드시 적용\n",
    "# - 타깃 시프트: tgt_in = [BOS]+A  / tgt_out = A+[EOS]\n",
    "# - Loss: PAD 무시 + label_smoothing\n",
    "# - 추론: model.eval() + @torch.no_grad()\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84090f96-3fe1-4616-be49-6381034a1fec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
