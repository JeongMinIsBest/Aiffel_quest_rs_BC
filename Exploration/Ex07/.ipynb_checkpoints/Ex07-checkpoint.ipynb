{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "384b1a2b-dbbd-413b-86a4-13b11c1a1d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c2a8b3-644c-47be-a88a-acfa620e5e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece torch torchvision --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "444579bb-38e1-4743-9610-b716d35148e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Q            A  label\n",
      "0           12시 땡!   하루가 또 가네요.      0\n",
      "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
      "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "4          PPL 심하네   눈살이 찌푸려지죠.      0\n",
      "(11823, 3)\n"
     ]
    }
   ],
   "source": [
    "import os, re, unicodedata, random, math\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "URL = \"https://github.com/songys/Chatbot_data/raw/master/ChatbotData.csv\"\n",
    "df = pd.read_csv(URL)\n",
    "\n",
    "# 데이터 컬럼: Q(질문), A(답변), label(의도 분류용 정답, 여기서는 사용하지 않음)\n",
    "print(df.head())\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eea45e0a-538a-4756-8a27-a969c504c8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: (11750, 5)\n",
      "11162 588\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(s: str) -> str:\n",
    "    # 1) NFKC 정규화\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s))\n",
    "    # 2) 허용 문자만 남기기 (한글, 영문, 숫자, 기본 문장부호)\n",
    "    #    필요시 허용 문자 세트를 조정하세요.\n",
    "    s = re.sub(r\"[^0-9A-Za-z가-힣ㄱ-ㅎㅏ-ㅣ\\s.,?!~’'\\\"()\\-\\:;@/]\", \" \", s)\n",
    "    # 3) 공백 정리\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "df = df.dropna(subset=[\"Q\",\"A\"]).copy()\n",
    "df[\"Q_orig\"] = df[\"Q\"]\n",
    "df[\"A_orig\"] = df[\"A\"]\n",
    "df[\"Q\"] = df[\"Q\"].map(normalize_text)\n",
    "df[\"A\"] = df[\"A\"].map(normalize_text)\n",
    "\n",
    "# 너무 짧거나 너무 긴 샘플 제거 (길이 기준은 자유롭게 조정)\n",
    "MIN_CHARS, MAX_CHARS = 1, 128\n",
    "mask = (\n",
    "    df[\"Q\"].str.len().between(MIN_CHARS, MAX_CHARS) &\n",
    "    df[\"A\"].str.len().between(MIN_CHARS, MAX_CHARS)\n",
    ")\n",
    "df = df[mask].drop_duplicates(subset=[\"Q\",\"A\"]).reset_index(drop=True)\n",
    "print(\"After cleaning:\", df.shape)\n",
    "\n",
    "# 학습/검증 분리\n",
    "train_df, valid_df = train_test_split(df[[\"Q\",\"A\"]], test_size=0.05, random_state=42, shuffle=True)\n",
    "print(len(train_df), len(valid_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb83a85b-c5ac-4c8c-b685-262b6e2a71ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8000 PAD/BOS/EOS: 0 2 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: artifacts/spm_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: artifacts/spm_ko\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: [UNK]\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: [PAD]\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: artifacts/spm_corpus.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 22324 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: [PAD]\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: [UNK]\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=333945\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.9506% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=1078\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.999506\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 22324 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=147897\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 18260 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 22324\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 21093\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 21093 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10563 obj=13.2275 num_tokens=44914 num_tokens/piece=4.25201\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9322 obj=12.1264 num_tokens=45052 num_tokens/piece=4.83287\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8795 obj=12.2154 num_tokens=45539 num_tokens/piece=5.17783\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8761 obj=12.1969 num_tokens=45597 num_tokens/piece=5.20454\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: artifacts/spm_ko.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: artifacts/spm_ko.vocab\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import sentencepiece as spm\n",
    "\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "\n",
    "# SentencePiece 학습용 말뭉치 파일 생성 (질문/답변 합치기)\n",
    "corpus_path = \"artifacts/spm_corpus.txt\"\n",
    "with io.open(corpus_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for s in pd.concat([train_df[\"Q\"], train_df[\"A\"]], axis=0).astype(str):\n",
    "        if s: f.write(s + \"\\n\")\n",
    "\n",
    "spm_model_prefix = \"artifacts/spm_ko\"\n",
    "VOCAB_SIZE = 8000  # 데이터 크기를 고려해 4k~16k 사이 추천\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=corpus_path,\n",
    "    model_prefix=spm_model_prefix,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    model_type=\"unigram\",            # \"bpe\"도 가능\n",
    "    character_coverage=0.9995,       # 한/영 혼용 문서에 적합\n",
    "    # 고정 special token IDs (PyTorch에서 쓰기 좋게)\n",
    "    pad_id=0, unk_id=1, bos_id=2, eos_id=3,\n",
    "    pad_piece=\"[PAD]\", unk_piece=\"[UNK]\", bos_piece=\"<s>\", eos_piece=\"</s>\",\n",
    "    user_defined_symbols=[],\n",
    ")\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(f\"{spm_model_prefix}.model\")\n",
    "\n",
    "PAD_ID = sp.pad_id()   # 0\n",
    "UNK_ID = sp.unk_id()   # 1\n",
    "BOS_ID = sp.bos_id()   # 2\n",
    "EOS_ID = sp.eos_id()   # 3\n",
    "\n",
    "print(\"Vocab size:\", sp.get_piece_size(), \"PAD/BOS/EOS:\", PAD_ID, BOS_ID, EOS_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4b75296-5269-48d9-a1c2-1fa306b4002f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "MAX_TOKENS = 64  # 최대 서브워드 길이 (질문/답변 모두 동일 제한)\n",
    "\n",
    "def encode_text(s: str, sp, max_len=MAX_TOKENS):\n",
    "    ids = sp.encode(s, out_type=int)\n",
    "    # 너무 길면 자르기 (eos 고려해서 -1)\n",
    "    ids = ids[:max_len-2]  # BOS/EOS용 자리\n",
    "    return ids\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, frame, sp, max_len=MAX_TOKENS):\n",
    "        self.q = frame[\"Q\"].tolist()\n",
    "        self.a = frame[\"A\"].tolist()\n",
    "        self.sp = sp\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self): return len(self.q)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        src = encode_text(self.q[i], self.sp, self.max_len)\n",
    "        tgt = encode_text(self.a[i], self.sp, self.max_len)\n",
    "        # 입력/출력 시퀀스 만들기\n",
    "        src_ids = [BOS_ID] + src + [EOS_ID]\n",
    "        tgt_in  = [BOS_ID] + tgt\n",
    "        tgt_out = tgt + [EOS_ID]\n",
    "        return torch.tensor(src_ids), torch.tensor(tgt_in), torch.tensor(tgt_out)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    srcs, tgts_in, tgts_out = zip(*batch)\n",
    "    def pad_to_max(seqs):\n",
    "        maxlen = max(x.size(0) for x in seqs)\n",
    "        padded = torch.full((len(seqs), maxlen), PAD_ID, dtype=torch.long)\n",
    "        for i, s in enumerate(seqs):\n",
    "            padded[i, :s.size(0)] = s\n",
    "        return padded\n",
    "    return pad_to_max(srcs), pad_to_max(tgts_in), pad_to_max(tgts_out)\n",
    "\n",
    "train_ds = ChatDataset(train_df, sp, MAX_TOKENS)\n",
    "valid_ds = ChatDataset(valid_df, sp, MAX_TOKENS)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=128, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "#########################################\n",
    "# Positional Encoding (sine/cosine)\n",
    "#########################################\n",
    "import math\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(1))  # (max_len, 1, d_model)\n",
    "\n",
    "    def forward(self, x): # x: (seq, batch, d_model)\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "#########################################\n",
    "# Transformer Seq2Seq\n",
    "#########################################\n",
    "class TransformerChatbot(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=4, dim_ff=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n",
    "        self.pos_enc = PositionalEncoding(d_model, dropout=dropout)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_ff, dropout=dropout,\n",
    "            batch_first=False  # PyTorch 기본: (seq, batch, dim)\n",
    "        )\n",
    "        self.generator = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt_in, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask):\n",
    "        # src/tgt_in: (seq_len, batch)\n",
    "        src = self.tok_emb(src) * math.sqrt(self.tok_emb.embedding_dim)\n",
    "        tgt = self.tok_emb(tgt_in) * math.sqrt(self.tok_emb.embedding_dim)\n",
    "\n",
    "        src = self.pos_enc(src)\n",
    "        tgt = self.pos_enc(tgt)\n",
    "\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(0)).to(src.device)\n",
    "        out = self.transformer(\n",
    "            src=src, tgt=tgt,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "        logits = self.generator(out)  # (seq, batch, vocab)\n",
    "        return logits\n",
    "\n",
    "VOCAB = sp.get_piece_size()\n",
    "model = TransformerChatbot(VOCAB).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID, label_smoothing=0.1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.98), eps=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92703d8a-5731-418c-8f76-7b6eb49c65a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_padding_mask(batch_ids):  # (batch, seq) → (batch, seq) boolean\n",
    "    return (batch_ids == PAD_ID)\n",
    "\n",
    "def to_seq_first(x):  # (batch, seq) → (seq, batch)\n",
    "    return x.transpose(0, 1).contiguous()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "892e1e4b-27a8-4493-8678-5ddb25db564e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] train loss 2.2193 | ppl 9.20  ||  valid loss 4.0000 | ppl 54.60\n",
      "[02] train loss 2.1635 | ppl 8.70  ||  valid loss 4.0130 | ppl 55.31\n",
      "[03] train loss 2.1091 | ppl 8.24  ||  valid loss 4.0251 | ppl 55.98\n",
      "[04] train loss 2.0541 | ppl 7.80  ||  valid loss 4.0081 | ppl 55.04\n",
      "[05] train loss 2.0071 | ppl 7.44  ||  valid loss 4.0348 | ppl 56.53\n",
      "[06] train loss 1.9574 | ppl 7.08  ||  valid loss 4.0194 | ppl 55.67\n",
      "[07] train loss 1.9161 | ppl 6.79  ||  valid loss 4.0543 | ppl 57.65\n",
      "[08] train loss 1.8736 | ppl 6.51  ||  valid loss 4.0303 | ppl 56.28\n",
      "[09] train loss 1.8361 | ppl 6.27  ||  valid loss 4.0588 | ppl 57.90\n",
      "[10] train loss 1.8003 | ppl 6.05  ||  valid loss 4.0780 | ppl 59.03\n",
      "[11] train loss 1.7668 | ppl 5.85  ||  valid loss 4.0941 | ppl 59.99\n",
      "[12] train loss 1.7431 | ppl 5.72  ||  valid loss 4.0851 | ppl 59.45\n",
      "[13] train loss 1.7101 | ppl 5.53  ||  valid loss 4.1026 | ppl 60.49\n",
      "[14] train loss 1.6876 | ppl 5.41  ||  valid loss 4.1207 | ppl 61.60\n",
      "[15] train loss 1.6639 | ppl 5.28  ||  valid loss 4.1155 | ppl 61.28\n",
      "[16] train loss 1.6408 | ppl 5.16  ||  valid loss 4.1262 | ppl 61.94\n",
      "[17] train loss 1.6253 | ppl 5.08  ||  valid loss 4.1393 | ppl 62.76\n",
      "[18] train loss 1.6070 | ppl 4.99  ||  valid loss 4.1556 | ppl 63.79\n",
      "[19] train loss 1.5907 | ppl 4.91  ||  valid loss 4.1560 | ppl 63.82\n",
      "[20] train loss 1.5776 | ppl 4.84  ||  valid loss 4.1588 | ppl 64.00\n",
      "[21] train loss 1.5662 | ppl 4.79  ||  valid loss 4.1533 | ppl 63.65\n",
      "[22] train loss 1.5555 | ppl 4.74  ||  valid loss 4.1638 | ppl 64.32\n",
      "[23] train loss 1.5430 | ppl 4.68  ||  valid loss 4.1636 | ppl 64.30\n",
      "[24] train loss 1.5359 | ppl 4.65  ||  valid loss 4.1741 | ppl 64.98\n",
      "[25] train loss 1.5272 | ppl 4.61  ||  valid loss 4.1671 | ppl 64.53\n",
      "[26] train loss 1.5180 | ppl 4.56  ||  valid loss 4.1735 | ppl 64.94\n",
      "[27] train loss 1.5116 | ppl 4.53  ||  valid loss 4.1635 | ppl 64.30\n",
      "[28] train loss 1.5052 | ppl 4.50  ||  valid loss 4.1628 | ppl 64.25\n",
      "[29] train loss 1.4959 | ppl 4.46  ||  valid loss 4.1814 | ppl 65.46\n",
      "[30] train loss 1.4917 | ppl 4.44  ||  valid loss 4.1732 | ppl 64.92\n",
      "[31] train loss 1.4841 | ppl 4.41  ||  valid loss 4.1509 | ppl 63.49\n",
      "[32] train loss 1.4802 | ppl 4.39  ||  valid loss 4.1459 | ppl 63.17\n",
      "[33] train loss 1.4750 | ppl 4.37  ||  valid loss 4.1616 | ppl 64.17\n",
      "[34] train loss 1.4684 | ppl 4.34  ||  valid loss 4.1582 | ppl 63.96\n",
      "[35] train loss 1.4684 | ppl 4.34  ||  valid loss 4.1528 | ppl 63.61\n",
      "[36] train loss 1.4621 | ppl 4.32  ||  valid loss 4.1612 | ppl 64.15\n",
      "[37] train loss 1.4551 | ppl 4.28  ||  valid loss 4.1357 | ppl 62.54\n",
      "[38] train loss 1.4570 | ppl 4.29  ||  valid loss 4.1506 | ppl 63.47\n",
      "[39] train loss 1.4494 | ppl 4.26  ||  valid loss 4.1291 | ppl 62.12\n",
      "[40] train loss 1.4473 | ppl 4.25  ||  valid loss 4.1229 | ppl 61.74\n",
      "[41] train loss 1.4421 | ppl 4.23  ||  valid loss 4.1435 | ppl 63.02\n",
      "[42] train loss 1.4413 | ppl 4.23  ||  valid loss 4.1334 | ppl 62.39\n",
      "[43] train loss 1.4355 | ppl 4.20  ||  valid loss 4.1192 | ppl 61.51\n",
      "[44] train loss 1.4329 | ppl 4.19  ||  valid loss 4.1123 | ppl 61.08\n",
      "[45] train loss 1.4312 | ppl 4.18  ||  valid loss 4.0919 | ppl 59.86\n",
      "[46] train loss 1.4275 | ppl 4.17  ||  valid loss 4.1061 | ppl 60.71\n",
      "[47] train loss 1.4241 | ppl 4.15  ||  valid loss 4.0834 | ppl 59.35\n",
      "[48] train loss 1.4220 | ppl 4.15  ||  valid loss 4.1045 | ppl 60.61\n",
      "[49] train loss 1.4191 | ppl 4.13  ||  valid loss 4.0802 | ppl 59.16\n",
      "[50] train loss 1.4160 | ppl 4.12  ||  valid loss 4.0761 | ppl 58.91\n",
      "[51] train loss 1.4148 | ppl 4.12  ||  valid loss 4.0819 | ppl 59.26\n",
      "[52] train loss 1.4134 | ppl 4.11  ||  valid loss 4.0792 | ppl 59.10\n",
      "[53] train loss 1.4101 | ppl 4.10  ||  valid loss 4.0610 | ppl 58.03\n",
      "[54] train loss 1.4088 | ppl 4.09  ||  valid loss 4.0682 | ppl 58.45\n",
      "[55] train loss 1.4075 | ppl 4.09  ||  valid loss 4.0798 | ppl 59.13\n",
      "[56] train loss 1.4028 | ppl 4.07  ||  valid loss 4.0602 | ppl 57.99\n",
      "[57] train loss 1.4029 | ppl 4.07  ||  valid loss 4.0517 | ppl 57.49\n",
      "[58] train loss 1.3978 | ppl 4.05  ||  valid loss 4.0582 | ppl 57.87\n",
      "[59] train loss 1.3981 | ppl 4.05  ||  valid loss 4.0485 | ppl 57.31\n",
      "[60] train loss 1.3969 | ppl 4.04  ||  valid loss 4.0459 | ppl 57.16\n",
      "[61] train loss 1.3941 | ppl 4.03  ||  valid loss 4.0432 | ppl 57.01\n",
      "[62] train loss 1.3918 | ppl 4.02  ||  valid loss 4.0228 | ppl 55.86\n",
      "[63] train loss 1.3919 | ppl 4.02  ||  valid loss 4.0341 | ppl 56.49\n",
      "[64] train loss 1.3891 | ppl 4.01  ||  valid loss 4.0380 | ppl 56.72\n",
      "[65] train loss 1.3865 | ppl 4.00  ||  valid loss 4.0284 | ppl 56.17\n",
      "[66] train loss 1.3848 | ppl 3.99  ||  valid loss 4.0342 | ppl 56.50\n",
      "[67] train loss 1.3844 | ppl 3.99  ||  valid loss 4.0232 | ppl 55.88\n",
      "[68] train loss 1.3827 | ppl 3.99  ||  valid loss 4.0225 | ppl 55.84\n",
      "[69] train loss 1.3804 | ppl 3.98  ||  valid loss 4.0116 | ppl 55.24\n",
      "[70] train loss 1.3812 | ppl 3.98  ||  valid loss 4.0192 | ppl 55.66\n",
      "[71] train loss 1.3788 | ppl 3.97  ||  valid loss 4.0180 | ppl 55.59\n",
      "[72] train loss 1.3771 | ppl 3.96  ||  valid loss 3.9934 | ppl 54.24\n",
      "[73] train loss 1.3764 | ppl 3.96  ||  valid loss 4.0152 | ppl 55.44\n",
      "[74] train loss 1.3759 | ppl 3.96  ||  valid loss 4.0131 | ppl 55.32\n",
      "[75] train loss 1.3745 | ppl 3.95  ||  valid loss 3.9997 | ppl 54.58\n",
      "[76] train loss 1.3716 | ppl 3.94  ||  valid loss 3.9996 | ppl 54.58\n",
      "[77] train loss 1.3719 | ppl 3.94  ||  valid loss 3.9942 | ppl 54.28\n",
      "[78] train loss 1.3699 | ppl 3.94  ||  valid loss 4.0134 | ppl 55.34\n",
      "[79] train loss 1.3693 | ppl 3.93  ||  valid loss 4.0039 | ppl 54.81\n",
      "[80] train loss 1.3678 | ppl 3.93  ||  valid loss 4.0026 | ppl 54.74\n",
      "[81] train loss 1.3677 | ppl 3.93  ||  valid loss 3.9896 | ppl 54.03\n",
      "[82] train loss 1.3666 | ppl 3.92  ||  valid loss 3.9766 | ppl 53.33\n",
      "[83] train loss 1.3652 | ppl 3.92  ||  valid loss 3.9865 | ppl 53.86\n",
      "[84] train loss 1.3639 | ppl 3.91  ||  valid loss 3.9842 | ppl 53.74\n",
      "[85] train loss 1.3627 | ppl 3.91  ||  valid loss 3.9756 | ppl 53.28\n",
      "[86] train loss 1.3614 | ppl 3.90  ||  valid loss 3.9643 | ppl 52.69\n",
      "[87] train loss 1.3614 | ppl 3.90  ||  valid loss 3.9722 | ppl 53.10\n",
      "[88] train loss 1.3613 | ppl 3.90  ||  valid loss 3.9754 | ppl 53.27\n",
      "[89] train loss 1.3589 | ppl 3.89  ||  valid loss 3.9797 | ppl 53.50\n",
      "[90] train loss 1.3573 | ppl 3.89  ||  valid loss 3.9611 | ppl 52.52\n",
      "[91] train loss 1.3589 | ppl 3.89  ||  valid loss 3.9666 | ppl 52.80\n",
      "[92] train loss 1.3585 | ppl 3.89  ||  valid loss 3.9650 | ppl 52.72\n",
      "[93] train loss 1.3559 | ppl 3.88  ||  valid loss 3.9772 | ppl 53.37\n",
      "[94] train loss 1.3555 | ppl 3.88  ||  valid loss 3.9734 | ppl 53.16\n",
      "[95] train loss 1.3541 | ppl 3.87  ||  valid loss 3.9696 | ppl 52.96\n",
      "[96] train loss 1.3542 | ppl 3.87  ||  valid loss 3.9669 | ppl 52.82\n",
      "[97] train loss 1.3532 | ppl 3.87  ||  valid loss 3.9634 | ppl 52.64\n",
      "[98] train loss 1.3512 | ppl 3.86  ||  valid loss 3.9704 | ppl 53.01\n",
      "[99] train loss 1.3512 | ppl 3.86  ||  valid loss 3.9479 | ppl 51.83\n",
      "[100] train loss 1.3512 | ppl 3.86  ||  valid loss 3.9660 | ppl 52.78\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def run_epoch(dataloader, train=True):\n",
    "    model.train(train)\n",
    "    total_loss, total_tokens = 0.0, 0\n",
    "    for src, tgt_in, tgt_out in dataloader:\n",
    "        # batch, seq → seq, batch\n",
    "        src = to_seq_first(src).to(DEVICE)\n",
    "        tgt_in = to_seq_first(tgt_in).to(DEVICE)\n",
    "        tgt_out = to_seq_first(tgt_out).to(DEVICE)\n",
    "\n",
    "        # key_padding_mask: (batch, seq)\n",
    "        src_kpm = make_padding_mask(src.transpose(0,1)).to(DEVICE)\n",
    "        tgt_kpm = make_padding_mask(tgt_in.transpose(0,1)).to(DEVICE)\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            logits = model(src, tgt_in, src_kpm, tgt_kpm, src_kpm)  # (seq, batch, vocab)\n",
    "            loss = criterion(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                tgt_out.reshape(-1)\n",
    "            )\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "        n_tokens = (tgt_out != PAD_ID).sum().item()\n",
    "        total_loss += loss.item() * n_tokens\n",
    "        total_tokens += n_tokens\n",
    "\n",
    "    ppl = math.exp(total_loss / max(1, total_tokens))\n",
    "    return total_loss / max(1, total_tokens), ppl\n",
    "\n",
    "EPOCHS = 100 \n",
    "# 에폭 조정에 따라 답변 성능이 달라짐\n",
    "# 10 > 기본적인 인사만 할 수 있음 (ex 안녕하세요.)\n",
    "# 50 > 상태의 대한 질문에 공감을 할 수 있음 (ex 싸우면서 정 들 거예요. 잠시 쉬어도 돼요.)\n",
    "# 100 > 채팅봇 본인의 상태에 대한 대답을 할 수 있음 (ex 위로봇이요. 저는 배터리가 밥이예요.)\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_ppl = run_epoch(train_loader, train=True)\n",
    "    va_loss, va_ppl = run_epoch(valid_loader, train=False)\n",
    "    print(f\"[{epoch:02d}] train loss {tr_loss:.4f} | ppl {tr_ppl:.2f}  ||  valid loss {va_loss:.4f} | ppl {va_ppl:.2f}\")\n",
    "    if va_loss < best_val:\n",
    "        best_val = va_loss\n",
    "        torch.save(model.state_dict(), \"artifacts/best_transformer.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d286ff3a-f360-4b96-abf4-26af9c963651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 안녕하세요\n",
      "A: 안녕하세요.\n",
      "--------------------------------------------------\n",
      "Q: 너 이름이 뭐야?\n",
      "A: 위로봇이요.\n",
      "--------------------------------------------------\n",
      "Q: 오늘 너무 피곤하다\n",
      "A: 맛있는거 드세요.\n",
      "--------------------------------------------------\n",
      "Q: 여자친구랑 싸웠어\n",
      "A: 싸우면서 정 들 거예요.\n",
      "--------------------------------------------------\n",
      "Q: 공부하기 싫어\n",
      "A: 잠시 쉬어도 돼요.\n",
      "--------------------------------------------------\n",
      "Q: 1+1이 뭐야?\n",
      "A: 잘 이겨내고 있네요.\n",
      "--------------------------------------------------\n",
      "Q: 수련이 필요해\n",
      "A: 지금보다 더 잘 살 거예요.\n",
      "--------------------------------------------------\n",
      "Q: 밥 먹었어?\n",
      "A: 저는 배터리가 밥이예요.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_decode(question: str, max_len=MAX_TOKENS):\n",
    "    model.eval()\n",
    "    # 인코더 입력\n",
    "    src_ids = [BOS_ID] + sp.encode(normalize_text(question), out_type=int)[:max_len-2] + [EOS_ID]\n",
    "    src = torch.tensor(src_ids, dtype=torch.long).unsqueeze(1).to(DEVICE)  # (seq, 1)\n",
    "    src_kpm = make_padding_mask(src.transpose(0,1))  # (1, seq)\n",
    "    src_kpm = src_kpm.to(DEVICE)\n",
    "\n",
    "    # 디코더 입력 시작 (BOS)\n",
    "    ys = torch.tensor([BOS_ID], dtype=torch.long, device=DEVICE).unsqueeze(1)  # (1, 1)\n",
    "\n",
    "    for _ in range(max_len-1):\n",
    "        tgt_kpm = make_padding_mask(ys.transpose(0,1))\n",
    "        tgt_kpm = tgt_kpm.to(DEVICE)\n",
    "\n",
    "        logits = model(src, ys, src_kpm, tgt_kpm, src_kpm)  # (tgt_seq, 1, vocab)\n",
    "        next_token = logits[-1, 0].argmax(-1).item()\n",
    "        ys = torch.cat([ys, torch.tensor([[next_token]], device=DEVICE)], dim=0)\n",
    "        if next_token == EOS_ID: break\n",
    "\n",
    "    out_ids = ys.squeeze(1).tolist()[1:]  # BOS 제거\n",
    "    # EOS 이전까지만\n",
    "    if EOS_ID in out_ids:\n",
    "        out_ids = out_ids[:out_ids.index(EOS_ID)]\n",
    "    return sp.decode(out_ids)\n",
    "\n",
    "# 학습된 베스트 모델 로드(가장 좋은 검증 손실)\n",
    "model.load_state_dict(torch.load(\"artifacts/best_transformer.pt\", map_location=DEVICE))\n",
    "\n",
    "# 샘플 질의 테스트\n",
    "samples = [\n",
    "    \"안녕하세요\",\n",
    "    \"너 이름이 뭐야?\",\n",
    "    \"오늘 너무 피곤하다\",\n",
    "    \"여자친구랑 싸웠어\",\n",
    "    \"공부하기 싫어\",\n",
    "    \"1+1이 뭐야?\", # 연산을 학습하지 않았으므로 올바른 답변을 하지 못함\n",
    "    \"수련이 필요해\", # 일상에서 많이 쓰이지 않는 단어들은 잘 답변하지 못함\n",
    "    \"밥 먹었어?\"\n",
    "]\n",
    "for q in samples:\n",
    "    print(\"Q:\", q)\n",
    "    print(\"A:\", greedy_decode(q))\n",
    "    print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15749ea0-8e3e-492d-bbf3-db17894f4acd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
