{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09cebbf0-bb40-402c-bc9c-78aff5a76ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch        : 2.7.1+cu118\n",
      "numpy        : 2.2.6\n",
      "matplotlib   : 3.10.3\n",
      "konlpy       : 0.6.0\n"
     ]
    }
   ],
   "source": [
    "# ===== 0) Library versions =====\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import konlpy\n",
    "\n",
    "print(\"torch        :\", torch.__version__)\n",
    "print(\"numpy        :\", np.__version__)\n",
    "print(\"matplotlib   :\", matplotlib.__version__)\n",
    "print(\"konlpy       :\", konlpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0973994-864e-4d39-8912-084440237324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "# ===== 1) Install =====\n",
    "\n",
    "# 재현성 고정\n",
    "import random, os\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e97b309-16f8-4822-b470-f7771007b24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 크기 : 146182 {0: 73342, 1: 72840}\n",
      "테스트 데이터 크기 : 49157 {1: 24711, 0: 24446}\n",
      "Train: 131563 Val: 14619 Test: 49157\n"
     ]
    }
   ],
   "source": [
    "# ===== 2) NSMC 데이터 준비 =====\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "DATA_DIR = Path(\"./data_nsmc\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_path = DATA_DIR / \"ratings_train.txt\"\n",
    "test_path  = DATA_DIR / \"ratings_test.txt\"\n",
    "\n",
    "def _download_if_needed():\n",
    "    base = \"https://raw.githubusercontent.com/e9t/nsmc/master\"\n",
    "    files = [\n",
    "        (\"ratings_train.txt\", f\"{base}/ratings_train.txt\"),\n",
    "        (\"ratings_test.txt\",  f\"{base}/ratings_test.txt\"),\n",
    "    ]\n",
    "    for fname, url in files:\n",
    "        fpath = DATA_DIR / fname\n",
    "        if not fpath.exists():\n",
    "            try:\n",
    "                print(f\"다운 중 {fname} ...\")\n",
    "                urllib.request.urlretrieve(url, fpath.as_posix())\n",
    "            except Exception as e:\n",
    "                print(f\"다운 실패 {fname}: {e}\\n 파일 위치를 확인 {fpath}.\")\n",
    "\n",
    "_download_if_needed()\n",
    "\n",
    "# 로드\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "test_df  = pd.read_csv(test_path,  sep='\\t')\n",
    "\n",
    "# 결측/중복 제거 + 라벨/텍스트 정리\n",
    "def clean_df(df):\n",
    "    df = df.dropna(subset=['document']).copy()\n",
    "    df['document'] = df['document'].astype(str).str.strip()\n",
    "    df = df[df['document'].str.len() > 0]\n",
    "    df = df.drop_duplicates(subset=['document'])\n",
    "    # label은 0/1 (부정/긍정)\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    return df\n",
    "\n",
    "train_df = clean_df(train_df)\n",
    "test_df  = clean_df(test_df)\n",
    "\n",
    "# 간단 분포 확인\n",
    "print(\"학습 데이터 크기 :\", len(train_df), train_df['label'].value_counts().to_dict())\n",
    "print(\"테스트 데이터 크기 :\", len(test_df),  test_df['label'].value_counts().to_dict())\n",
    "\n",
    "# 학습/검증 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "tr_df, val_df = train_test_split(train_df, test_size=0.1, random_state=SEED, stratify=train_df['label'])\n",
    "print(\"Train:\", len(tr_df), \"Val:\", len(val_df), \"Test:\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ead8e7cf-9656-4711-adb3-46b4fbc7138c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: spm_unigram_8k.model\n",
      "Exists: spm_bpe_8k.model\n",
      "Exists: spm_unigram_16k.model\n"
     ]
    }
   ],
   "source": [
    "# ===== 3) SentencePiece 학습 =====\n",
    "import sentencepiece as spm\n",
    "\n",
    "SPM_DIR = Path(\"./spm_models\")\n",
    "SPM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def write_corpus_txt(df, out_path):\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for s in df['document'].tolist():\n",
    "            f.write(s.replace(\"\\n\", \" \") + \"\\n\")\n",
    "\n",
    "def train_spm_model(train_df, model_prefix, vocab_size=8000, model_type=\"unigram\"):\n",
    "    txt_path = SPM_DIR / f\"{model_prefix}.txt\"\n",
    "    write_corpus_txt(train_df, txt_path)\n",
    "\n",
    "    # 필수: pad/unk/bos/eos id를 고정\n",
    "    cmd = (\n",
    "        f\"--input={txt_path} \"\n",
    "        f\"--model_prefix={SPM_DIR / model_prefix} \"\n",
    "        f\"--vocab_size={vocab_size} \"\n",
    "        f\"--model_type={model_type} \"\n",
    "        f\"--character_coverage=0.9995 \"\n",
    "        f\"--pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 \"\n",
    "        f\"--input_sentence_size=1000000 --shuffle_input_sentence=true\"\n",
    "    )\n",
    "    print(\"SPM Train cmd:\\n\", cmd)\n",
    "    spm.SentencePieceTrainer.Train(cmd)\n",
    "\n",
    "def load_spm(model_path):\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(str(model_path))\n",
    "    return sp\n",
    "\n",
    "# 예: 기본 두 개 모델\n",
    "SPM_CONFIGS = [\n",
    "    dict(model_prefix=\"spm_unigram_8k\", vocab_size=8000,  model_type=\"unigram\"),\n",
    "    dict(model_prefix=\"spm_bpe_8k\",     vocab_size=8000,  model_type=\"bpe\"),\n",
    "    # 필요시 대형 vocab 추가\n",
    "    dict(model_prefix=\"spm_unigram_16k\", vocab_size=16000, model_type=\"unigram\"),\n",
    "]\n",
    "\n",
    "# 없으면 학습\n",
    "for cfg in SPM_CONFIGS:\n",
    "    model_path = SPM_DIR / f\"{cfg['model_prefix']}.model\"\n",
    "    if not model_path.exists():\n",
    "        train_spm_model(tr_df, **cfg)\n",
    "    else:\n",
    "        print(\"Exists:\", model_path.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d97c995-8ec1-4f28-9bb6-e2648069e3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 4) sp_tokenize 구현 =====\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "def sp_tokenize(sp_processor, corpus, vocab_path):\n",
    "    \"\"\"\n",
    "    sp_processor : sentencepiece.SentencePieceProcessor (로드된 모델)\n",
    "    corpus       : List[str] (원문장 리스트)\n",
    "    vocab_path   : str or Path (SentencePiece가 만든 .vocab 파일 경로)\n",
    "\n",
    "    return:\n",
    "      - tensor      : LongTensor [B, T] (pad=0)\n",
    "      - word_index  : dict {piece: idx}\n",
    "      - index_word  : dict {idx: piece}\n",
    "    \"\"\"\n",
    "    # 1) 문장들을 ID 시퀀스로\n",
    "    seqs = [torch.tensor(sp_processor.EncodeAsIds(s), dtype=torch.long) for s in corpus]\n",
    "    tensor = pad_sequence(seqs, batch_first=True, padding_value=0)  # pad_id=0 으로 맞춰 학습했음\n",
    "\n",
    "    # 2) vocab 읽어 사전 구성 (SentencePiece .vocab은 ID순으로 정렬되어 있음)\n",
    "    word_index, index_word = {}, {}\n",
    "    with open(vocab_path, encoding=\"utf-8\") as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            piece = line.split(\"\\t\")[0].strip()\n",
    "            word_index[piece] = idx\n",
    "            index_word[idx]   = piece\n",
    "\n",
    "    return tensor, word_index, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ffbc034-fbeb-455e-8bcb-ebe4dfbcb4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가능한 모델 : ['okt', 'mecab', 'kkma']\n"
     ]
    }
   ],
   "source": [
    "# ===== 5) KoNLPy 토크나이저 파이프라인 =====\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# 사용 가능 분석기 점검\n",
    "AVAILABLE = {}\n",
    "try:\n",
    "    from konlpy.tag import Okt\n",
    "    AVAILABLE[\"okt\"] = Okt()\n",
    "except Exception as e:\n",
    "    print(\"OKT 사용 불가 :\", e)\n",
    "\n",
    "try:\n",
    "    from konlpy.tag import Mecab\n",
    "    AVAILABLE[\"mecab\"] = Mecab()\n",
    "except Exception as e:\n",
    "    print(\"MeCab 사용 불가 :\", e)\n",
    "\n",
    "try:\n",
    "    from konlpy.tag import Kkma\n",
    "    AVAILABLE[\"kkma\"] = Kkma()\n",
    "except Exception as e:\n",
    "    print(\"Kkma 사용 불가 :\", e)\n",
    "\n",
    "print(\"가능한 모델 :\", list(AVAILABLE.keys()))\n",
    "\n",
    "def tokenize_konlpy(text, analyzer=\"okt\"):\n",
    "    if analyzer not in AVAILABLE:\n",
    "        analyzer = \"okt\" if \"okt\" in AVAILABLE else list(AVAILABLE.keys())[0]\n",
    "    if analyzer == \"okt\":\n",
    "        return AVAILABLE[analyzer].morphs(text)\n",
    "    elif analyzer == \"mecab\":\n",
    "        return AVAILABLE[analyzer].morphs(text)\n",
    "    elif analyzer == \"kkma\":\n",
    "        # Kkma는 품질은 좋지만 상대적으로 느립니다.\n",
    "        return AVAILABLE[analyzer].morphs(text)\n",
    "    else:\n",
    "        return text.split()\n",
    "\n",
    "def build_vocab_from_tokens(token_lists, min_freq=1, pad_id=0, unk_id=1, max_vocab=None):\n",
    "    \"\"\"\n",
    "    token_lists : List[List[str]]\n",
    "    \"\"\"\n",
    "    cnt = Counter([t for lst in token_lists for t in lst])\n",
    "    # ID 예약\n",
    "    word2id = {\"<pad>\": pad_id, \"<unk>\": unk_id}\n",
    "    start = 2\n",
    "    # 빈도순으로 정렬\n",
    "    most = cnt.most_common()\n",
    "    if max_vocab is not None:\n",
    "        most = most[:max(0, max_vocab - start)]\n",
    "    for w, _ in most:\n",
    "        if w not in word2id:\n",
    "            word2id[w] = len(word2id)\n",
    "    id2word = {i:w for w,i in word2id.items()}\n",
    "    return word2id, id2word\n",
    "\n",
    "def texts_to_tensor_by_vocab(texts, word2id, analyzer=\"okt\", pad_id=0, unk_id=1):\n",
    "    seqs = []\n",
    "    for s in texts:\n",
    "        toks = tokenize_konlpy(s, analyzer=analyzer)\n",
    "        ids  = [word2id.get(t, unk_id) for t in toks]\n",
    "        seqs.append(torch.tensor(ids, dtype=torch.long))\n",
    "    tensor = pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03572864-9848-4819-b9c0-1c202e054754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 6) 데이터셋, 모델 =====\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SimpleTensorDataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_array):\n",
    "        self.x = x_tensor\n",
    "        self.y = torch.tensor(y_array, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i): return self.x[i], self.y[i]\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=256, hidden=256, num_layers=1, bidirectional=True, dropout=0.2, pad_id=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden, num_layers=num_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout if num_layers > 1 else 0)\n",
    "        out_dim = hidden * (2 if bidirectional else 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(out_dim, 1)\n",
    "\n",
    "    def forward(self, x, pad_id=0):\n",
    "        # x: [B, T]\n",
    "        mask = (x != pad_id).float()               # [B, T]\n",
    "        emb  = self.embedding(x)                    # [B, T, E]\n",
    "        out, _ = self.lstm(emb)                     # [B, T, H*D]\n",
    "        # masked mean pooling\n",
    "        mask_exp = mask.unsqueeze(-1)               # [B, T, 1]\n",
    "        sum_out  = (out * mask_exp).sum(dim=1)      # [B, H*D]\n",
    "        len_out  = mask.sum(dim=1).clamp(min=1)     # [B]\n",
    "        pooled   = sum_out / len_out.unsqueeze(-1)\n",
    "        logits   = self.fc(self.dropout(pooled)).squeeze(1)  # [B]\n",
    "        return logits\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, pad_id=0):\n",
    "    model.train()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    for x, y in tqdm(loader, leave=False):\n",
    "        x = x.to(DEVICE); y = y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x, pad_id=pad_id)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "    return loss_sum/total, correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, criterion, pad_id=0):\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    for x, y in loader:\n",
    "        x = x.to(DEVICE); y = y.to(DEVICE)\n",
    "        logits = model(x, pad_id=pad_id)\n",
    "        loss = criterion(logits, y)\n",
    "        preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        loss_sum += loss.item() * y.size(0)\n",
    "    return loss_sum/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8cacfac-5804-42e3-9bae-dc6446fb54f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 7) 하이퍼파라미터 조정 =====\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional, Dict\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS     = 5\n",
    "EMBED_DIM  = 256\n",
    "HIDDEN     = 256\n",
    "DROPOUT    = 0.2\n",
    "LR         = 0.001\n",
    "\n",
    "@dataclass\n",
    "class ExpConfig:\n",
    "    name: str\n",
    "    kind: str            # 'spm' or 'konlpy'\n",
    "    spm_model: Optional[str] = None\n",
    "    spm_vocab : Optional[str] = None\n",
    "    analyzer  : Optional[str] = None         # 'okt'/'mecab'/'kkma'\n",
    "    max_vocab : Optional[int] = None         # konlpy 전용: vocab cap\n",
    "    note      : str = \"\"\n",
    "\n",
    "def build_tensors_from_config(cfg: ExpConfig, tr_df, val_df, test_df):\n",
    "    if cfg.kind == \"spm\":\n",
    "        sp = load_spm(cfg.spm_model)\n",
    "        # train/val/test → 텐서\n",
    "        xtr, wi, iw = sp_tokenize(sp, tr_df['document'].tolist(), cfg.spm_vocab)\n",
    "        xva, _, _   = sp_tokenize(sp, val_df['document'].tolist(), cfg.spm_vocab)\n",
    "        xte, _, _   = sp_tokenize(sp, test_df['document'].tolist(), cfg.spm_vocab)\n",
    "        vocab_size  = len(wi)\n",
    "        pad_id = 0\n",
    "        return (xtr, tr_df['label'].values,\n",
    "                xva, val_df['label'].values,\n",
    "                xte, test_df['label'].values,\n",
    "                vocab_size, pad_id, wi, iw)\n",
    "\n",
    "    elif cfg.kind == \"konlpy\":\n",
    "        analyzer = cfg.analyzer if cfg.analyzer in AVAILABLE else (\"okt\" if \"okt\" in AVAILABLE else list(AVAILABLE.keys())[0])\n",
    "        tr_tokens = [tokenize_konlpy(s, analyzer=analyzer) for s in tr_df['document'].tolist()]\n",
    "        word2id, id2word = build_vocab_from_tokens(tr_tokens, min_freq=1, max_vocab=cfg.max_vocab, pad_id=0, unk_id=1)\n",
    "        xtr = texts_to_tensor_by_vocab(tr_df['document'].tolist(), word2id, analyzer=analyzer, pad_id=0, unk_id=1)\n",
    "        xva = texts_to_tensor_by_vocab(val_df['document'].tolist(), word2id, analyzer=analyzer, pad_id=0, unk_id=1)\n",
    "        xte = texts_to_tensor_by_vocab(test_df['document'].tolist(), word2id, analyzer=analyzer, pad_id=0, unk_id=1)\n",
    "        vocab_size = len(word2id)\n",
    "        pad_id = 0\n",
    "        return (xtr, tr_df['label'].values,\n",
    "                xva, val_df['label'].values,\n",
    "                xte, test_df['label'].values,\n",
    "                vocab_size, pad_id, word2id, id2word)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown kind\")\n",
    "\n",
    "def run_experiment(cfg: ExpConfig):\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"Running:\", cfg.name)\n",
    "    print(\"==============================\")\n",
    "    (xtr, ytr, xva, yva, xte, yte, vocab_size, pad_id, w2i, i2w) = build_tensors_from_config(cfg, tr_df, val_df, test_df)\n",
    "\n",
    "    # Dataloaders\n",
    "    tr_loader  = DataLoader(SimpleTensorDataset(xtr, ytr), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    va_loader  = DataLoader(SimpleTensorDataset(xva, yva), batch_size=BATCH_SIZE, shuffle=False)\n",
    "    te_loader  = DataLoader(SimpleTensorDataset(xte, yte), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    model = BiLSTMClassifier(vocab_size=vocab_size, embed_dim=EMBED_DIM, hidden=HIDDEN, dropout=DROPOUT, pad_id=pad_id).to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state   = None\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        tr_loss, tr_acc = train_epoch(model, tr_loader, optimizer, criterion, pad_id=pad_id)\n",
    "        va_loss, va_acc = eval_epoch(model, va_loader, criterion, pad_id=pad_id)\n",
    "        print(f\"[{cfg.name}] Epoch {epoch:02d}/{EPOCHS} | \"\n",
    "              f\"train loss {tr_loss:.4f} acc {tr_acc:.4f} | \"\n",
    "              f\"val loss {va_loss:.4f} acc {va_acc:.4f}\")\n",
    "        if va_acc > best_val_acc:\n",
    "            best_val_acc = va_acc\n",
    "            best_state   = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "    # best로 평가\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    te_loss, te_acc = eval_epoch(model, te_loader, criterion, pad_id=pad_id)\n",
    "    print(f\"[{cfg.name}] TEST acc {te_acc:.4f}, loss {te_loss:.4f}\")\n",
    "\n",
    "    # 상세 리포트\n",
    "    y_true, y_pred = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in te_loader:\n",
    "            x = x.to(DEVICE)\n",
    "            logits = model(x, pad_id=pad_id)\n",
    "            pred = (torch.sigmoid(logits) >= 0.5).long().cpu().numpy().tolist()\n",
    "            y_pred += pred\n",
    "    y_true = test_df['label'].astype(int).tolist()\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    return dict(\n",
    "        name=cfg.name, kind=cfg.kind, note=cfg.note,\n",
    "        vocab_size=vocab_size, test_acc=te_acc, test_loss=te_loss, val_best=best_val_acc, report=report\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad201ea8-540f-4ce3-92e7-b5d6cc3011c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Running: SPM-Unigram-8k\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-8k] Epoch 01/5 | train loss 0.4445 acc 0.7863 | val loss 0.3595 acc 0.8412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-8k] Epoch 02/5 | train loss 0.3113 acc 0.8656 | val loss 0.3396 acc 0.8544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-8k] Epoch 03/5 | train loss 0.2717 acc 0.8865 | val loss 0.3555 acc 0.8516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-8k] Epoch 04/5 | train loss 0.2082 acc 0.9153 | val loss 0.3681 acc 0.8549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-8k] Epoch 05/5 | train loss 0.1427 acc 0.9447 | val loss 0.4294 acc 0.8482\n",
      "[SPM-Unigram-8k] TEST acc 0.8535, loss 0.3653\n",
      "\n",
      "==============================\n",
      "Running: SPM-BPE-8k\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-BPE-8k] Epoch 01/5 | train loss 0.4428 acc 0.7867 | val loss 0.3614 acc 0.8380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-BPE-8k] Epoch 02/5 | train loss 0.3139 acc 0.8649 | val loss 0.3347 acc 0.8540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-BPE-8k] Epoch 03/5 | train loss 0.2642 acc 0.8894 | val loss 0.3409 acc 0.8554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-BPE-8k] Epoch 04/5 | train loss 0.2001 acc 0.9193 | val loss 0.3745 acc 0.8520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-BPE-8k] Epoch 05/5 | train loss 0.1325 acc 0.9493 | val loss 0.4364 acc 0.8514\n",
      "[SPM-BPE-8k] TEST acc 0.8544, loss 0.3453\n",
      "\n",
      "==============================\n",
      "Running: SPM-Unigram-16k\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-16k] Epoch 01/5 | train loss 0.4584 acc 0.7776 | val loss 0.3753 acc 0.8317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-16k] Epoch 02/5 | train loss 0.3088 acc 0.8679 | val loss 0.3462 acc 0.8501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-16k] Epoch 03/5 | train loss 0.2392 acc 0.9016 | val loss 0.3542 acc 0.8553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-16k] Epoch 04/5 | train loss 0.1741 acc 0.9315 | val loss 0.3882 acc 0.8498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-16k] Epoch 05/5 | train loss 0.1411 acc 0.9476 | val loss 0.4629 acc 0.8483\n",
      "[SPM-Unigram-16k] TEST acc 0.8531, loss 0.3528\n",
      "\n",
      "==============================\n",
      "Running: OKT-30k\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OKT-30k] Epoch 01/5 | train loss 0.4441 acc 0.7889 | val loss 0.3730 acc 0.8352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OKT-30k] Epoch 02/5 | train loss 0.3079 acc 0.8670 | val loss 0.3287 acc 0.8560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OKT-30k] Epoch 03/5 | train loss 0.2387 acc 0.9017 | val loss 0.3304 acc 0.8600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OKT-30k] Epoch 04/5 | train loss 0.1921 acc 0.9246 | val loss 0.3737 acc 0.8555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OKT-30k] Epoch 05/5 | train loss 0.1223 acc 0.9539 | val loss 0.4369 acc 0.8535\n",
      "[OKT-30k] TEST acc 0.8551, loss 0.3390\n",
      "\n",
      "==============================\n",
      "Running: MeCab-30k\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MeCab-30k] Epoch 01/5 | train loss 0.4021 acc 0.8131 | val loss 0.3345 acc 0.8537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MeCab-30k] Epoch 02/5 | train loss 0.2906 acc 0.8758 | val loss 0.3071 acc 0.8706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MeCab-30k] Epoch 03/5 | train loss 0.2293 acc 0.9069 | val loss 0.3111 acc 0.8729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MeCab-30k] Epoch 04/5 | train loss 0.1719 acc 0.9335 | val loss 0.3405 acc 0.8681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MeCab-30k] Epoch 05/5 | train loss 0.1158 acc 0.9583 | val loss 0.3904 acc 0.8673\n",
      "[MeCab-30k] TEST acc 0.8661, loss 0.3202\n",
      "\n",
      "==============================\n",
      "Running: Kkma-30k\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# ===== 8) 실험 목록 & 실행 =====\n",
    "EXP_LIST = [\n",
    "    ExpConfig(name=\"SPM-Unigram-8k\",  kind=\"spm\",\n",
    "              spm_model=str(SPM_DIR/\"spm_unigram_8k.model\"),\n",
    "              spm_vocab=str(SPM_DIR/\"spm_unigram_8k.vocab\"),\n",
    "              note=\"SentencePiece unigram, vocab=8k\"),\n",
    "    ExpConfig(name=\"SPM-BPE-8k\",      kind=\"spm\",\n",
    "              spm_model=str(SPM_DIR/\"spm_bpe_8k.model\"),\n",
    "              spm_vocab=str(SPM_DIR/\"spm_bpe_8k.vocab\"),\n",
    "              note=\"SentencePiece BPE, vocab=8k\"),\n",
    "    ExpConfig(name=\"SPM-Unigram-16k\", kind=\"spm\",\n",
    "              spm_model=str(SPM_DIR/\"spm_unigram_16k.model\"),\n",
    "              spm_vocab=str(SPM_DIR/\"spm_unigram_16k.vocab\"),\n",
    "              note=\"SentencePiece unigram, vocab=16k\"),\n",
    "    ExpConfig(name=\"OKT-30k\", kind=\"konlpy\", analyzer=\"okt\", max_vocab=30000, note=\"OKT morphs, vocab cap 30k\"),\n",
    "]\n",
    "\n",
    "if \"mecab\" in AVAILABLE:\n",
    "    EXP_LIST.append(ExpConfig(name=\"MeCab-30k\", kind=\"konlpy\", analyzer=\"mecab\", max_vocab=30000, note=\"MeCab morphs, vocab cap 30k\"))\n",
    "if \"kkma\" in AVAILABLE:\n",
    "    EXP_LIST.append(ExpConfig(name=\"Kkma-30k\", kind=\"konlpy\", analyzer=\"kkma\", max_vocab=30000, note=\"Kkma morphs, vocab cap 30k\"))\n",
    "\n",
    "RESULTS = []\n",
    "for cfg in EXP_LIST:\n",
    "    RESULTS.append(run_experiment(cfg))\n",
    "\n",
    "# 결과 표\n",
    "import pandas as pd\n",
    "res_table = pd.DataFrame([{\n",
    "    \"name\": r[\"name\"],\n",
    "    \"kind\": r[\"kind\"],\n",
    "    \"vocab_size\": r[\"vocab_size\"],\n",
    "    \"val_best\": round(float(r[\"val_best\"]), 4),\n",
    "    \"test_acc\": round(float(r[\"test_acc\"]), 4),\n",
    "    \"test_loss\": round(float(r[\"test_loss\"]), 4),\n",
    "    \"note\": r[\"note\"]\n",
    "} for r in RESULTS]).sort_values(by=\"test_acc\", ascending=False)\n",
    "res_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc84644-899b-4e4d-8a8b-00f97946e4e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
