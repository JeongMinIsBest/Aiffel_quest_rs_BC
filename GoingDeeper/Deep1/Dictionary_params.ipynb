{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "614f0bb0-457a-4824-b2bc-79d199740b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch        : 2.7.1+cu118\n",
      "numpy        : 2.2.6\n",
      "matplotlib   : 3.10.3\n",
      "konlpy       : 0.6.0\n"
     ]
    }
   ],
   "source": [
    "# ===== 0) Library versions =====\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import konlpy\n",
    "\n",
    "print(\"torch        :\", torch.__version__)\n",
    "print(\"numpy        :\", np.__version__)\n",
    "print(\"matplotlib   :\", matplotlib.__version__)\n",
    "print(\"konlpy       :\", konlpy.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb555f81-8fa3-4d57-99cc-44eb55bd3e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "# ===== 1) Install =====\n",
    "\n",
    "# 재현성 고정\n",
    "import random, os\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05af3d7b-148b-440e-a49f-2977f051252b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 크기 : 146182 {0: 73342, 1: 72840}\n",
      "테스트 데이터 크기 : 49157 {1: 24711, 0: 24446}\n",
      "Train: 131563 Val: 14619 Test: 49157\n"
     ]
    }
   ],
   "source": [
    "# ===== 2) NSMC 데이터 준비 =====\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "DATA_DIR = Path(\"./data_nsmc\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_path = DATA_DIR / \"ratings_train.txt\"\n",
    "test_path  = DATA_DIR / \"ratings_test.txt\"\n",
    "\n",
    "def _download_if_needed():\n",
    "    base = \"https://raw.githubusercontent.com/e9t/nsmc/master\"\n",
    "    files = [\n",
    "        (\"ratings_train.txt\", f\"{base}/ratings_train.txt\"),\n",
    "        (\"ratings_test.txt\",  f\"{base}/ratings_test.txt\"),\n",
    "    ]\n",
    "    for fname, url in files:\n",
    "        fpath = DATA_DIR / fname\n",
    "        if not fpath.exists():\n",
    "            try:\n",
    "                print(f\"다운 중 {fname} ...\")\n",
    "                urllib.request.urlretrieve(url, fpath.as_posix())\n",
    "            except Exception as e:\n",
    "                print(f\"다운 실패 {fname}: {e}\\n 파일 위치를 확인 {fpath}.\")\n",
    "\n",
    "_download_if_needed()\n",
    "\n",
    "# 로드\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "test_df  = pd.read_csv(test_path,  sep='\\t')\n",
    "\n",
    "# 결측/중복 제거 + 라벨/텍스트 정리\n",
    "def clean_df(df):\n",
    "    df = df.dropna(subset=['document']).copy()\n",
    "    df['document'] = df['document'].astype(str).str.strip()\n",
    "    df = df[df['document'].str.len() > 0]\n",
    "    df = df.drop_duplicates(subset=['document'])\n",
    "    # label은 0/1 (부정/긍정)\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    return df\n",
    "\n",
    "train_df = clean_df(train_df)\n",
    "test_df  = clean_df(test_df)\n",
    "\n",
    "# 간단 분포 확인\n",
    "print(\"학습 데이터 크기 :\", len(train_df), train_df['label'].value_counts().to_dict())\n",
    "print(\"테스트 데이터 크기 :\", len(test_df),  test_df['label'].value_counts().to_dict())\n",
    "\n",
    "# 학습/검증 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "tr_df, val_df = train_test_split(train_df, test_size=0.1, random_state=SEED, stratify=train_df['label'])\n",
    "print(\"Train:\", len(tr_df), \"Val:\", len(val_df), \"Test:\", len(test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fba9f7c-7e9b-47fe-bdc3-78a7978297fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: spm_unigram_8k.model\n",
      "Exists: spm_bpe_8k.model\n",
      "Exists: spm_unigram_16k.model\n"
     ]
    }
   ],
   "source": [
    "# ===== 3) SentencePiece 학습 =====\n",
    "import sentencepiece as spm\n",
    "\n",
    "SPM_DIR = Path(\"./spm_models\")\n",
    "SPM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def write_corpus_txt(df, out_path):\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for s in df['document'].tolist():\n",
    "            f.write(s.replace(\"\\n\", \" \") + \"\\n\")\n",
    "\n",
    "def train_spm_model(train_df, model_prefix, vocab_size=8000, model_type=\"unigram\"):\n",
    "    txt_path = SPM_DIR / f\"{model_prefix}.txt\"\n",
    "    write_corpus_txt(train_df, txt_path)\n",
    "\n",
    "    # 필수: pad/unk/bos/eos id를 고정\n",
    "    cmd = (\n",
    "        f\"--input={txt_path} \"\n",
    "        f\"--model_prefix={SPM_DIR / model_prefix} \"\n",
    "        f\"--vocab_size={vocab_size} \"\n",
    "        f\"--model_type={model_type} \"\n",
    "        f\"--character_coverage=0.9995 \"\n",
    "        f\"--pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 \"\n",
    "        f\"--input_sentence_size=1000000 --shuffle_input_sentence=true\"\n",
    "    )\n",
    "    print(\"SPM Train cmd:\\n\", cmd)\n",
    "    spm.SentencePieceTrainer.Train(cmd)\n",
    "\n",
    "def load_spm(model_path):\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(str(model_path))\n",
    "    return sp\n",
    "\n",
    "# 예: 기본 두 개 모델\n",
    "SPM_CONFIGS = [\n",
    "    dict(model_prefix=\"spm_unigram_8k\", vocab_size=8000,  model_type=\"unigram\"),\n",
    "    dict(model_prefix=\"spm_bpe_8k\",     vocab_size=8000,  model_type=\"bpe\"),\n",
    "    # 필요시 대형 vocab 추가\n",
    "    dict(model_prefix=\"spm_unigram_16k\", vocab_size=16000, model_type=\"unigram\"),\n",
    "]\n",
    "\n",
    "# 없으면 학습\n",
    "for cfg in SPM_CONFIGS:\n",
    "    model_path = SPM_DIR / f\"{cfg['model_prefix']}.model\"\n",
    "    if not model_path.exists():\n",
    "        train_spm_model(tr_df, **cfg)\n",
    "    else:\n",
    "        print(\"Exists:\", model_path.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8b20f8a-d79e-45f4-893b-efab078590ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 4) sp_tokenize 구현 =====\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "def sp_tokenize(sp_processor, corpus, vocab_path):\n",
    "    \"\"\"\n",
    "    sp_processor : sentencepiece.SentencePieceProcessor (로드된 모델)\n",
    "    corpus       : List[str] (원문장 리스트)\n",
    "    vocab_path   : str or Path (SentencePiece가 만든 .vocab 파일 경로)\n",
    "\n",
    "    return:\n",
    "      - tensor      : LongTensor [B, T] (pad=0)\n",
    "      - word_index  : dict {piece: idx}\n",
    "      - index_word  : dict {idx: piece}\n",
    "    \"\"\"\n",
    "    # 1) 문장들을 ID 시퀀스로\n",
    "    seqs = [torch.tensor(sp_processor.EncodeAsIds(s), dtype=torch.long) for s in corpus]\n",
    "    tensor = pad_sequence(seqs, batch_first=True, padding_value=0)  # pad_id=0 으로 맞춰 학습했음\n",
    "\n",
    "    # 2) vocab 읽어 사전 구성 (SentencePiece .vocab은 ID순으로 정렬되어 있음)\n",
    "    word_index, index_word = {}, {}\n",
    "    with open(vocab_path, encoding=\"utf-8\") as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            piece = line.split(\"\\t\")[0].strip()\n",
    "            word_index[piece] = idx\n",
    "            index_word[idx]   = piece\n",
    "\n",
    "    return tensor, word_index, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbcf7f32-3df5-48df-8060-087015584136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가능한 모델 : ['okt', 'mecab', 'kkma']\n"
     ]
    }
   ],
   "source": [
    "# ===== 5) KoNLPy 토크나이저 파이프라인 =====\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# 사용 가능 분석기 점검\n",
    "AVAILABLE = {}\n",
    "try:\n",
    "    from konlpy.tag import Okt\n",
    "    AVAILABLE[\"okt\"] = Okt()\n",
    "except Exception as e:\n",
    "    print(\"OKT 사용 불가 :\", e)\n",
    "\n",
    "try:\n",
    "    from konlpy.tag import Mecab\n",
    "    AVAILABLE[\"mecab\"] = Mecab()\n",
    "except Exception as e:\n",
    "    print(\"MeCab 사용 불가 :\", e)\n",
    "\n",
    "try:\n",
    "    from konlpy.tag import Kkma\n",
    "    AVAILABLE[\"kkma\"] = Kkma()\n",
    "except Exception as e:\n",
    "    print(\"Kkma 사용 불가 :\", e)\n",
    "\n",
    "print(\"가능한 모델 :\", list(AVAILABLE.keys()))\n",
    "\n",
    "def tokenize_konlpy(text, analyzer=\"okt\"):\n",
    "    if analyzer not in AVAILABLE:\n",
    "        analyzer = \"okt\" if \"okt\" in AVAILABLE else list(AVAILABLE.keys())[0]\n",
    "    if analyzer == \"okt\":\n",
    "        return AVAILABLE[analyzer].morphs(text)\n",
    "    elif analyzer == \"mecab\":\n",
    "        return AVAILABLE[analyzer].morphs(text)\n",
    "    elif analyzer == \"kkma\":\n",
    "        # Kkma는 품질은 좋지만 상대적으로 느립니다.\n",
    "        return AVAILABLE[analyzer].morphs(text)\n",
    "    else:\n",
    "        return text.split()\n",
    "\n",
    "def build_vocab_from_tokens(token_lists, min_freq=1, pad_id=0, unk_id=1, max_vocab=None):\n",
    "    \"\"\"\n",
    "    token_lists : List[List[str]]\n",
    "    \"\"\"\n",
    "    cnt = Counter([t for lst in token_lists for t in lst])\n",
    "    # ID 예약\n",
    "    word2id = {\"<pad>\": pad_id, \"<unk>\": unk_id}\n",
    "    start = 2\n",
    "    # 빈도순으로 정렬\n",
    "    most = cnt.most_common()\n",
    "    if max_vocab is not None:\n",
    "        most = most[:max(0, max_vocab - start)]\n",
    "    for w, _ in most:\n",
    "        if w not in word2id:\n",
    "            word2id[w] = len(word2id)\n",
    "    id2word = {i:w for w,i in word2id.items()}\n",
    "    return word2id, id2word\n",
    "\n",
    "def texts_to_tensor_by_vocab(texts, word2id, analyzer=\"okt\", pad_id=0, unk_id=1):\n",
    "    seqs = []\n",
    "    for s in texts:\n",
    "        toks = tokenize_konlpy(s, analyzer=analyzer)\n",
    "        ids  = [word2id.get(t, unk_id) for t in toks]\n",
    "        seqs.append(torch.tensor(ids, dtype=torch.long))\n",
    "    tensor = pad_sequence(seqs, batch_first=True, padding_value=pad_id)\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0a70abc-4a32-48a1-a609-c18d94bde385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 6) 데이터셋, 모델 =====\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SimpleTensorDataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_array):\n",
    "        self.x = x_tensor\n",
    "        self.y = torch.tensor(y_array, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i): return self.x[i], self.y[i]\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=256, hidden=256, num_layers=1, bidirectional=True, dropout=0.2, pad_id=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden, num_layers=num_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout if num_layers > 1 else 0)\n",
    "        out_dim = hidden * (2 if bidirectional else 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(out_dim, 1)\n",
    "\n",
    "    def forward(self, x, pad_id=0):\n",
    "        # x: [B, T]\n",
    "        mask = (x != pad_id).float()               # [B, T]\n",
    "        emb  = self.embedding(x)                    # [B, T, E]\n",
    "        out, _ = self.lstm(emb)                     # [B, T, H*D]\n",
    "        # masked mean pooling\n",
    "        mask_exp = mask.unsqueeze(-1)               # [B, T, 1]\n",
    "        sum_out  = (out * mask_exp).sum(dim=1)      # [B, H*D]\n",
    "        len_out  = mask.sum(dim=1).clamp(min=1)     # [B]\n",
    "        pooled   = sum_out / len_out.unsqueeze(-1)\n",
    "        logits   = self.fc(self.dropout(pooled)).squeeze(1)  # [B]\n",
    "        return logits\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, pad_id=0):\n",
    "    model.train()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    for x, y in tqdm(loader, leave=False):\n",
    "        x = x.to(DEVICE); y = y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x, pad_id=pad_id)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "    return loss_sum/total, correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, criterion, pad_id=0):\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    for x, y in loader:\n",
    "        x = x.to(DEVICE); y = y.to(DEVICE)\n",
    "        logits = model(x, pad_id=pad_id)\n",
    "        loss = criterion(logits, y)\n",
    "        preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        loss_sum += loss.item() * y.size(0)\n",
    "    return loss_sum/total, correct/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e240abde-23c5-4a0c-aa2b-81a93c0a01d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 7) 하이퍼파라미터 조정 =====\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional, Dict\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS     = 10\n",
    "EMBED_DIM  = 512\n",
    "HIDDEN     = 512\n",
    "DROPOUT    = 0.2\n",
    "LR         = 0.0003\n",
    "\n",
    "@dataclass\n",
    "class ExpConfig:\n",
    "    name: str\n",
    "    kind: str            # 'spm' or 'konlpy'\n",
    "    spm_model: Optional[str] = None\n",
    "    spm_vocab : Optional[str] = None\n",
    "    analyzer  : Optional[str] = None         # 'okt'/'mecab'/'kkma'\n",
    "    max_vocab : Optional[int] = None         # konlpy 전용: vocab cap\n",
    "    note      : str = \"\"\n",
    "\n",
    "def build_tensors_from_config(cfg: ExpConfig, tr_df, val_df, test_df):\n",
    "    if cfg.kind == \"spm\":\n",
    "        sp = load_spm(cfg.spm_model)\n",
    "        # train/val/test → 텐서\n",
    "        xtr, wi, iw = sp_tokenize(sp, tr_df['document'].tolist(), cfg.spm_vocab)\n",
    "        xva, _, _   = sp_tokenize(sp, val_df['document'].tolist(), cfg.spm_vocab)\n",
    "        xte, _, _   = sp_tokenize(sp, test_df['document'].tolist(), cfg.spm_vocab)\n",
    "        vocab_size  = len(wi)\n",
    "        pad_id = 0\n",
    "        return (xtr, tr_df['label'].values,\n",
    "                xva, val_df['label'].values,\n",
    "                xte, test_df['label'].values,\n",
    "                vocab_size, pad_id, wi, iw)\n",
    "\n",
    "    elif cfg.kind == \"konlpy\":\n",
    "        analyzer = cfg.analyzer if cfg.analyzer in AVAILABLE else (\"okt\" if \"okt\" in AVAILABLE else list(AVAILABLE.keys())[0])\n",
    "        tr_tokens = [tokenize_konlpy(s, analyzer=analyzer) for s in tr_df['document'].tolist()]\n",
    "        word2id, id2word = build_vocab_from_tokens(tr_tokens, min_freq=1, max_vocab=cfg.max_vocab, pad_id=0, unk_id=1)\n",
    "        xtr = texts_to_tensor_by_vocab(tr_df['document'].tolist(), word2id, analyzer=analyzer, pad_id=0, unk_id=1)\n",
    "        xva = texts_to_tensor_by_vocab(val_df['document'].tolist(), word2id, analyzer=analyzer, pad_id=0, unk_id=1)\n",
    "        xte = texts_to_tensor_by_vocab(test_df['document'].tolist(), word2id, analyzer=analyzer, pad_id=0, unk_id=1)\n",
    "        vocab_size = len(word2id)\n",
    "        pad_id = 0\n",
    "        return (xtr, tr_df['label'].values,\n",
    "                xva, val_df['label'].values,\n",
    "                xte, test_df['label'].values,\n",
    "                vocab_size, pad_id, word2id, id2word)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown kind\")\n",
    "\n",
    "def run_experiment(cfg: ExpConfig):\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"Running:\", cfg.name)\n",
    "    print(\"==============================\")\n",
    "    (xtr, ytr, xva, yva, xte, yte, vocab_size, pad_id, w2i, i2w) = build_tensors_from_config(cfg, tr_df, val_df, test_df)\n",
    "\n",
    "    # Dataloaders\n",
    "    tr_loader  = DataLoader(SimpleTensorDataset(xtr, ytr), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    va_loader  = DataLoader(SimpleTensorDataset(xva, yva), batch_size=BATCH_SIZE, shuffle=False)\n",
    "    te_loader  = DataLoader(SimpleTensorDataset(xte, yte), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    model = BiLSTMClassifier(vocab_size=vocab_size, embed_dim=EMBED_DIM, hidden=HIDDEN, dropout=DROPOUT, pad_id=pad_id).to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state   = None\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        tr_loss, tr_acc = train_epoch(model, tr_loader, optimizer, criterion, pad_id=pad_id)\n",
    "        va_loss, va_acc = eval_epoch(model, va_loader, criterion, pad_id=pad_id)\n",
    "        print(f\"[{cfg.name}] Epoch {epoch:02d}/{EPOCHS} | \"\n",
    "              f\"train loss {tr_loss:.4f} acc {tr_acc:.4f} | \"\n",
    "              f\"val loss {va_loss:.4f} acc {va_acc:.4f}\")\n",
    "        if va_acc > best_val_acc:\n",
    "            best_val_acc = va_acc\n",
    "            best_state   = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "    # best로 평가\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    te_loss, te_acc = eval_epoch(model, te_loader, criterion, pad_id=pad_id)\n",
    "    print(f\"[{cfg.name}] TEST acc {te_acc:.4f}, loss {te_loss:.4f}\")\n",
    "\n",
    "    # 상세 리포트\n",
    "    y_true, y_pred = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in te_loader:\n",
    "            x = x.to(DEVICE)\n",
    "            logits = model(x, pad_id=pad_id)\n",
    "            pred = (torch.sigmoid(logits) >= 0.5).long().cpu().numpy().tolist()\n",
    "            y_pred += pred\n",
    "    y_true = test_df['label'].astype(int).tolist()\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    return dict(\n",
    "        name=cfg.name, kind=cfg.kind, note=cfg.note,\n",
    "        vocab_size=vocab_size, test_acc=te_acc, test_loss=te_loss, val_best=best_val_acc, report=report\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f27f8cb-18e6-468e-b549-a13e34aeacf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Running: SPM-Unigram-8k\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-8k] Epoch 01/10 | train loss 0.4731 acc 0.7749 | val loss 0.3786 acc 0.8303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-8k] Epoch 02/10 | train loss 0.3319 acc 0.8563 | val loss 0.3495 acc 0.8464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-8k] Epoch 03/10 | train loss 0.2744 acc 0.8842 | val loss 0.3441 acc 0.8521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-8k] Epoch 04/10 | train loss 0.2229 acc 0.9088 | val loss 0.3616 acc 0.8486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-8k] Epoch 05/10 | train loss 0.1670 acc 0.9336 | val loss 0.3906 acc 0.8503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-8k] Epoch 06/10 | train loss 0.1155 acc 0.9567 | val loss 0.4820 acc 0.8481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-8k] Epoch 07/10 | train loss 0.0776 acc 0.9723 | val loss 0.5525 acc 0.8457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-8k] Epoch 08/10 | train loss 0.0537 acc 0.9814 | val loss 0.6078 acc 0.8457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-8k] Epoch 09/10 | train loss 0.0386 acc 0.9873 | val loss 0.6956 acc 0.8451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-8k] Epoch 10/10 | train loss 0.0287 acc 0.9907 | val loss 0.7504 acc 0.8449\n",
      "[SPM-Unigram-8k] TEST acc 0.8493, loss 0.3483\n",
      "\n",
      "==============================\n",
      "Running: SPM-BPE-8k\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-BPE-8k] Epoch 01/10 | train loss 0.4588 acc 0.7796 | val loss 0.3733 acc 0.8332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-BPE-8k] Epoch 02/10 | train loss 0.3271 acc 0.8575 | val loss 0.3479 acc 0.8438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-BPE-8k] Epoch 03/10 | train loss 0.2703 acc 0.8860 | val loss 0.3465 acc 0.8486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-BPE-8k] Epoch 04/10 | train loss 0.2182 acc 0.9113 | val loss 0.3567 acc 0.8528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-BPE-8k] Epoch 05/10 | train loss 0.1631 acc 0.9363 | val loss 0.4208 acc 0.8499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-BPE-8k] Epoch 06/10 | train loss 0.1118 acc 0.9575 | val loss 0.4877 acc 0.8458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-BPE-8k] Epoch 07/10 | train loss 0.0742 acc 0.9730 | val loss 0.5757 acc 0.8414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-BPE-8k] Epoch 08/10 | train loss 0.0492 acc 0.9830 | val loss 0.6808 acc 0.8417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-BPE-8k] Epoch 09/10 | train loss 0.0376 acc 0.9875 | val loss 0.6587 acc 0.8482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-BPE-8k] Epoch 10/10 | train loss 0.0278 acc 0.9910 | val loss 0.7598 acc 0.8460\n",
      "[SPM-BPE-8k] TEST acc 0.8517, loss 0.3632\n",
      "\n",
      "==============================\n",
      "Running: SPM-Unigram-16k\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-16k] Epoch 01/10 | train loss 0.4801 acc 0.7633 | val loss 0.3978 acc 0.8195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-16k] Epoch 02/10 | train loss 0.3346 acc 0.8551 | val loss 0.3644 acc 0.8385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-16k] Epoch 03/10 | train loss 0.2613 acc 0.8916 | val loss 0.3593 acc 0.8466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-16k] Epoch 04/10 | train loss 0.1993 acc 0.9208 | val loss 0.3822 acc 0.8495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-16k] Epoch 05/10 | train loss 0.1390 acc 0.9468 | val loss 0.4575 acc 0.8462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-16k] Epoch 06/10 | train loss 0.0909 acc 0.9668 | val loss 0.5251 acc 0.8425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-16k] Epoch 07/10 | train loss 0.0588 acc 0.9797 | val loss 0.6162 acc 0.8397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-16k] Epoch 08/10 | train loss 0.0403 acc 0.9868 | val loss 0.6905 acc 0.8431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-16k] Epoch 09/10 | train loss 0.0311 acc 0.9898 | val loss 0.7527 acc 0.8385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPM-Unigram-16k] Epoch 10/10 | train loss 0.0249 acc 0.9922 | val loss 0.8159 acc 0.8387\n",
      "[SPM-Unigram-16k] TEST acc 0.8430, loss 0.3870\n",
      "\n",
      "==============================\n",
      "Running: OKT-30k\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OKT-30k] Epoch 01/10 | train loss 0.4500 acc 0.7820 | val loss 0.3774 acc 0.8273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OKT-30k] Epoch 02/10 | train loss 0.3325 acc 0.8534 | val loss 0.3470 acc 0.8479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OKT-30k] Epoch 03/10 | train loss 0.2677 acc 0.8871 | val loss 0.3489 acc 0.8482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OKT-30k] Epoch 04/10 | train loss 0.2122 acc 0.9138 | val loss 0.3613 acc 0.8537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OKT-30k] Epoch 05/10 | train loss 0.1572 acc 0.9392 | val loss 0.4089 acc 0.8509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OKT-30k] Epoch 06/10 | train loss 0.1128 acc 0.9573 | val loss 0.4606 acc 0.8529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OKT-30k] Epoch 07/10 | train loss 0.0765 acc 0.9729 | val loss 0.5449 acc 0.8465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OKT-30k] Epoch 08/10 | train loss 0.0557 acc 0.9800 | val loss 0.6083 acc 0.8455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OKT-30k] Epoch 09/10 | train loss 0.0427 acc 0.9847 | val loss 0.6749 acc 0.8449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OKT-30k] Epoch 10/10 | train loss 0.0346 acc 0.9879 | val loss 0.7019 acc 0.8507\n",
      "[OKT-30k] TEST acc 0.8486, loss 0.3708\n",
      "\n",
      "==============================\n",
      "Running: MeCab-30k\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MeCab-30k] Epoch 01/10 | train loss 0.4143 acc 0.8060 | val loss 0.3486 acc 0.8467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 124/514 [00:28<01:27,  4.47it/s]"
     ]
    }
   ],
   "source": [
    "# ===== 8) 실험 목록 & 실행 =====\n",
    "EXP_LIST = [\n",
    "    ExpConfig(name=\"SPM-Unigram-8k\",  kind=\"spm\",\n",
    "              spm_model=str(SPM_DIR/\"spm_unigram_8k.model\"),\n",
    "              spm_vocab=str(SPM_DIR/\"spm_unigram_8k.vocab\"),\n",
    "              note=\"SentencePiece unigram, vocab=8k\"),\n",
    "    ExpConfig(name=\"SPM-BPE-8k\",      kind=\"spm\",\n",
    "              spm_model=str(SPM_DIR/\"spm_bpe_8k.model\"),\n",
    "              spm_vocab=str(SPM_DIR/\"spm_bpe_8k.vocab\"),\n",
    "              note=\"SentencePiece BPE, vocab=8k\"),\n",
    "    ExpConfig(name=\"SPM-Unigram-16k\", kind=\"spm\",\n",
    "              spm_model=str(SPM_DIR/\"spm_unigram_16k.model\"),\n",
    "              spm_vocab=str(SPM_DIR/\"spm_unigram_16k.vocab\"),\n",
    "              note=\"SentencePiece unigram, vocab=16k\"),\n",
    "    ExpConfig(name=\"OKT-30k\", kind=\"konlpy\", analyzer=\"okt\", max_vocab=30000, note=\"OKT morphs, vocab cap 30k\"),\n",
    "]\n",
    "\n",
    "if \"mecab\" in AVAILABLE:\n",
    "    EXP_LIST.append(ExpConfig(name=\"MeCab-30k\", kind=\"konlpy\", analyzer=\"mecab\", max_vocab=30000, note=\"MeCab morphs, vocab cap 30k\"))\n",
    "if \"kkma\" in AVAILABLE:\n",
    "    EXP_LIST.append(ExpConfig(name=\"Kkma-30k\", kind=\"konlpy\", analyzer=\"kkma\", max_vocab=30000, note=\"Kkma morphs, vocab cap 30k\"))\n",
    "\n",
    "RESULTS = []\n",
    "for cfg in EXP_LIST:\n",
    "    RESULTS.append(run_experiment(cfg))\n",
    "\n",
    "# 결과 표\n",
    "import pandas as pd\n",
    "res_table = pd.DataFrame([{\n",
    "    \"name\": r[\"name\"],\n",
    "    \"kind\": r[\"kind\"],\n",
    "    \"vocab_size\": r[\"vocab_size\"],\n",
    "    \"val_best\": round(float(r[\"val_best\"]), 4),\n",
    "    \"test_acc\": round(float(r[\"test_acc\"]), 4),\n",
    "    \"test_loss\": round(float(r[\"test_loss\"]), 4),\n",
    "    \"note\": r[\"note\"]\n",
    "} for r in RESULTS]).sort_values(by=\"test_acc\", ascending=False)\n",
    "res_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961beab0-f0f3-4441-a351-51a3c8e97d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
