# Transformer(Ko-En)를 통해 번역기 만들기

---

### 프로젝트 목표
- 번역기 모델 학습에 필요한 텍스트 데이터 전처리가 잘 이루어졌다.
- Transformer 번역기 모델이 정상적으로 구동된다.
- 테스트 결과 의미가 통하는 수준의 번역문이 생성되었다.


### 팀 단위 역할 배분
- 기본적인 baseline 구축 이후 크게 전처리, 데이터 증강, 빔 서치로 나눔
- 전처리 파트 : 김재성
- 데이터 증강 : 박범찬, 임정민
- 빔 서치 : 최범용


### 데이터 증강
1. 원문을 전처리
2. 한국어는 Mecab/Okt로 형태소 단위로 쪼갠 문자열을 만듦
3. SentencePiece 토크나이저는 원본으로 먼저 학습
4. 일부 문장쌍만 골라 아래 방법으로 살짝 변형해서 원본에 더함 (1.5배)
5. 섞어서(shuffle) 학습 (검증/테스트 데이터는 증강 X)


### 프로젝트를 위한 시도
- 한국어-영어 번역을 위한 트랜스포머 모델에 메캅 형태소 분석 추가
- 파라미터 튜닝을 통해 성능 향상
- 데이터 증강을 통해 데이터셋 확보
- Label Smoothing 적용
  - 모델이 예측을 너무 확신하지 않도록 정답 레이블을 부드럽게 만들어주는 기법
  - Transformer 학습 시 거의 표준적으로 사용되며, 성능 향상에 도움
- 빔 서치(Beam Search) 디코딩 구현
  - 현재는 가장 확률이 높은 단어 하나만 선택하는 Greedy Search 방식으로 번역문을 생성
  - 이는 속도는 빠르지만 최적의 번역이 아닐 수 있음
  - 매 스텝마다 k개의 후보 단어를 유지하며 최종적으로 가장 확률 높은 문장 전체를 찾는 빔 서치를 구현하여 번역 품질 향상 도모


### 회고
- Transformer, 특히 pytorch를 이용한 Transformer는 정말 정말 정말 학습속도가 매우 느림
- 데이터 증강을 할 경우 Memory가 크게 급증하여 Out of Memory 오류를 자주 뱉어냄
  - kernel을 모두 shut down 시키고 진행해야 함
- 빔 서치는 모델이 온전하고 똑똑할 경우에 효율적임
  - PPL이 많으면 오히려 후보군을 줄이게 됐을 때 안좋은 선택지일 확률이 증가함
