{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e620122a-d910-49f5-9af3-e9cd77a46ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "=====================================================================\n",
    "# \"프로젝트 : 모든 장르 간 편향성 측정해 보기\"\n",
    "=====================================================================\n",
    "\n",
    "[프로젝트 목표]\n",
    "1) 전 장르 × 영화 구분(예술/일반/독립)의 내재 편향을 한 번에 보기\n",
    "    - 모든 타깃쌍(X vs Y)에 대해 장르별 WEAT 효과크기(=Cohen's d)를 계산하고 히트맵으로 비교\n",
    "\n",
    "2) 대표 단어 셋 개선으로 해석력 높이기\n",
    "    - TF-IDF의 중복/길이 편향 문제를 줄이기 위해\n",
    "      [로그-오즈(Dirichlet 사전) → 상위 후보] + [MMR 중복 억제]를 적용\n",
    "\n",
    "3) 어디서나 돌아가는 실행성/재현성\n",
    "    - konlpy(Mecab > Okt), 한글 폰트 자동탐색(실패시 영어 라벨 폴백)\n",
    "    - 난수 시드 고정, 결과 CSV/PNG 저장\n",
    "\n",
    "4) 독립영화 코퍼스가 없어도 분석 지속\n",
    "    - 실제 파일이 없으면 장르 전체 토큰 − (예술/일반)으로 코퍼스 합성\n",
    "\n",
    "[핵심 설계]\n",
    "    - 명사 중심 토큰화(Mecab>Okt 폴백)\n",
    "    - 대표어 : 로그-오즈(Dirichlet 사전)로 ‘해당 클래스에 특이한 단어’ 선별 > MMR로 중복 억제\n",
    "    - 타깃쌍(예술, 일반) × 장르별 WEAT 계산 > 히트맵 저장\n",
    "    - 폰트 이슈 자동 처리(한글 폰트 탐색, 실패 시 영문으로)\n",
    "    \n",
    "[참고]\n",
    "- WEAT: Caliskan et al. (2017)\n",
    "\n",
    "=====================================================================\n",
    "# \"프로젝트 회고\"\n",
    "=====================================================================\n",
    "\n",
    "[개인적인 해석]\n",
    "    - 대표어의 질이 전부다. 대표어가 장르의 개념축을 잘 잡아야 WEAT 신호가 또렷하다.\n",
    "    - 상관 = 인과(x), WEAT는 분포 상의 연관성이다. 제작,마케팅, 데이터 수집 편향이 모두 섞일 수 있다.\n",
    "    - WEAT 부호\n",
    "        > WEAT(예술 vs 일반)이 양수이면: 예술 타깃 단어들이 해당 장르 대표어(A)에 더 가깝다는 신호.\n",
    "        > 음수이면 반대(일반이 더 가깝다).\n",
    "\n",
    "[한계와 리스크]\n",
    "- 독립영화 합성 편향 : 잔여 토큰으로 만든 코퍼스는 실제 독립영화의 언어적 특질을 완벽히 반영하지 않을 수 있음.\n",
    "    > 가능하면 실제 독립영화 텍스트를 확보하고 만들어야 함\n",
    "- 명사 한정 : 형용사/동사(감성·스타일 지표)가 빠져 미세한 스타일 차가 덜 잡힐 수 있음.\n",
    "- MMR_SELECT_K, ALPHA_DIR, min_count, window 등에 따라 결과가 변동\n",
    "\n",
    "[회고]\n",
    "데이터의 대표어를 잡아는 것을 처음해봤다. 처음에는 TF-IDF로는 비슷한 단어가 우르르 몰려와서 다른 알고리즘을 찾아봤는데,\n",
    "로그-오즈, MMR을 돌리니 장르마다 다양한 축이 나타났다. 생각의 폭이 넓어지는 느낌.\n",
    "다만 독립영화는 결국 타협점을 찾아서 했고, 실제 데이터가 있을 떄 다시 해봐야 할 것 같다.\n",
    "마지막으로, 시각화하는데 자꾸 ㅁㅁㅁ 이렇게 떠서 스트레스를 받았다. 그래서 폰트 설정하는 법을 배웠다.\n",
    "\n",
    "\n",
    "=====================================================================\n",
    "'''\n",
    "\n",
    "# ==========================\n",
    "# 필요한 라이브러리 Import\n",
    "# ==========================\n",
    "from __future__ import annotations\n",
    "import os, re, math, json, random, unicodedata, warnings\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple, Iterable, Set\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 임베딩\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 형태소 분석기 폴백(Mecab > Okt)\n",
    "# 환경 의존도 낮추기: Mecab이 없을 때도 파이프라인이 멈추지 않도록 안전 폴백\n",
    "try:\n",
    "    from konlpy.tag import Mecab\n",
    "    _HAS_MECAB = True\n",
    "except Exception:\n",
    "    _HAS_MECAB = False\n",
    "    from konlpy.tag import Okt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rcParams\n",
    "\n",
    "RANDOM_SEED = 42               # 재현성 결과 재현을 위해 모든 난수 고정\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e9a418-d5ad-4512-b722-8cef296e92b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tokenizer] Mecab unavailable → fallback to Okt.nouns(): Install MeCab in order to use it: http://konlpy.org/en/latest/install/\n",
      "[Tokenizer] Using Okt.nouns() (fallback)\n",
      "[Genre] SF        :   1020 sentences\n",
      "[Genre] 가족        :    416 sentences\n",
      "[Genre] 공연        :    334 sentences\n",
      "[Genre] 공포(호러)    :   3063 sentences\n",
      "[Genre] 기타        :   3625 sentences\n",
      "[Genre] 다큐멘터리     :   7080 sentences\n",
      "[Genre] 드라마       :  18988 sentences\n",
      "[Genre] 멜로로맨스     :   5732 sentences\n",
      "[Genre] 뮤지컬       :    168 sentences\n",
      "[Genre] 미스터리      :   1066 sentences\n",
      "[Genre] 범죄        :   1680 sentences\n",
      "[Genre] 사극        :    238 sentences\n",
      "[Genre] 서부극(웨스턴)  :     48 sentences\n",
      "[Genre] 성인물(에로)   :   1625 sentences\n",
      "[Genre] 스릴러       :   2246 sentences\n",
      "[Genre] 애니메이션     :   8681 sentences\n",
      "[Genre] 액션        :   5829 sentences\n",
      "[Genre] 어드벤처      :    534 sentences\n",
      "[Genre] 전쟁        :    386 sentences\n",
      "[Genre] 코미디       :   4610 sentences\n",
      "[Genre] 판타지       :    791 sentences\n",
      "[Target] 예술영화      :  14286 sentences\n",
      "[Target] 일반영화      :  47602 sentences\n",
      "[Target] 독립영화      :   5139 sentences (synthesized)\n",
      "[W2V] Training on 135,187 sentences...\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# 경로/파일 불러오기\n",
    "# ==========================\n",
    "DATA_DIR = os.path.join(os.getenv(\"HOME\"), \"work/weat/data\")\n",
    "\n",
    "# 데이터 파일\n",
    "genre_txt = [\n",
    "    'synopsis_SF.txt', 'synopsis_family.txt', 'synopsis_show.txt', 'synopsis_horror.txt', 'synopsis_etc.txt',\n",
    "    'synopsis_documentary.txt', 'synopsis_drama.txt', 'synopsis_romance.txt', 'synopsis_musical.txt',\n",
    "    'synopsis_mystery.txt', 'synopsis_crime.txt', 'synopsis_historical.txt', 'synopsis_western.txt',\n",
    "    'synopsis_adult.txt', 'synopsis_thriller.txt', 'synopsis_animation.txt', 'synopsis_action.txt',\n",
    "    'synopsis_adventure.txt', 'synopsis_war.txt', 'synopsis_comedy.txt', 'synopsis_fantasy.txt'\n",
    "]\n",
    "\n",
    "genre_name = [\n",
    "    'SF', '가족', '공연', '공포(호러)', '기타', '다큐멘터리', '드라마', '멜로로맨스', '뮤지컬', '미스터리', '범죄', '사극',\n",
    "    '서부극(웨스턴)', '성인물(에로)', '스릴러', '애니메이션', '액션', '어드벤처', '전쟁', '코미디', '판타지'\n",
    "]\n",
    "\n",
    "# 영화 구분 타깃(독립은 있으면)\n",
    "# 최소 타깃(예술/일반)은 필수. 독립은 있으면 사용\n",
    "TARGET_TXT = {\n",
    "    '예술영화': 'synopsis_art.txt',\n",
    "    '일반영화': 'synopsis_gen.txt',\n",
    "    # '독립영화': 'synopsis_indie.txt',  # 존재하면 인식\n",
    "}\n",
    "OPTIONAL_INDEPENDENT_FILENAME = 'synopsis_indie.txt'  # 있으면 사용, 없으면 합성\n",
    "\n",
    "# 대표단어 셋 하이퍼파라미터\n",
    "TOPN_CANDIDATES = 200 \n",
    "MMR_SELECT_K   = 40     # 최종 대표단어 개수 (25~50)\n",
    "MIN_TERM_LEN   = 2\n",
    "MIN_DF         = 5 \n",
    "ALPHA_DIR      = 0.01\n",
    "MMR_LAMBDA     = 0.65   # MMR 관련성/다양성 균형(0.5~0.8)\n",
    "\n",
    "# Word2Vec 설정\n",
    "W2V_VECTOR_SIZE = 300\n",
    "W2V_WINDOW      = 5\n",
    "W2V_MIN_COUNT   = 5\n",
    "W2V_WORKERS     = 4\n",
    "W2V_SG          = 1   # 1: skip-gram(희소 단어에 강함), 0: CBOW\n",
    "W2V_EPOCHS      = 10\n",
    "\n",
    "WEAT_PERMUTATIONS = 0  # 예: 1000\n",
    "\n",
    "# 출력 파일\n",
    "OUT_CSV = 'weat_genre_bias_all_pairs.csv'\n",
    "OUT_FIG = 'weat_genre_heatmap_all_pairs.png'\n",
    "\n",
    "# 플롯 라벨 언어 정책\n",
    "# 한글 폰트 부재 환경에서도 결과 해석 가능하도록 자동 폴백\n",
    "PLOT_LABEL_LANG = 'auto'  # 'auto', 'ko', 'en'\n",
    "\n",
    "# 한글 > 영어 매핑(축 라벨)\n",
    "GENRE_EN = {\n",
    "    'SF':'Sci-Fi','가족':'Family','공연':'Performance','공포(호러)':'Horror','기타':'Misc',\n",
    "    '다큐멘터리':'Documentary','드라마':'Drama','멜로로맨스':'Romance','뮤지컬':'Musical',\n",
    "    '미스터리':'Mystery','범죄':'Crime','사극':'Historical','서부극(웨스턴)':'Western',\n",
    "    '성인물(에로)':'Adult','스릴러':'Thriller','애니메이션':'Animation','액션':'Action',\n",
    "    '어드벤처':'Adventure','전쟁':'War','코미디':'Comedy','판타지':'Fantasy'\n",
    "}\n",
    "TARGET_EN = {'예술영화':'Art','일반영화':'General','독립영화':'Indie'}\n",
    "\n",
    "# ==========================\n",
    "# 토큰화/정규화\n",
    "# ==========================\n",
    "_ALNUM_RE  = re.compile(r'^[가-힣A-Za-z0-9]+$')\n",
    "\n",
    "def normalize_token(tok: str) -> str:\n",
    "    tok = tok.strip()\n",
    "    tok = unicodedata.normalize('NFC', tok)\n",
    "    tok = re.sub(r'[^가-힣A-Za-z0-9]', '', tok)  # 기호 제거\n",
    "    return tok\n",
    "\n",
    "def is_good_token(tok: str) -> bool:\n",
    "    if len(tok) < MIN_TERM_LEN:\n",
    "        return False\n",
    "    if not _ALNUM_RE.match(tok):\n",
    "        return False\n",
    "    if tok.isdigit():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_tokenizer():\n",
    "    \"\"\"\n",
    "    [의도] 품사 기반 '명사' 중심 추출로 장르/구분의 핵심 개념축을 잡는다.\n",
    "    - Mecab이 있으면 정확한 품사 태깅 사용\n",
    "    - 없으면 Okt.nouns()로 폴백(실행성 우선)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from konlpy.tag import Mecab\n",
    "        mecab = Mecab()\n",
    "        def _tok(text: str) -> List[str]:\n",
    "            return [w for w, t in mecab.pos(text) if t.startswith('NN')]\n",
    "        print('[Tokenizer] Using Mecab (NN* only)')\n",
    "        return _tok\n",
    "    except Exception as e:\n",
    "        print(f'[Tokenizer] Mecab unavailable → fallback to Okt.nouns(): {e}')\n",
    "        from konlpy.tag import Okt\n",
    "        okt = Okt()\n",
    "        def _tok(text: str) -> List[str]:\n",
    "            return okt.nouns(text)\n",
    "        print('[Tokenizer] Using Okt.nouns() (fallback)')\n",
    "        return _tok\n",
    "\n",
    "def read_lines(path: str) -> List[str]:\n",
    "    \"\"\"입력 경로 체크, 공백 라인 제거\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        warnings.warn(f'파일 없음: {path}')\n",
    "        return []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = [ln.strip() for ln in f if ln.strip()]\n",
    "    return lines\n",
    "\n",
    "def tokenize_file(path: str, tokenizer) -> List[List[str]]:\n",
    "    \"\"\"문장 단위로 명사 토큰화, 정규화, 노이즈 제거\"\"\"\n",
    "    sents = read_lines(path)\n",
    "    out: List[List[str]] = []\n",
    "    for s in sents:\n",
    "        toks = [normalize_token(t) for t in tokenizer(s)]\n",
    "        toks = [t for t in toks if is_good_token(t)]\n",
    "        if toks:\n",
    "            out.append(toks)\n",
    "    return out\n",
    "\n",
    "# ==========================\n",
    "# 폰트 처리 (시각화 자동 폴백)\n",
    "# ==========================\n",
    "def try_enable_korean_font() -> bool:\n",
    "    rcParams['axes.unicode_minus'] = False  # 마이너스 부호 깨짐 방지\n",
    "    candidates = [\n",
    "        'AppleGothic',                          # macOS\n",
    "        'Malgun Gothic', '맑은 고딕',           # Windows\n",
    "        'NanumGothic', 'NanumGothicCoding', 'NanumBarunGothic',  # Linux\n",
    "        'Noto Sans CJK KR', 'Noto Sans KR',\n",
    "        'UnDotum', '돋움'\n",
    "    ]\n",
    "    for name in candidates:\n",
    "        try:\n",
    "            _ = font_manager.findfont(font_manager.FontProperties(family=name), fallback_to_default=False)\n",
    "            rcParams['font.family'] = name\n",
    "            print(f'[Font] Using Korean font: {name}')\n",
    "            return True\n",
    "        except Exception:\n",
    "            continue\n",
    "    print('[Font] No Korean font found. Will fall back to English labels.')\n",
    "    return False\n",
    "\n",
    "# ==========================\n",
    "# 임베딩 학습\n",
    "# ==========================\n",
    "def train_word2vec(all_sentences: List[List[str]]) -> Word2Vec:\n",
    "    \"\"\"\n",
    "         장르, 타깃 전체 문장을 한 번에 학습해 '공용 임베딩 공간'을 만든다.\n",
    "         > X/Y/A/B 단어 간 상대 거리 비교가 일관되고 안정적\n",
    "    \"\"\"\n",
    "    print(f'[W2V] Training on {len(all_sentences):,} sentences...')\n",
    "    model = Word2Vec(\n",
    "        sentences=all_sentences,\n",
    "        vector_size=W2V_VECTOR_SIZE,\n",
    "        window=W2V_WINDOW,\n",
    "        min_count=W2V_MIN_COUNT,\n",
    "        workers=W2V_WORKERS,\n",
    "        sg=W2V_SG,\n",
    "        seed=RANDOM_SEED,\n",
    "        epochs=W2V_EPOCHS,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# ==========================\n",
    "# 대표어 스코어링: 로그-오즈 + MMR\n",
    "# ==========================\n",
    "def counts_from_docs(docs: List[List[str]]) -> Counter:\n",
    "    \"\"\"클래스 내 단어 수(BoW)를 집계 > 로그오즈 계산의 입력\"\"\"\n",
    "    c = Counter()\n",
    "    for sent in docs:\n",
    "        c.update(sent)\n",
    "    return c\n",
    "\n",
    "def log_odds_dirichlet(class_counts: Counter, other_counts: Counter, alpha: float = ALPHA_DIR) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "         TF-IDF의 한계(TF/길이 편향, 동의어 중복)를 보완하기 위해\n",
    "         '그 클래스에 얼마나 특이한가'를 z-score 근사로 계산\n",
    "    \"\"\"\n",
    "    vocab: Set[str] = set(class_counts) | set(other_counts)\n",
    "    V = len(vocab)\n",
    "    n_c = sum(class_counts.values())\n",
    "    n_o = sum(other_counts.values())\n",
    "\n",
    "    zscores: Dict[str, float] = {}\n",
    "    for w in vocab:\n",
    "        f_c = class_counts.get(w, 0)\n",
    "        f_o = other_counts.get(w, 0)\n",
    "        # 로짓 차이(사전 alpha로 스무딩)\n",
    "        num_c = f_c + alpha\n",
    "        den_c = (n_c - f_c) + alpha * (V - 1)\n",
    "        num_o = f_o + alpha\n",
    "        den_o = (n_o - f_o) + alpha * (V - 1)\n",
    "        delta = math.log(num_c / den_c) - math.log(num_o / den_o)\n",
    "        var = 1.0 / num_c + 1.0 / num_o\n",
    "        z = delta / math.sqrt(var)\n",
    "        zscores[w] = z\n",
    "    return zscores\n",
    "\n",
    "def mmr_select(candidates: List[str], scores: Dict[str, float], model: Word2Vec,\n",
    "               k: int = MMR_SELECT_K, lamb: float = MMR_LAMBDA) -> List[str]:\n",
    "    \"\"\"\n",
    "         MMR(Maximal Marginal Relevance)로 유사한 단어의 중복 선정을 억제\n",
    "         '다양성 있는' 대표어 셋을 구성해 WEAT의 해석력을 높임\n",
    "    \"\"\"\n",
    "    cand = [w for w in candidates if w in model.wv]\n",
    "    if not cand:\n",
    "        return []\n",
    "    vals = np.array([scores[w] for w in cand], dtype=float)\n",
    "    if np.ptp(vals) == 0:\n",
    "        rel = {w: 0.5 for w in cand}\n",
    "    else:\n",
    "        vmin, vmax = float(vals.min()), float(vals.max())\n",
    "        rel = {w: (scores[w] - vmin) / (vmax - vmin) for w in cand}\n",
    "\n",
    "    selected: List[str] = []\n",
    "    while len(selected) < min(k, len(cand)):\n",
    "        best_w, best_score = None, -1e9\n",
    "        for w in cand:\n",
    "            if w in selected:\n",
    "                continue\n",
    "            if not selected:\n",
    "                redundancy = 0.0\n",
    "            else:\n",
    "                v = model.wv[w]\n",
    "                sims = []\n",
    "                for s in selected:\n",
    "                    svec = model.wv[s]\n",
    "                    denom = (np.linalg.norm(v) * np.linalg.norm(svec)) or 1e-9\n",
    "                    sims.append(float(np.dot(v, svec) / denom))\n",
    "                redundancy = max(sims) if sims else 0.0\n",
    "            mmr = lamb * rel[w] - (1.0 - lamb) * redundancy\n",
    "            if mmr > best_score:\n",
    "                best_score, best_w = mmr, w\n",
    "        assert best_w is not None\n",
    "        selected.append(best_w)\n",
    "    return selected\n",
    "\n",
    "def build_representatives(class_docs: Dict[str, List[List[str]]], model: Word2Vec,\n",
    "                          topn_candidates: int = TOPN_CANDIDATES, mmr_k: int = MMR_SELECT_K,\n",
    "                          min_df: int = MIN_DF, alpha: float = ALPHA_DIR) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "         각 클래스 마다\n",
    "         BoW 집계 > 로그오즈 점수 > 상위 후보 추출 > MMR로 중복 억제\n",
    "    \"\"\"\n",
    "    # 전체 카운트(다른 집단 비교용)\n",
    "    global_counts = counts_from_docs([tok for docs in class_docs.values() for tok in docs])\n",
    "    reps: Dict[str, List[str]] = {}\n",
    "    for cname, docs in class_docs.items():\n",
    "        c_counts = counts_from_docs(docs)\n",
    "        o_counts = global_counts.copy()\n",
    "        # other = global - class (음수 방지 보정)\n",
    "        for w, f in c_counts.items():\n",
    "            if w in o_counts:\n",
    "                o_counts[w] = max(0, o_counts[w] - f)\n",
    "        # 희소 토큰 컷\n",
    "        c_counts = Counter({w: f for w, f in c_counts.items() if f >= min_df})\n",
    "        # 점수\n",
    "        z = log_odds_dirichlet(c_counts, o_counts, alpha=alpha)\n",
    "        # 상위 후보\n",
    "        cand = [w for w, _ in sorted(z.items(), key=lambda x: x[1], reverse=True)]\n",
    "        cand = [w for w in cand if w in c_counts][:topn_candidates]\n",
    "        # MMR로 최종 대표어\n",
    "        reps[cname] = mmr_select(cand, z, model, k=mmr_k)\n",
    "    return reps\n",
    "\n",
    "# ==========================\n",
    "# WEAT 계산\n",
    "# ==========================\n",
    "def cos_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b)) or 1e-9\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "def mean_cos_sim(word: str, S: Iterable[str], model: Word2Vec) -> float:\n",
    "    if word not in model.wv:\n",
    "        return 0.0\n",
    "    v = model.wv[word]\n",
    "    sims = []\n",
    "    for s in S:\n",
    "        if s in model.wv:\n",
    "            sims.append(cos_sim(v, model.wv[s]))\n",
    "    return float(np.mean(sims)) if sims else 0.0\n",
    "\n",
    "def s_wAB(word: str, A: Iterable[str], B: Iterable[str], model: Word2Vec) -> float:\n",
    "    return mean_cos_sim(word, A, model) - mean_cos_sim(word, B, model)\n",
    "\n",
    "def weat_effect_size(X: Iterable[str], Y: Iterable[str], A: Iterable[str], B: Iterable[str], model: Word2Vec) -> float:\n",
    "    X, Y, A, B = list(X), list(Y), list(A), list(B)\n",
    "    X = [w for w in X if w in model.wv]\n",
    "    Y = [w for w in Y if w in model.wv]\n",
    "    A = [w for w in A if w in model.wv]\n",
    "    B = [w for w in B if w in model.wv]\n",
    "    if not X or not Y or not A or not B:\n",
    "        return float('nan')\n",
    "    sX = np.array([s_wAB(w, A, B, model) for w in X])\n",
    "    sY = np.array([s_wAB(w, A, B, model) for w in Y])\n",
    "    num = np.mean(sX) - np.mean(sY)\n",
    "    denom = np.std(np.concatenate([sX, sY], axis=0), ddof=1) or 1e-9\n",
    "    return float(num / denom)\n",
    "\n",
    "def weat_pvalue_permutation(X: List[str], Y: List[str], A: List[str], B: List[str],\n",
    "                            model: Word2Vec, n_perm: int = 1000) -> float:\n",
    "    if n_perm <= 0:\n",
    "        return np.nan\n",
    "    X = [w for w in X if w in model.wv]\n",
    "    Y = [w for w in Y if w in model.wv]\n",
    "    A = [w for w in A if w in model.wv]\n",
    "    B = [w for w in B if w in model.wv]\n",
    "    if not X or not Y or not A or not B:\n",
    "        return np.nan\n",
    "    all_words = X + Y\n",
    "    observed = weat_effect_size(X, Y, A, B, model)\n",
    "    count = 0\n",
    "    for _ in range(n_perm):\n",
    "        random.shuffle(all_words)\n",
    "        Xp = all_words[:len(X)]\n",
    "        Yp = all_words[len(X):]\n",
    "        val = weat_effect_size(Xp, Yp, A, B, model)\n",
    "        if abs(val) >= abs(observed):\n",
    "            count += 1\n",
    "    return (count + 1) / (n_perm + 1)\n",
    "\n",
    "# ==========================\n",
    "# 독립영화 자동 합성\n",
    "# ==========================\n",
    "def bow_to_sentences(counter: Counter, sentence_len: int = 10, max_sentences: int | None = None) -> List[List[str]]:\n",
    "    tokens = []\n",
    "    for w, f in counter.items():\n",
    "        tokens.extend([w] * int(f))\n",
    "    if not tokens:\n",
    "        return []\n",
    "    random.shuffle(tokens)\n",
    "    sents: List[List[str]] = []\n",
    "    for i in range(0, len(tokens), sentence_len):\n",
    "        chunk = tokens[i:i+sentence_len]\n",
    "        if len(chunk) >= max(3, sentence_len//2):  # 너무 짧은 문장 방지\n",
    "            sents.append(chunk)\n",
    "        if max_sentences is not None and len(sents) >= max_sentences:\n",
    "            break\n",
    "    return sents\n",
    "\n",
    "def synthesize_indie_docs(genre_docs: Dict[str, List[List[str]]],\n",
    "                          target_docs_partial: Dict[str, List[List[str]]]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "        '독립영화' 코퍼스가 없을 시에도 분석을 진행할 수 있게,\n",
    "         장르 전체 토큰, 예술+일반 토큰 = 잔여 토큰으로 코퍼스 생성\n",
    "    \"\"\"\n",
    "    # 장르 전체 BoW\n",
    "    global_counts = counts_from_docs([s for docs in genre_docs.values() for s in docs])\n",
    "    known_counts = Counter()\n",
    "    for k in ['예술영화', '일반영화']:\n",
    "        if k in target_docs_partial:\n",
    "            known_counts.update(counts_from_docs(target_docs_partial[k]))\n",
    "    indie_counts = Counter()\n",
    "    for w, f in global_counts.items():\n",
    "        indie_counts[w] = max(0, f - known_counts.get(w, 0))\n",
    "    indie_counts = Counter({w: f for w, f in indie_counts.items() if f >= MIN_DF})\n",
    "    # 문장 합성\n",
    "    indie_docs = bow_to_sentences(indie_counts, sentence_len=10, max_sentences=50000)\n",
    "    return indie_docs\n",
    "\n",
    "# ==========================\n",
    "# 메인 파이프라인\n",
    "# ==========================\n",
    "def main():\n",
    "    tokenizer = get_tokenizer()\n",
    "\n",
    "    # 1) 장르 코퍼스 로딩 & 토큰화\n",
    "    genre_files = {g: os.path.join(DATA_DIR, f) for g, f in zip(genre_name, genre_txt)}\n",
    "    genre_docs: Dict[str, List[List[str]]] = {}\n",
    "\n",
    "    for g, fpath in genre_files.items():\n",
    "        docs = tokenize_file(fpath, tokenizer)\n",
    "        if len(docs) == 0:\n",
    "            warnings.warn(f'장르 \"{g}\"에서 유효 문서가 없습니다: {fpath}')\n",
    "        genre_docs[g] = docs\n",
    "        print(f'[Genre] {g:10s}: {len(docs):6d} sentences')\n",
    "\n",
    "    # 2) 타깃(영화 구분) 코퍼스 로딩 & 토큰화\n",
    "    target_docs: Dict[str, List[List[str]]] = {}\n",
    "    for k, fname in TARGET_TXT.items():\n",
    "        fpath = os.path.join(DATA_DIR, fname)\n",
    "        docs = tokenize_file(fpath, tokenizer)\n",
    "        if len(docs) == 0:\n",
    "            warnings.warn(f'타깃 \"{k}\"에서 유효 문서가 없습니다: {fpath}')\n",
    "        target_docs[k] = docs\n",
    "        print(f'[Target] {k:10s}: {len(docs):6d} sentences')\n",
    "\n",
    "    # 독립영화: 파일 있으면 사용, 없으면 합성\n",
    "    indie_path = os.path.join(DATA_DIR, OPTIONAL_INDEPENDENT_FILENAME)\n",
    "    if os.path.exists(indie_path):\n",
    "        indie_docs = tokenize_file(indie_path, tokenizer)\n",
    "        target_docs['독립영화'] = indie_docs\n",
    "        print(f'[Target] {\"독립영화\":10s}: {len(indie_docs):6d} sentences (from file)')\n",
    "    else:\n",
    "        indie_docs = synthesize_indie_docs(genre_docs, target_docs_partial=target_docs)\n",
    "        if indie_docs:\n",
    "            target_docs['독립영화'] = indie_docs\n",
    "            print(f'[Target] {\"독립영화\":10s}: {len(indie_docs):6d} sentences (synthesized)')\n",
    "\n",
    "    # 최소 두 타깃 이상 필요\n",
    "    if len(target_docs) < 2:\n",
    "        raise RuntimeError('타깃(영화 구분) 코퍼스가 2개 미만입니다. synopsis_art.txt / synopsis_gen.txt를 확인하세요.')\n",
    "\n",
    "    # 3) 임베딩 학습\n",
    "    all_sentences = [s for docs in genre_docs.values() for s in docs] + [s for docs in target_docs.values() for s in docs]\n",
    "    if not all_sentences:\n",
    "        raise RuntimeError('학습할 문장이 없습니다. 파일 경로/인코딩을 확인하세요.')\n",
    "    w2v = train_word2vec(all_sentences)\n",
    "\n",
    "    # 4) 대표어 선정\n",
    "    genre_reps  = build_representatives(genre_docs,  w2v, topn_candidates=TOPN_CANDIDATES, mmr_k=MMR_SELECT_K)\n",
    "    target_reps = build_representatives(target_docs, w2v, topn_candidates=TOPN_CANDIDATES, mmr_k=MMR_SELECT_K)\n",
    "\n",
    "    # 5) 모든 타깃쌍에 대해, 각 장르 g의 WEAT 계산\n",
    "    rows = []\n",
    "    tgt_keys = list(target_reps.keys())\n",
    "    tgt_pairs = list(combinations(tgt_keys, 2))\n",
    "    # 기타 장르 풀(장르 g 이외의 대표어 모음) → A와 크기 맞춘 B를 MMR로 구성\n",
    "    all_other_pool = {g: [w for gg, ws in genre_reps.items() if gg != g for w in ws] for g in genre_reps}\n",
    "\n",
    "    for g in genre_name:\n",
    "        row = {'genre': g, 'A_words': ','.join(genre_reps.get(g, [])[:10])}\n",
    "        A = genre_reps.get(g, [])\n",
    "        for (t1, t2) in tgt_pairs:\n",
    "            col_name = f'WEAT({t1} vs {t2})'\n",
    "            if not A:\n",
    "                row[col_name] = np.nan\n",
    "                pcol = 'p_value' if WEAT_PERMUTATIONS > 0 else 'p_value(NA)'\n",
    "                row[f'{pcol}({t1} vs {t2})'] = np.nan\n",
    "                continue\n",
    "            pool = [w for w in all_other_pool.get(g, []) if w not in set(A)]\n",
    "            pseudo_scores = {w: 1.0 for w in pool}\n",
    "            B = mmr_select(pool, pseudo_scores, w2v, k=len(A)) if pool else []\n",
    "            X = target_reps.get(t1, [])\n",
    "            Y = target_reps.get(t2, [])\n",
    "            score = weat_effect_size(X, Y, A, B, w2v)\n",
    "            pval = weat_pvalue_permutation(X, Y, A, B, w2v, n_perm=WEAT_PERMUTATIONS)\n",
    "            row[col_name] = score\n",
    "            pcol = 'p_value' if WEAT_PERMUTATIONS > 0 else 'p_value(NA)'\n",
    "            row[f'{pcol}({t1} vs {t2})'] = pval\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows).set_index('genre')\n",
    "    df.to_csv(OUT_CSV, encoding='utf-8-sig')\n",
    "    print(f'[OUT] Saved: {OUT_CSV}')\n",
    "\n",
    "    # 6) 히트맵 시각화\n",
    "    weat_cols = [c for c in df.columns if c.startswith('WEAT(')]\n",
    "    vals = df[weat_cols].values.astype(float)\n",
    "\n",
    "    maxabs = np.nanmax(np.abs(vals)) if np.isfinite(vals).any() else 1.0\n",
    "    if not np.isfinite(maxabs) or maxabs == 0:\n",
    "        maxabs = 1.0\n",
    "\n",
    "    # 라벨 언어 결정 (한글 폰트가 없으면 영어로)\n",
    "    if PLOT_LABEL_LANG == 'ko':\n",
    "        use_korean = True\n",
    "        _ = try_enable_korean_font()\n",
    "    elif PLOT_LABEL_LANG == 'en':\n",
    "        use_korean = False\n",
    "    else:  # 모두 아닐 경우 auto\n",
    "        use_korean = try_enable_korean_font()\n",
    "\n",
    "    # y축(장르)\n",
    "    ylabels = df.index.tolist() if use_korean else [GENRE_EN.get(g, g) for g in df.index.tolist()]\n",
    "\n",
    "    # x축(WEAT쌍)\n",
    "    xlabels = []\n",
    "    for c in weat_cols:\n",
    "        m = re.match(r'WEAT\\((.+) vs (.+)\\)', c)\n",
    "        if m:\n",
    "            a, b = m.group(1), m.group(2)\n",
    "            if not use_korean:\n",
    "                a = TARGET_EN.get(a, a)\n",
    "                b = TARGET_EN.get(b, b)\n",
    "            xlabels.append(f'WEAT({a} vs {b})')\n",
    "        else:\n",
    "            xlabels.append(c)\n",
    "\n",
    "    fig_w = max(6, 2 + 1.5 * len(weat_cols))\n",
    "    fig_h = max(6, len(df) * 0.35)\n",
    "    fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n",
    "    im = ax.imshow(vals, aspect='auto', cmap='coolwarm', vmin=-maxabs, vmax=+maxabs)\n",
    "\n",
    "    # 축 라벨\n",
    "    ax.set_yticks(range(len(df)))\n",
    "    ax.set_yticklabels(ylabels, fontsize=9)\n",
    "    ax.set_xticks(range(len(weat_cols)))\n",
    "    ax.set_xticklabels(xlabels, rotation=0, fontsize=10)\n",
    "\n",
    "    for i in range(vals.shape[0]):\n",
    "        for j in range(vals.shape[1]):\n",
    "            v = vals[i, j]\n",
    "            txt = 'nan' if np.isnan(v) else f'{v:.2f}'\n",
    "            ax.text(j, i, txt, ha='center', va='center', fontsize=8)\n",
    "\n",
    "    cbar = fig.colorbar(im, ax=ax, fraction=0.025, pad=0.02)\n",
    "    cbar.set_label('WEAT effect size (Cohen d, centered at 0)', rotation=90)\n",
    "\n",
    "    title_ko = '장르별 WEAT 효과크기 (타깃: 영화 구분 — 모든 쌍)'\n",
    "    title_en = 'WEAT effect sizes by genre (targets: film categories — all pairs)'\n",
    "    ax.set_title(title_ko if use_korean else title_en, fontsize=12)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(OUT_FIG, dpi=220)\n",
    "    print(f'[OUT] Saved: {OUT_FIG}')\n",
    "\n",
    "    # 7) 대표어 결과 미리보기(빠른 검시)\n",
    "    print('\\n[장르 대표어 미리보기]')\n",
    "    for g in genre_name:\n",
    "        print(f'- {g:10s}:', ', '.join(genre_reps.get(g, [])[:15]))\n",
    "    print('\\n[타깃 대표어 미리보기]')\n",
    "    for k in tgt_keys:\n",
    "        print(f'- {k}: {\", \".join(target_reps.get(k, [])[:20])}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fe31e3-5e37-4d5f-8c0f-0ddd6689526c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
