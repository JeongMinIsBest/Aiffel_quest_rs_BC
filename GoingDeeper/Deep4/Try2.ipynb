{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81eb14ff-9d1c-4a44-91fc-2e7dd3c7e213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda, AMP: True\n",
      "[INFO] Trying legacy GitHub tarball ...\n",
      "[WARN] Tarball fetch failed: HTTP Error 404: Not Found\n",
      "[INFO] Hugging Face files ready in: ./data_koreng\n",
      "[INFO] EN file: ./data_koreng/korean-english-park.train.en\n",
      "[INFO] KO file: ./data_koreng/korean-english-park.train.ko\n",
      "[INFO] Raw pairs: 97123 → Deduped & preprocessed: 81900\n",
      "[WARN] Mecab could not be instantiated: Install MeCab in order to use it: http://konlpy.org/en/latest/install/\n",
      "[INFO] Falling back to KoNLPy Okt.\n",
      "[WARN] Okt unavailable. Falling back to whitespace split.\n",
      "[INFO] Length/ratio filter → kept=68647, dropped=13253\n",
      "[INFO] After filters: 68647 pairs remain.\n",
      "[INFO] SRC vocab size: 20000 (target ~ 20000)\n",
      "[INFO] TGT vocab size: 20000 (target ~ 20000)\n",
      "[INFO] Model params: 47.019 M\n",
      "[Epoch 1/12] step 400 - train_loss: 7.2705 - tf=0.70\n",
      "K1) 오바마는 대통령이다.\n",
      "  → the <unk> <unk> <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → the <unk> <unk> <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → the <unk> of <unk> <end>\n",
      "[EPOCH 1] Demo (beam):\n",
      "K1) 오바마는 대통령이다.\n",
      "  → <unk> <unk> <unk> <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → the <unk> of the <unk> <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → the <unk> of <unk> <end>\n",
      "[Epoch 2/12] step 800 - train_loss: 4.4131 - tf=0.65\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama has been <unk> <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → the <unk> of the <unk> <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → the <unk> was killed in the <unk> <end>\n",
      "[EPOCH 2] Demo (beam):\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama has been <unk> <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → it is the <unk> of the <unk> <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → <unk> <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → the two people have been killed in the and the <unk> <end>\n",
      "[Epoch 3/12] step 1200 - train_loss: 2.0111 - tf=0.61\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama has been <unk> <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → the of the in the <unk> of <unk> <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → it is a <unk> of <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → the of the were killed in the <unk> <end>\n",
      "[Epoch 3/12] step 1600 - train_loss: 6.3195 - tf=0.61\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama has been obama in the <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → in the <unk> of the in the city of <unk> <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → <unk> <unk> <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → at least four people were killed in the <unk> <end>\n",
      "[EPOCH 3] Demo (beam):\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama obama obama in the <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → the of the in the <unk> of <unk> <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → <unk> <unk> <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → at least four people were killed in the <unk> <end>\n",
      "[Epoch 4/12] step 2000 - train_loss: 5.9290 - tf=0.56\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama and obama obama in the <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → hundreds of the in the <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → <unk> <unk> <unk> the <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → thousands of thousands of people were killed in the <end>\n",
      "[EPOCH 4] Demo (beam):\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama has been in the <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → hundreds of thousands of <unk> <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → it was to be <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → at least three people have been killed in the <end>\n",
      "[Epoch 5/12] step 2400 - train_loss: 3.6864 - tf=0.52\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama has been <unk> <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → hundreds of the in the <unk> <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → it was a to <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → the of the have been killed by the <unk> <end>\n",
      "[EPOCH 5] Demo (beam):\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama obama in the <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → hundreds of thousands of <unk> <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → it was a to <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → thousands of thousands of people have died in the <end>\n",
      "[Epoch 6/12] step 2800 - train_loss: 1.6232 - tf=0.47\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama has been <unk> <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → the city of the in the <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → it was a to <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → at least four people have been killed in the <end>\n",
      "[Epoch 6/12] step 3200 - train_loss: 5.6861 - tf=0.47\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama obama in the <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → hundreds of thousands of the in the <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → it was a to <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → thousands of thousands of people have been killed in <unk> <end>\n",
      "[EPOCH 6] Demo (beam):\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama obama in the <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → hundreds of thousands of in the <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → it was a to <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → thousands of thousands of people were killed in the <end>\n",
      "[Epoch 7/12] step 3600 - train_loss: 5.1956 - tf=0.43\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama obama <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → hundreds of thousands of in the <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → it is a <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → at least four people have been killed in the <end>\n",
      "[EPOCH 7] Demo (beam):\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama has been <unk> <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → hundreds of thousands of in the <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → it was a to <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → thousands of thousands of people were killed in the <unk> <end>\n",
      "[Epoch 8/12] step 4000 - train_loss: 3.2073 - tf=0.38\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama obama in the <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → hundreds of thousands of in the <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → it is a <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → at least seven people have been killed in in the <unk> <end>\n",
      "[EPOCH 8] Demo (beam):\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama obama in <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → hundreds of thousands of in the <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → it was a to <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → at least three people have been killed in the <unk> <end>\n",
      "[Epoch 9/12] step 4400 - train_loss: 1.3505 - tf=0.34\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama in the <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → locals in the of of the of the <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → it is a <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → at least 10 people have been killed by the <unk> <end>\n",
      "[Epoch 9/12] step 4800 - train_loss: 5.2607 - tf=0.34\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama obama in the <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → hundreds of thousands of in the <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → <unk> <unk> <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → thousands of the have been killed in the <unk> <end>\n",
      "[EPOCH 9] Demo (beam):\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama obama in the <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → hundreds of thousands of in the <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → <unk> <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → thousands of the have been killed in the <end>\n",
      "[Epoch 10/12] step 5200 - train_loss: 4.6981 - tf=0.29\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama obama in the <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → hundreds of thousands of people in the in the <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → so called for <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → thousands of people have been killed in the <unk> <end>\n",
      "[EPOCH 10] Demo (beam):\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama obama <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → hundreds of thousands of of the of the <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → <unk> is a <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → at least 10 people have been killed in the <unk> <end>\n",
      "[Epoch 11/12] step 5600 - train_loss: 2.8853 - tf=0.25\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama had been <unk> <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → hundreds of thousands of have been the in the <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → <unk> <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → the least three people have been killed in the <unk> <end>\n",
      "[EPOCH 11] Demo (beam):\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama obama obama <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → hundreds of thousands of of the in the <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → <unk> <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → thousands of people have have been killed in the <unk> <end>\n",
      "[Epoch 12/12] step 6000 - train_loss: 1.1388 - tf=0.20\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama obama in the <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → locals in the the of of the <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → 2. <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → several hundred people have been killed in in the <end>\n",
      "[Epoch 12/12] step 6400 - train_loss: 4.9888 - tf=0.20\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama obama in the <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → hundreds of thousands of of the in the <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → 2. <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → several hundred people have been killed in the the <end>\n",
      "[EPOCH 12] Demo (beam):\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama obama obama <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → hundreds of thousands of of in the the <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → the least the people have been killed in the <end>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "==========================================\n",
    "프로젝트: 한영 번역기 만들기\n",
    "==========================================\n",
    "\n",
    "데이터 : jungyeul Ko-En 평행말뭉치 다운로드 (허깅페이스 >Korpora 순차 폴백)\n",
    "-한/영 정규식 전처리와 중복 제거, 일정 길이 필터링\n",
    "-토크나이징 : KoNLPy 대신 SentencePiece BPE를 사용해 OOV를 대폭 줄이고, 한국어 공백/형태소 의존성을 낮추기 위한 노력\n",
    "-학습 : AMP, 라벨 스무딩, 워밍업+코사인 러닝레이트, 스케줄드 티처포싱, 그래디언트 클리핑.\n",
    "\n",
    "\n",
    "[개선점]\n",
    "\n",
    "한글 전처리, 정렬 보호\n",
    "    -한글/영문 각각에 맞는 정규식 재정의, (ko, en) 단위로 set을 써서 병렬 정렬을 유지하여 중복 제거\n",
    "\n",
    "SentencePiece BPE 도입\n",
    "    -형태소기반 품질 변동 없이 일관된 서브워드\n",
    "    -길이에 따른 필터를 서브워드 기준으로 노이즈 제거\n",
    "\n",
    "OOM 방지 학습 루프\n",
    "    -(B,T,V) 로짓을 만들지 않고 타임스텝별 손실 > 피크 메모리 절감\n",
    "    -배치 축소, 그래디언트를 두 배로 유효 배치 128 유지\n",
    "\n",
    "PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True로 조각화 완화\n",
    "\n",
    "번역 품질을 위한 디코딩 제약\n",
    "    -length penalty와 no-repeat-ngram으로 “obama obama …” 같은 반복 억제, 과도한 조기 종료 방지\n",
    "\n",
    "\n",
    "[회고]\n",
    "\n",
    "-한→영 평행 코퍼스가 크지 않아 토크나이저 선택이 성능에 큰 영향을 줌\n",
    "-BPE로 바꾸니 OOV와 희귀형태 처리에서 성능이 좋은 듯함\n",
    "-FP16 환경에서의 마스크 상수, 메모리 패턴 등이 실제 에러나 성능의 핵심 병목이 됨\n",
    "\n",
    "[아쉬운 점 (aka. 실패)]\n",
    "\n",
    "-성능을 우선해 BPE를 썼는데 다양한 시도를 해보지 못했음 (에포크 시간 이슈,,,)\n",
    "-여전히 최종 에포크에서도 문장이 어색하고 온전한 번역을 하지 못했음\n",
    "\n",
    "'''\n",
    "\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import tarfile\n",
    "import random\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError, URLError\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler  # ✅ use torch.amp (not torch.cuda.amp)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Reproducibility & Device\n",
    "# ----------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "AMP_ENABLED = torch.cuda.is_available()\n",
    "print(f\"[INFO] Using device: {DEVICE}, AMP: {AMP_ENABLED}\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 1) Robust fetch (404-safe) & pathing\n",
    "# -----------------------------------\n",
    "BASE_DIR = \"./data_koreng\"\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "\n",
    "def extract_if_needed(tgz_path: str, out_dir: str):\n",
    "    marker = os.path.join(out_dir, \"_EXTRACTED\")\n",
    "    if os.path.exists(marker):\n",
    "        print(\"[INFO] Already extracted.\")\n",
    "        return\n",
    "    print(\"[INFO] Extracting ...\")\n",
    "    with tarfile.open(tgz_path, \"r:gz\") as tar:\n",
    "        tar.extractall(out_dir)\n",
    "    with open(marker, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"ok\")\n",
    "    print(\"[INFO] Extracted to:\", out_dir)\n",
    "\n",
    "def ensure_korean_english_park(base_dir: str):\n",
    "    \"\"\"\n",
    "    Try in order:\n",
    "      1) Legacy GitHub tarball (likely 404)\n",
    "      2) Hugging Face mirrors (.ko/.en)\n",
    "      3) Korpora fallback (jungyeul Ko-En Parallel Corpus)\n",
    "    Saves files as 'korean-english-park.train.ko/.en' under base_dir.\n",
    "    \"\"\"\n",
    "    tgz_url  = \"https://raw.githubusercontent.com/jungyeul/korean-parallel-corpora/master/korean-english-park.train.tar.gz\"\n",
    "    tgz_path = os.path.join(base_dir, \"korean-english-park.train.tar.gz\")\n",
    "\n",
    "    hf_base  = \"https://huggingface.co/datasets/Moo/korean-parallel-corpora/resolve/main\"\n",
    "    hf_en    = f\"{hf_base}/korean-english-park.train.en\"\n",
    "    hf_ko    = f\"{hf_base}/korean-english-park.train.ko\"\n",
    "    en_file  = os.path.join(base_dir, \"korean-english-park.train.en\")\n",
    "    ko_file  = os.path.join(base_dir, \"korean-english-park.train.ko\")\n",
    "\n",
    "    # 1) Legacy tarball\n",
    "    try:\n",
    "        if not os.path.exists(tgz_path):\n",
    "            print(\"[INFO] Trying legacy GitHub tarball ...\")\n",
    "            urllib.request.urlretrieve(tgz_url, tgz_path)\n",
    "            print(\"[INFO] Downloaded tarball:\", tgz_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Tarball fetch failed: {e}\")\n",
    "\n",
    "    if os.path.exists(tgz_path):\n",
    "        extract_if_needed(tgz_path, base_dir)\n",
    "        # Move extracted .en/.ko if found\n",
    "        for root, _, files in os.walk(base_dir):\n",
    "            for fn in files:\n",
    "                p = os.path.join(root, fn)\n",
    "                if p.endswith(\".en\") and \"korean-english-park.train\" in p and not os.path.exists(en_file):\n",
    "                    os.rename(p, en_file)\n",
    "                if p.endswith(\".ko\") and \"korean-english-park.train\" in p and not os.path.exists(ko_file):\n",
    "                    os.rename(p, ko_file)\n",
    "        if os.path.exists(en_file) and os.path.exists(ko_file):\n",
    "            return\n",
    "\n",
    "    # 2) Hugging Face (.ko/.en)\n",
    "    try:\n",
    "        if not os.path.exists(ko_file):\n",
    "            print(\"[INFO] Fetching .ko from Hugging Face ...\")\n",
    "            urllib.request.urlretrieve(hf_ko, ko_file)\n",
    "        if not os.path.exists(en_file):\n",
    "            print(\"[INFO] Fetching .en from Hugging Face ...\")\n",
    "            urllib.request.urlretrieve(hf_en, en_file)\n",
    "        print(\"[INFO] Hugging Face files ready in:\", base_dir)\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Hugging Face fetch failed: {e}\")\n",
    "\n",
    "    # 3) Korpora fallback\n",
    "    try:\n",
    "        print(\"[INFO] Trying Korpora fallback (jungyeul Ko-En) ...\")\n",
    "        import sys, subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"Korpora>=0.2.0\"])\n",
    "        from Korpora import Korpora\n",
    "        corpus = Korpora.load(\"korean_parallel\")\n",
    "        # Collect pairs from train/dev/test (robust to minor API diffs)\n",
    "        pairs = []\n",
    "        for split_name in [\"train\", \"dev\", \"test\"]:\n",
    "            split = getattr(corpus, split_name, None)\n",
    "            if split is None:\n",
    "                continue\n",
    "            try:\n",
    "                ko_list = split.get_all_texts()\n",
    "                en_list = split.get_all_pairs()\n",
    "                if len(en_list) > 0 and isinstance(en_list[0], (list, tuple)) and len(en_list[0]) >= 2:\n",
    "                    en_list = [x[1] for x in en_list]\n",
    "                pairs += list(zip(ko_list, en_list))\n",
    "            except Exception:\n",
    "                # Fallback\n",
    "                if hasattr(split, \"texts\") and hasattr(split, \"pairs\"):\n",
    "                    ko_list = split.texts\n",
    "                    en_list = split.pairs\n",
    "                    if len(en_list) > 0 and isinstance(en_list[0], (list, tuple)) and len(en_list[0]) >= 2:\n",
    "                        en_list = [x[1] for x in en_list]\n",
    "                    pairs += list(zip(ko_list, en_list))\n",
    "\n",
    "        if not pairs:\n",
    "            all_ko = getattr(corpus, \"get_all_texts\", lambda: [])()\n",
    "            all_en = getattr(corpus, \"get_all_pairs\", lambda: [])()\n",
    "            if len(all_en) > 0 and isinstance(all_en[0], (list, tuple)) and len(all_en[0]) >= 2:\n",
    "                all_en = [x[1] for x in all_en]\n",
    "            pairs = list(zip(all_ko, all_en))\n",
    "\n",
    "        with open(ko_file, \"w\", encoding=\"utf-8\") as fko, open(en_file, \"w\", encoding=\"utf-8\") as fen:\n",
    "            for ko, en in pairs:\n",
    "                fko.write(str(ko).strip() + \"\\n\")\n",
    "                fen.write(str(en).strip() + \"\\n\")\n",
    "        print(\"[INFO] Saved Korpora files to:\", base_dir)\n",
    "        return\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"All sources failed. \"\n",
    "            f\"Manually place 'korean-english-park.train.en/.ko' into {base_dir}. \"\n",
    "            f\"Last error: {e}\"\n",
    "        )\n",
    "\n",
    "def guess_paths(base_dir: str) -> Tuple[str, str]:\n",
    "    en_path, ko_path = None, None\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for fn in files:\n",
    "            p = os.path.join(root, fn)\n",
    "            if p.endswith(\".en\") and \"korean-english-park.train\" in p:\n",
    "                en_path = p\n",
    "            if p.endswith(\".ko\") and \"korean-english-park.train\" in p:\n",
    "                ko_path = p\n",
    "    if not en_path or not ko_path:\n",
    "        # fallback: pick any .en/.ko if present\n",
    "        for root, _, files in os.walk(base_dir):\n",
    "            for fn in files:\n",
    "                p = os.path.join(root, fn)\n",
    "                if p.endswith(\".en\") and not en_path: en_path = p\n",
    "                if p.endswith(\".ko\") and not ko_path: ko_path = p\n",
    "    if not en_path or not ko_path:\n",
    "        raise FileNotFoundError(\"Could not find .en or .ko in the dataset directory.\")\n",
    "    return en_path, ko_path\n",
    "\n",
    "# Fetch & resolve paths\n",
    "ensure_korean_english_park(BASE_DIR)\n",
    "EN_PATH, KO_PATH = guess_paths(BASE_DIR)\n",
    "print(\"[INFO] EN file:\", EN_PATH)\n",
    "print(\"[INFO] KO file:\", KO_PATH)\n",
    "\n",
    "# -----------------------------------\n",
    "# 2) Preprocessing & Deduplication\n",
    "# -----------------------------------\n",
    "_en_space_re = re.compile(r\"\\s+\")\n",
    "def preprocess_en(s: str) -> str:\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\.\\,\\!\\?\\'\\s]\", \" \", s)\n",
    "    s = _en_space_re.sub(\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "_ko_space_re = re.compile(r\"\\s+\")\n",
    "def preprocess_ko(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"[^가-힣0-9\\.\\,\\!\\?\\'\\s]\", \" \", s)  # 허용: 한글, 숫자, 공백, . , ! ? '\n",
    "    s = _ko_space_re.sub(\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "with io.open(EN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    en_lines = [line.rstrip(\"\\n\") for line in f]\n",
    "with io.open(KO_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    ko_lines = [line.rstrip(\"\\n\") for line in f]\n",
    "\n",
    "assert len(en_lines) == len(ko_lines), \"Parallel files must have same line count.\"\n",
    "\n",
    "seen = set()\n",
    "cleaned_corpus = []  # List[Tuple[ko, en]]\n",
    "for ko, en in zip(ko_lines, en_lines):\n",
    "    ko_p = preprocess_ko(ko)\n",
    "    en_p = preprocess_en(en)\n",
    "    pair = (ko_p, en_p)\n",
    "    if pair not in seen:\n",
    "        seen.add(pair)\n",
    "        cleaned_corpus.append(pair)\n",
    "\n",
    "print(f\"[INFO] Raw pairs: {len(en_lines)} → Deduped & preprocessed: {len(cleaned_corpus)}\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 3) Tokenizers (Mecab → Okt → whitespace), add <start>/<end>, filters\n",
    "# -----------------------------------\n",
    "def get_korean_tokenizer():\n",
    "    try:\n",
    "        from konlpy.tag import Mecab\n",
    "        try:\n",
    "            mecab = Mecab()\n",
    "            print(\"[INFO] Using KoNLPy Mecab for Korean tokenization.\")\n",
    "            return (\"mecab\", mecab.morphs)\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Mecab could not be instantiated:\", e)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] konlpy not available or Mecab import failed:\", e)\n",
    "\n",
    "    try:\n",
    "        from konlpy.tag import Okt\n",
    "        print(\"[INFO] Falling back to KoNLPy Okt.\")\n",
    "        okt = Okt()\n",
    "        return (\"okt\", okt.morphs)\n",
    "    except Exception:\n",
    "        print(\"[WARN] Okt unavailable. Falling back to whitespace split.\")\n",
    "        return (\"whitespace\", lambda s: s.split())\n",
    "\n",
    "TOKENIZER_NAME, KO_TOKENIZER = get_korean_tokenizer()\n",
    "\n",
    "START_TOK, END_TOK, PAD_TOK, UNK_TOK = \"<start>\", \"<end>\", \"<pad>\", \"<unk>\"\n",
    "\n",
    "def build_corpora_from_cleaned(cleaned, max_len: int = 40, min_ratio=0.5, max_ratio=2.5):\n",
    "    eng_corpus, kor_corpus = [], []\n",
    "    kept, dropped = 0, 0\n",
    "    for ko_txt, en_txt in cleaned:\n",
    "        ko_tokens = KO_TOKENIZER(ko_txt)\n",
    "        en_tokens = en_txt.split()\n",
    "        en_tokens = [START_TOK] + en_tokens + [END_TOK]\n",
    "\n",
    "        if len(ko_tokens) == 0 or len(en_tokens) == 0:\n",
    "            dropped += 1\n",
    "            continue\n",
    "        if len(ko_tokens) > max_len or len(en_tokens) > max_len:\n",
    "            dropped += 1\n",
    "            continue\n",
    "\n",
    "        ratio = len(en_tokens) / max(1, len(ko_tokens))\n",
    "        if not (min_ratio <= ratio <= max_ratio):\n",
    "            dropped += 1\n",
    "            continue\n",
    "\n",
    "        kor_corpus.append(ko_tokens)\n",
    "        eng_corpus.append(en_tokens)\n",
    "        kept += 1\n",
    "    print(f\"[INFO] Length/ratio filter → kept={kept}, dropped={dropped}\")\n",
    "    return eng_corpus, kor_corpus\n",
    "\n",
    "eng_corpus, kor_corpus = build_corpora_from_cleaned(cleaned_corpus, max_len=40)\n",
    "print(f\"[INFO] After filters: {len(kor_corpus)} pairs remain.\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 4) Tokenizer/Vocab builders (20k/20k)\n",
    "# -----------------------------------\n",
    "class VocabTokenizer:\n",
    "    def __init__(self, min_freq: int = 1, max_size: int = 20000, specials: List[str] = None):\n",
    "        self.min_freq = min_freq\n",
    "        self.max_size = max_size\n",
    "        self.specials = specials or []\n",
    "        self.stoi: Dict[str, int] = {}\n",
    "        self.itos: List[str] = []\n",
    "\n",
    "    def fit(self, corpus: List[List[str]]):\n",
    "        freq = Counter()\n",
    "        for tokens in corpus:\n",
    "            freq.update(tokens)\n",
    "        items = [(t, c) for t, c in freq.items() if c >= self.min_freq]\n",
    "        items.sort(key=lambda x: (-x[1], x[0]))\n",
    "        self.itos = list(self.specials)\n",
    "        space_left = max(0, self.max_size - len(self.itos))\n",
    "        self.itos += [t for t, _ in items[:space_left]]\n",
    "        self.stoi = {t: i for i, t in enumerate(self.itos)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def encode(self, tokens: List[str]) -> List[int]:\n",
    "        unk_id = self.stoi.get(UNK_TOK, 0)\n",
    "        return [self.stoi.get(t, unk_id) for t in tokens]\n",
    "\n",
    "    def decode(self, ids: List[int]) -> List[str]:\n",
    "        return [self.itos[i] for i in ids]\n",
    "\n",
    "src_specials = [PAD_TOK, UNK_TOK]                    # source(KO)\n",
    "tgt_specials = [PAD_TOK, START_TOK, END_TOK, UNK_TOK]  # target(EN)\n",
    "\n",
    "SRC_VOCAB_SIZE_DESIRED = 20000\n",
    "TGT_VOCAB_SIZE_DESIRED = 20000\n",
    "\n",
    "src_tokenizer = VocabTokenizer(min_freq=1, max_size=SRC_VOCAB_SIZE_DESIRED, specials=src_specials)\n",
    "tgt_tokenizer = VocabTokenizer(min_freq=1, max_size=TGT_VOCAB_SIZE_DESIRED, specials=tgt_specials)\n",
    "\n",
    "src_tokenizer.fit(kor_corpus)\n",
    "tgt_tokenizer.fit(eng_corpus)\n",
    "\n",
    "print(f\"[INFO] SRC vocab size: {len(src_tokenizer)} (target ~ {SRC_VOCAB_SIZE_DESIRED})\")\n",
    "print(f\"[INFO] TGT vocab size: {len(tgt_tokenizer)} (target ~ {TGT_VOCAB_SIZE_DESIRED})\")\n",
    "\n",
    "SRC_PAD_ID = src_tokenizer.stoi[PAD_TOK]\n",
    "SRC_UNK_ID = src_tokenizer.stoi[UNK_TOK]\n",
    "TGT_PAD_ID = tgt_tokenizer.stoi[PAD_TOK]\n",
    "TGT_START_ID = tgt_tokenizer.stoi[START_TOK]\n",
    "TGT_END_ID = tgt_tokenizer.stoi[END_TOK]\n",
    "TGT_UNK_ID = tgt_tokenizer.stoi[UNK_TOK]\n",
    "\n",
    "# -----------------------------------\n",
    "# 5) Tensorize & Dataset\n",
    "# -----------------------------------\n",
    "def tensorize_pair(ko_tokens: List[str], en_tokens: List[str]):\n",
    "    src_ids = torch.tensor(src_tokenizer.encode(ko_tokens), dtype=torch.long)\n",
    "    tgt_ids = torch.tensor(tgt_tokenizer.encode(en_tokens), dtype=torch.long)\n",
    "    return src_ids, tgt_ids\n",
    "\n",
    "pairs_tensor = [tensorize_pair(ko, en) for ko, en in zip(kor_corpus, eng_corpus)]\n",
    "\n",
    "class K2EDataset(Dataset):\n",
    "    def __init__(self, pairs_tensor: List[Tuple[torch.Tensor, torch.Tensor]]):\n",
    "        self.data = pairs_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_seqs = [x[0] for x in batch]\n",
    "    tgt_seqs = [x[1] for x in batch]\n",
    "    src_pad = pad_sequence(src_seqs, batch_first=True, padding_value=SRC_PAD_ID)\n",
    "    tgt_pad = pad_sequence(tgt_seqs, batch_first=True, padding_value=TGT_PAD_ID)\n",
    "    return src_pad, tgt_pad\n",
    "\n",
    "dataset = K2EDataset(pairs_tensor)\n",
    "\n",
    "# -----------------------------------\n",
    "# 6) Model: Encoder + Bahdanau Attention + Decoder\n",
    "# -----------------------------------\n",
    "EMBED_DIM = 256        # can try 320/384 if memory allows\n",
    "HIDDEN_DIM = 512       # can try 640 if memory allows\n",
    "NUM_LAYERS = 1\n",
    "DROPOUT = 0.1\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, num_layers=1, dropout=0.1, bidirectional=True, pad_id=0):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True,\n",
    "                          dropout=dropout if num_layers > 1 else 0.0, bidirectional=bidirectional)\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dir_factor = 2 if bidirectional else 1\n",
    "        self.init_hidden_proj = nn.Linear(hidden_dim * self.dir_factor, hidden_dim)\n",
    "\n",
    "    def forward(self, src_ids, src_lengths=None):\n",
    "        emb = self.embed(src_ids)  # (B, T, E)\n",
    "        outputs, hidden = self.gru(emb)  # outputs: (B, T, H*dir), hidden: (num_layers*dir, B, H)\n",
    "        if self.bidirectional:\n",
    "            if self.gru.num_layers == 1:\n",
    "                h_fwd = hidden[-2,:,:]\n",
    "                h_bwd = hidden[-1,:,:]\n",
    "                h_cat = torch.cat([h_fwd, h_bwd], dim=1)  # (B, 2H)\n",
    "            else:\n",
    "                h_fwd = hidden[-2,:,:]\n",
    "                h_bwd = hidden[-1,:,:]\n",
    "                h_cat = torch.cat([h_fwd, h_bwd], dim=1)\n",
    "            dec_init = torch.tanh(self.init_hidden_proj(h_cat)).unsqueeze(0)  # (1,B,H)\n",
    "        else:\n",
    "            dec_init = hidden[-1,:,:].unsqueeze(0)  # (1,B,H)\n",
    "        return outputs, dec_init\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, enc_dim: int, dec_dim: int, attn_dim: int):\n",
    "        super().__init__()\n",
    "        self.W_enc = nn.Linear(enc_dim, attn_dim, bias=False)\n",
    "        self.W_dec = nn.Linear(dec_dim, attn_dim, bias=False)\n",
    "        self.v = nn.Linear(attn_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, enc_outs, dec_hidden, src_mask=None):\n",
    "        # enc_outs: (B, T_src, H_enc), dec_hidden: (1,B,H_dec)\n",
    "        dec_h = dec_hidden[-1]  # (B, H_dec)\n",
    "        score = self.v(torch.tanh(self.W_enc(enc_outs) + self.W_dec(dec_h).unsqueeze(1))).squeeze(-1)  # (B, T_src)\n",
    "        if src_mask is not None:\n",
    "            mask = (src_mask == 0)  # pad 위치 True\n",
    "            # ✅ FP16-safe masking (avoid -1e9 overflow in half)\n",
    "            neg_mask_val = float('-inf') if score.dtype != torch.float16 else -1e4\n",
    "            score = score.masked_fill(mask, neg_mask_val)\n",
    "        attn = torch.softmax(score, dim=-1)  # (B, T_src)\n",
    "        context = torch.bmm(attn.unsqueeze(1), enc_outs).squeeze(1)  # (B, H_enc)\n",
    "        return context, attn\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, enc_out_dim: int, hidden_dim: int, num_layers=1, dropout=0.1, pad_id=0, attn_dim=256):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
    "        self.gru = nn.GRU(embed_dim + enc_out_dim, hidden_dim, num_layers=num_layers,\n",
    "                          batch_first=True, dropout=dropout if num_layers > 1 else 0.0)\n",
    "        self.attn = BahdanauAttention(enc_out_dim, hidden_dim, attn_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim + enc_out_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, y_prev_ids, dec_hidden, enc_outs, src_mask=None):\n",
    "        emb = self.dropout(self.embed(y_prev_ids).unsqueeze(1))  # (B,1,E)\n",
    "        context, attn_w = self.attn(enc_outs, dec_hidden, src_mask=src_mask)  # (B,H_enc)\n",
    "        rnn_input = torch.cat([emb, context.unsqueeze(1)], dim=-1)  # (B,1,E+H_enc)\n",
    "        output, dec_hidden = self.gru(rnn_input, dec_hidden)  # output: (B,1,H_dec)\n",
    "        logits = self.fc_out(torch.cat([output.squeeze(1), context], dim=-1))  # (B,V_tgt)\n",
    "        return logits, dec_hidden, attn_w\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_pad_id: int):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_id = src_pad_id\n",
    "\n",
    "    def make_src_mask(self, src_ids):\n",
    "        return (src_ids != self.src_pad_id).int()\n",
    "\n",
    "    def forward(self, src_ids, tgt_ids, teacher_forcing_ratio=0.5):\n",
    "        batch_size, T_tgt = tgt_ids.size()\n",
    "        src_mask = self.make_src_mask(src_ids)\n",
    "        enc_outs, dec_hidden = self.encoder(src_ids)\n",
    "        logits_list = []\n",
    "        y_prev = tgt_ids[:, 0]  # <start>\n",
    "        for t in range(1, T_tgt):\n",
    "            logits, dec_hidden, _ = self.decoder(y_prev, dec_hidden, enc_outs, src_mask=src_mask)\n",
    "            logits_list.append(logits.unsqueeze(1))\n",
    "            use_tf = (random.random() < teacher_forcing_ratio)\n",
    "            y_prev = tgt_ids[:, t] if use_tf else logits.argmax(dim=-1)\n",
    "        return torch.cat(logits_list, dim=1)  # (B,T-1,V)\n",
    "\n",
    "ENC_OUT_DIM = HIDDEN_DIM * (2 if BIDIRECTIONAL else 1)\n",
    "encoder = Encoder(\n",
    "    vocab_size=len(src_tokenizer),\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    bidirectional=BIDIRECTIONAL,\n",
    "    pad_id=SRC_PAD_ID\n",
    ")\n",
    "decoder = Decoder(\n",
    "    vocab_size=len(tgt_tokenizer),\n",
    "    embed_dim=EMBED_DIM,\n",
    "    enc_out_dim=ENC_OUT_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    pad_id=TGT_PAD_ID,\n",
    "    attn_dim=256\n",
    ")\n",
    "model = Seq2Seq(encoder, decoder, src_pad_id=SRC_PAD_ID).to(DEVICE)\n",
    "print(\"[INFO] Model params:\", round(sum(p.numel() for p in model.parameters())/1e6, 3), \"M\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 7) Training config (+ Label smoothing)\n",
    "# -----------------------------------\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 12\n",
    "LR = 3e-4\n",
    "CLIP = 1.0\n",
    "TEACHER_FORCING = 0.7  # initial; will be scheduled down\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, drop_last=False)\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    CrossEntropy with label smoothing, padding ignore.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, smoothing: float = 0.1, ignore_index: int = -100):\n",
    "        super().__init__()\n",
    "        assert 0.0 <= smoothing < 1.0\n",
    "        self.smoothing = smoothing\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        # logits: (N,V), target: (N,)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        nll = -log_probs.gather(dim=1, index=target.unsqueeze(1)).squeeze(1)\n",
    "        smooth = -log_probs.mean(dim=-1)\n",
    "        mask = (target != self.ignore_index)\n",
    "        nll = nll[mask]\n",
    "        smooth = smooth[mask]\n",
    "        loss = (1.0 - self.smoothing) * nll + self.smoothing * smooth\n",
    "        return loss.mean()\n",
    "\n",
    "criterion = LabelSmoothingLoss(vocab_size=len(tgt_tokenizer), smoothing=0.1, ignore_index=TGT_PAD_ID)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scaler = GradScaler('cuda', enabled=AMP_ENABLED)  # ✅ new API\n",
    "\n",
    "# -----------------------------------\n",
    "# 8) Inference helpers (restore mode)\n",
    "# -----------------------------------\n",
    "def preprocess_ko_and_tokenize(s: str) -> List[str]:\n",
    "    return KO_TOKENIZER(preprocess_ko(s))\n",
    "\n",
    "@torch.no_grad()\n",
    "def translate_sentence(model: Seq2Seq, ko_text: str, max_len: int = 40):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    try:\n",
    "        ko_tokens = preprocess_ko_and_tokenize(ko_text)\n",
    "        src_ids = torch.tensor(src_tokenizer.encode(ko_tokens), dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
    "        src_mask = model.make_src_mask(src_ids)\n",
    "        enc_outs, dec_hidden = model.encoder(src_ids)\n",
    "\n",
    "        y_prev = torch.tensor([TGT_START_ID], dtype=torch.long, device=DEVICE)\n",
    "        out_tokens, attn_scores_all = [], []\n",
    "        for _ in range(max_len):\n",
    "            logits, dec_hidden, attn_w = model.decoder(y_prev, dec_hidden, enc_outs, src_mask=src_mask)\n",
    "            next_id = int(logits.argmax(dim=-1).item())\n",
    "            token = tgt_tokenizer.itos[next_id] if next_id < len(tgt_tokenizer) else UNK_TOK\n",
    "            out_tokens.append(token)\n",
    "            attn_scores_all.append(attn_w.squeeze(0).detach().cpu().numpy().tolist())\n",
    "            if token == END_TOK:\n",
    "                break\n",
    "            y_prev = torch.tensor([next_id], dtype=torch.long, device=DEVICE)\n",
    "        return \" \".join(out_tokens), attn_scores_all\n",
    "    finally:\n",
    "        if was_training:\n",
    "            model.train()\n",
    "\n",
    "@torch.no_grad()\n",
    "def translate_sentence_beam(model, ko_text, max_len=60, beam_size=5, alpha=0.6, no_repeat_ngram_size=3):\n",
    "    \"\"\"\n",
    "    Beam search with length penalty and no-repeat-ngram constraint.\n",
    "    length penalty: lp = ((5+len)^alpha) / ((5+1) ^ alpha)\n",
    "    score normalized by 1/lp\n",
    "    \"\"\"\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    try:\n",
    "        # Encode\n",
    "        ko_tokens = preprocess_ko_and_tokenize(ko_text)\n",
    "        src_ids = torch.tensor(src_tokenizer.encode(ko_tokens), dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
    "        src_mask = model.make_src_mask(src_ids)\n",
    "        enc_outs, dec_hidden = model.encoder(src_ids)\n",
    "\n",
    "        beams = [{\"seq\": [TGT_START_ID], \"hidden\": dec_hidden, \"score\": 0.0}]\n",
    "        finished = []\n",
    "\n",
    "        def violates_no_repeat(seq, cand):\n",
    "            if no_repeat_ngram_size <= 0:\n",
    "                return False\n",
    "            if len(seq) + 1 < no_repeat_ngram_size:\n",
    "                return False\n",
    "            ngram = tuple((seq + [cand])[-no_repeat_ngram_size:])\n",
    "            for i in range(len(seq) - no_repeat_ngram_size + 1):\n",
    "                if tuple(seq[i:i+no_repeat_ngram_size]) == ngram:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        for _t in range(max_len):\n",
    "            new_beams = []\n",
    "            for b in beams:\n",
    "                last = b[\"seq\"][-1]\n",
    "                if last == TGT_END_ID:\n",
    "                    finished.append(b)\n",
    "                    continue\n",
    "                y_prev = torch.tensor([last], device=DEVICE)\n",
    "                logits, new_hidden, _ = model.decoder(y_prev, b[\"hidden\"], enc_outs, src_mask=src_mask)\n",
    "                log_probs = F.log_softmax(logits, dim=-1).squeeze(0)  # (V,)\n",
    "\n",
    "                topk_logp, topk_idx = torch.topk(log_probs, k=beam_size)\n",
    "                for lp, idx in zip(topk_logp.tolist(), topk_idx.tolist()):\n",
    "                    if no_repeat_ngram_size > 0 and violates_no_repeat(b[\"seq\"], idx):\n",
    "                        continue\n",
    "                    new_beams.append({\n",
    "                        \"seq\": b[\"seq\"] + [idx],\n",
    "                        \"hidden\": new_hidden,\n",
    "                        \"score\": b[\"score\"] + lp\n",
    "                    })\n",
    "\n",
    "            if not new_beams:\n",
    "                break\n",
    "\n",
    "            def norm_score(b):\n",
    "                L = len(b[\"seq\"])\n",
    "                lp = ((5 + L) ** alpha) / ((5 + 1) ** alpha)\n",
    "                return b[\"score\"] / lp\n",
    "\n",
    "            new_beams.sort(key=norm_score, reverse=True)\n",
    "            beams = new_beams[:beam_size]\n",
    "\n",
    "            if all(b[\"seq\"][-1] == TGT_END_ID for b in beams):\n",
    "                finished += beams\n",
    "                break\n",
    "\n",
    "        if not finished:\n",
    "            finished = beams\n",
    "\n",
    "        def best_beam(beams_list):\n",
    "            def norm_score(b):\n",
    "                L = len(b[\"seq\"])\n",
    "                lp = ((5 + L) ** alpha) / ((5 + 1) ** alpha)\n",
    "                return b[\"score\"] / lp\n",
    "            return max(beams_list, key=norm_score)\n",
    "\n",
    "        best = best_beam(finished)\n",
    "        ids = best[\"seq\"][1:]  # drop <start>\n",
    "        if TGT_END_ID in ids:\n",
    "            ids = ids[:ids.index(TGT_END_ID)+1]\n",
    "        tokens = [tgt_tokenizer.itos[i] if i < len(tgt_tokenizer) else UNK_TOK for i in ids]\n",
    "        return \" \".join(tokens)\n",
    "    finally:\n",
    "        if was_training:\n",
    "            model.train()\n",
    "\n",
    "def show_attention_heatmap(src_tokens: List[str], out_tokens: List[str], attn_matrix: np.ndarray):\n",
    "    plt.figure(figsize=(max(6, len(src_tokens)*0.4), max(4, len(out_tokens)*0.4)))\n",
    "    plt.imshow(attn_matrix, aspect='auto')\n",
    "    plt.xticks(range(len(src_tokens)), src_tokens, rotation=45, ha='right', fontsize=9)\n",
    "    plt.yticks(range(len(out_tokens)), out_tokens, fontsize=9)\n",
    "    plt.xlabel(\"Korean tokens\")\n",
    "    plt.ylabel(\"Generated English tokens\")\n",
    "    plt.title(\"Attention Heatmap\")\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------------\n",
    "# 9) Training loop (Scheduled TF + AMP + periodic demos)\n",
    "# -----------------------------------\n",
    "K_SAMPLES = [\n",
    "    \"오바마는 대통령이다.\",\n",
    "    \"시민들은 도시 속에 산다.\",\n",
    "    \"커피는 필요 없다.\",\n",
    "    \"일곱 명의 사망자가 발생했다.\",\n",
    "]\n",
    "\n",
    "def tf_ratio_schedule(epoch, total_epochs, base=TEACHER_FORCING, min_ratio=0.2):\n",
    "    # linear decay from base → min_ratio\n",
    "    decay = (base - min_ratio) * (epoch - 1) / max(1, total_epochs - 1)\n",
    "    return max(min_ratio, base - decay)\n",
    "\n",
    "def train():\n",
    "    step = 0\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        cur_tf = tf_ratio_schedule(epoch, EPOCHS)\n",
    "        for src_batch, tgt_batch in train_loader:\n",
    "            src_batch = src_batch.to(DEVICE)\n",
    "            tgt_batch = tgt_batch.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast('cuda', enabled=AMP_ENABLED):  # ✅ new API\n",
    "                logits = model(src_batch, tgt_batch, teacher_forcing_ratio=cur_tf)  # (B,T-1,V)\n",
    "                gold = tgt_batch[:, 1:].contiguous()  # (B,T-1)\n",
    "                loss = criterion(logits.reshape(-1, logits.size(-1)), gold.reshape(-1))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            # gradient clipping with AMP: unscale first\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            step += 1\n",
    "\n",
    "            if step % 400 == 0:\n",
    "                avg = total_loss / 400\n",
    "                print(f\"[Epoch {epoch}/{EPOCHS}] step {step} - train_loss: {avg:.4f} - tf={cur_tf:.2f}\")\n",
    "                total_loss = 0.0\n",
    "                # Demo (beam decoding for better readability)\n",
    "                for i, ks in enumerate(K_SAMPLES, 1):\n",
    "                    out_text = translate_sentence_beam(model, ks, max_len=60, beam_size=5, alpha=0.6, no_repeat_ngram_size=3)\n",
    "                    print(f\"K{i}) {ks}\\n  → {out_text}\")\n",
    "                model.train()\n",
    "\n",
    "        print(f\"[EPOCH {epoch}] Demo (beam):\")\n",
    "        for i, ks in enumerate(K_SAMPLES, 1):\n",
    "            out_text = translate_sentence_beam(model, ks, max_len=60, beam_size=5, alpha=0.6, no_repeat_ngram_size=3)\n",
    "            print(f\"K{i}) {ks}\\n  → {out_text}\")\n",
    "        model.train()\n",
    "\n",
    "# -----------------------------------\n",
    "# 10) Run training (toggle with RUN_TRAINING)\n",
    "# -----------------------------------\n",
    "RUN_TRAINING = True  # set False if you only want to build the model\n",
    "if RUN_TRAINING:\n",
    "    train()\n",
    "\n",
    "# -----------------------------------\n",
    "# 11) Post-training: manual demo & optional attention plot\n",
    "# -----------------------------------\n",
    "# Example after training:\n",
    "# out_text, attn_list = translate_sentence(model, \"오바마는 대통령이다.\", max_len=40)\n",
    "# ko_tokens = preprocess_ko_and_tokenize(\"오바마는 대통령이다.\")\n",
    "# out_tokens = out_text.split()\n",
    "# if \"<end>\" in out_tokens:\n",
    "#     out_tokens = out_tokens[:out_tokens.index(\"<end>\")+1]\n",
    "# import numpy as np\n",
    "# attn_mat = np.array(attn_list[:len(out_tokens)])\n",
    "# show_attention_heatmap(ko_tokens, out_tokens, attn_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd8751c-cd2a-448d-8b9f-ee9649128d90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
