{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cbc2543-51cc-4f5d-b879-8021fc62a961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "[INFO] Trying legacy GitHub tarball ...\n",
      "[WARN] Tarball fetch failed: HTTP Error 404: Not Found\n",
      "[INFO] Hugging Face files ready in: /home/jovyan/work/s2s_translation/data_koreng\n",
      "[INFO] EN file: /home/jovyan/work/s2s_translation/data_koreng/korean-english-park.train.en\n",
      "[INFO] KO file: /home/jovyan/work/s2s_translation/data_koreng/korean-english-park.train.ko\n",
      "[INFO] Raw pairs: 97123 → Deduped & preprocessed: 81900\n",
      "[WARN] Mecab could not be instantiated: Install MeCab in order to use it: http://konlpy.org/en/latest/install/\n",
      "[INFO] Falling back to KoNLPy Okt.\n",
      "[WARN] Okt unavailable. Falling back to whitespace split.\n",
      "[INFO] After length filter ≤ 40: 76413 pairs remain.\n",
      "[INFO] SRC vocab size: 12000 (desired ≥ 10000)\n",
      "[INFO] TGT vocab size: 12000 (desired ≥ 10000)\n",
      "[INFO] Model params: 30.627296 M\n",
      "[Epoch 1/6] step 400 - train_loss: 6.4731\n",
      "K1) 오바마는 대통령이다.\n",
      "  → the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → the the <unk> <unk> the <unk> <end>\n",
      "[EPOCH 1] Demo:\n",
      "K1) 오바마는 대통령이다.\n",
      "  → the <unk> <unk> the <unk> of the <unk> <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → the <unk> <unk> <unk> <unk> <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → the <unk> of the <unk> <end>\n",
      "[Epoch 2/6] step 800 - train_loss: 2.9609\n",
      "K1) 오바마는 대통령이다.\n",
      "  → the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → the <unk> <unk> <unk> <unk> <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → the two people were killed in the <end>\n",
      "[EPOCH 2] Demo:\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama obama obama <unk> <unk> <unk> <unk> <unk> <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → the <unk> <unk> <unk> <unk> <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → the two people were killed in the <end>\n",
      "[Epoch 3/6] step 1200 - train_loss: 0.0823\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama obama obama to the obama and <unk> <unk> <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → the <unk> <unk> <unk> <unk> <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → the two people were killed in the <end>\n",
      "[Epoch 3/6] step 1600 - train_loss: 5.4045\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama obama obama to the obama and obama <unk> <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → the <unk> <unk> <unk> <unk> <unk> <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → the two people were killed in the <unk> <end>\n",
      "[EPOCH 3] Demo:\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama has been <unk> <unk> obama and obama to <unk> <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → the <unk> <unk> the <unk> <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → the two people were killed in the <end>\n",
      "[Epoch 4/6] step 2000 - train_loss: 2.6691\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama has been <unk> to the obama and obama <unk> <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → the <unk> <unk> <unk> <unk> <unk> <unk> <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → the <unk> <unk> <unk> <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → two people were killed in the <end>\n",
      "[EPOCH 4] Demo:\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama obama obama has been <unk> obama in the <unk> <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → the <unk> <unk> <unk> <unk> <unk> <unk> <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → the <unk> <unk> <unk> <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → two people were killed in the <unk> <end>\n",
      "[Epoch 5/6] step 2400 - train_loss: 0.1440\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama obama obama has been <unk> <unk> <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → the <unk> <unk> <unk> <unk> <unk> <unk> <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → the <unk> <unk> the <unk> of the <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → the two people were killed in the <unk> <end>\n",
      "[Epoch 5/6] step 2800 - train_loss: 4.8245\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama obama has been <unk> obama and obama in the <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → the <unk> of the <unk> <unk> <unk> <unk> <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → the <unk> of the <unk> <unk> <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → two people were killed in the <unk> <end>\n",
      "[EPOCH 5] Demo:\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama obama obama has been <unk> <unk> <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → the <unk> <unk> <unk> <unk> <unk> <unk> <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → the <unk> <unk> <unk> <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → two people were killed in the <unk> <end>\n",
      "[Epoch 6/6] step 3200 - train_loss: 2.4591\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama obama obama in the <unk> <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → the <unk> <unk> <unk> <unk> <unk> <unk> <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → the is no longer to <unk> <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → two people were killed in the <unk> <end>\n",
      "[EPOCH 6] Demo:\n",
      "K1) 오바마는 대통령이다.\n",
      "  → obama obama obama has been <unk> <end>\n",
      "K2) 시민들은 도시 속에 산다.\n",
      "  → the <unk> <unk> <unk> <unk> <unk> <unk> <end>\n",
      "K3) 커피는 필요 없다.\n",
      "  → the <unk> is <unk> <unk> <unk> <end>\n",
      "K4) 일곱 명의 사망자가 발생했다.\n",
      "  → two people were killed in the <unk> <end>\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PyTorch: Korean → English Translator (Attentional Seq2Seq)\n",
    "# - Dataset: jungyeul/korean-parallel-corpora (korean-english-park.train)\n",
    "# - Steps: Download(404 대비) → Clean/Dedup → Tokenize → Build Vocab (10k+)\n",
    "#          → Filter by token length ≤ 40 → Train → Demo (K1~K4)\n",
    "# - Korean tokenizer: KoNLPy Mecab (fallback: Okt → whitespace)\n",
    "# - English (target): lowercase + whitespace, add <start>/<end>\n",
    "# - Model: Encoder(BiGRU) + Bahdanau Attention + Decoder(GRU)\n",
    "# - No separate val set (spec)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import tarfile\n",
    "import random\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError, URLError\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Reproducibility & Device\n",
    "# ----------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] Using device: {DEVICE}\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 1) Robust fetch (404-safe) & pathing\n",
    "# -----------------------------------\n",
    "BASE_DIR = os.path.join(os.getenv(\"HOME\"), \"work/s2s_translation/data_koreng\")\n",
    "\n",
    "def extract_if_needed(tgz_path: str, out_dir: str):\n",
    "    marker = os.path.join(out_dir, \"_EXTRACTED\")\n",
    "    if os.path.exists(marker):\n",
    "        print(\"[INFO] Already extracted.\")\n",
    "        return\n",
    "    print(\"[INFO] Extracting ...\")\n",
    "    with tarfile.open(tgz_path, \"r:gz\") as tar:\n",
    "        tar.extractall(out_dir)\n",
    "    with open(marker, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"ok\")\n",
    "    print(\"[INFO] Extracted to:\", out_dir)\n",
    "\n",
    "def ensure_korean_english_park(base_dir: str):\n",
    "    \"\"\"\n",
    "    Try in order:\n",
    "      1) Legacy GitHub tarball (likely 404)\n",
    "      2) Hugging Face mirrors (.ko/.en)\n",
    "      3) Korpora fallback (jungyeul Ko-En Parallel Corpus)\n",
    "    Saves files as 'korean-english-park.train.ko/.en' under base_dir.\n",
    "    \"\"\"\n",
    "    tgz_url  = \"https://raw.githubusercontent.com/jungyeul/korean-parallel-corpora/master/korean-english-park.train.tar.gz\"\n",
    "    tgz_path = os.path.join(base_dir, \"korean-english-park.train.tar.gz\")\n",
    "\n",
    "    hf_base  = \"https://huggingface.co/datasets/Moo/korean-parallel-corpora/resolve/main\"\n",
    "    hf_en    = f\"{hf_base}/korean-english-park.train.en\"\n",
    "    hf_ko    = f\"{hf_base}/korean-english-park.train.ko\"\n",
    "    en_file  = os.path.join(base_dir, \"korean-english-park.train.en\")\n",
    "    ko_file  = os.path.join(base_dir, \"korean-english-park.train.ko\")\n",
    "\n",
    "    # 1) Legacy tarball\n",
    "    try:\n",
    "        if not os.path.exists(tgz_path):\n",
    "            print(\"[INFO] Trying legacy GitHub tarball ...\")\n",
    "            urllib.request.urlretrieve(tgz_url, tgz_path)\n",
    "            print(\"[INFO] Downloaded tarball:\", tgz_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Tarball fetch failed: {e}\")\n",
    "\n",
    "    if os.path.exists(tgz_path):\n",
    "        extract_if_needed(tgz_path, base_dir)\n",
    "        # Move extracted .en/.ko if found\n",
    "        for root, _, files in os.walk(base_dir):\n",
    "            for fn in files:\n",
    "                p = os.path.join(root, fn)\n",
    "                if p.endswith(\".en\") and \"korean-english-park.train\" in p and not os.path.exists(en_file):\n",
    "                    os.rename(p, en_file)\n",
    "                if p.endswith(\".ko\") and \"korean-english-park.train\" in p and not os.path.exists(ko_file):\n",
    "                    os.rename(p, ko_file)\n",
    "        if os.path.exists(en_file) and os.path.exists(ko_file):\n",
    "            return\n",
    "\n",
    "    # 2) Hugging Face (.ko/.en)\n",
    "    try:\n",
    "        if not os.path.exists(ko_file):\n",
    "            print(\"[INFO] Fetching .ko from Hugging Face ...\")\n",
    "            urllib.request.urlretrieve(hf_ko, ko_file)\n",
    "        if not os.path.exists(en_file):\n",
    "            print(\"[INFO] Fetching .en from Hugging Face ...\")\n",
    "            urllib.request.urlretrieve(hf_en, en_file)\n",
    "        print(\"[INFO] Hugging Face files ready in:\", base_dir)\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Hugging Face fetch failed: {e}\")\n",
    "\n",
    "    # 3) Korpora fallback\n",
    "    try:\n",
    "        print(\"[INFO] Trying Korpora fallback (jungyeul Ko-En) ...\")\n",
    "        import sys, subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"Korpora>=0.2.0\"])\n",
    "        from Korpora import Korpora\n",
    "        corpus = Korpora.load(\"korean_parallel\")\n",
    "        # Collect pairs from train/dev/test (robust to minor API diffs)\n",
    "        pairs = []\n",
    "        for split_name in [\"train\", \"dev\", \"test\"]:\n",
    "            split = getattr(corpus, split_name, None)\n",
    "            if split is None:\n",
    "                continue\n",
    "            try:\n",
    "                ko_list = split.get_all_texts()\n",
    "                en_list = split.get_all_pairs()\n",
    "                if len(en_list) > 0 and isinstance(en_list[0], (list, tuple)) and len(en_list[0]) >= 2:\n",
    "                    en_list = [x[1] for x in en_list]\n",
    "                pairs += list(zip(ko_list, en_list))\n",
    "            except Exception:\n",
    "                # Fallback\n",
    "                if hasattr(split, \"texts\") and hasattr(split, \"pairs\"):\n",
    "                    ko_list = split.texts\n",
    "                    en_list = split.pairs\n",
    "                    if len(en_list) > 0 and isinstance(en_list[0], (list, tuple)) and len(en_list[0]) >= 2:\n",
    "                        en_list = [x[1] for x in en_list]\n",
    "                    pairs += list(zip(ko_list, en_list))\n",
    "\n",
    "        if not pairs:\n",
    "            all_ko = getattr(corpus, \"get_all_texts\", lambda: [])()\n",
    "            all_en = getattr(corpus, \"get_all_pairs\", lambda: [])()\n",
    "            if len(all_en) > 0 and isinstance(all_en[0], (list, tuple)) and len(all_en[0]) >= 2:\n",
    "                all_en = [x[1] for x in all_en]\n",
    "            pairs = list(zip(all_ko, all_en))\n",
    "\n",
    "        with open(ko_file, \"w\", encoding=\"utf-8\") as fko, open(en_file, \"w\", encoding=\"utf-8\") as fen:\n",
    "            for ko, en in pairs:\n",
    "                fko.write(str(ko).strip() + \"\\n\")\n",
    "                fen.write(str(en).strip() + \"\\n\")\n",
    "        print(\"[INFO] Saved Korpora files to:\", base_dir)\n",
    "        return\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"All sources failed. \"\n",
    "            f\"Manually place 'korean-english-park.train.en/.ko' into {base_dir}. \"\n",
    "            f\"Last error: {e}\"\n",
    "        )\n",
    "\n",
    "def guess_paths(base_dir: str) -> Tuple[str, str]:\n",
    "    en_path, ko_path = None, None\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for fn in files:\n",
    "            p = os.path.join(root, fn)\n",
    "            if p.endswith(\".en\") and \"korean-english-park.train\" in p:\n",
    "                en_path = p\n",
    "            if p.endswith(\".ko\") and \"korean-english-park.train\" in p:\n",
    "                ko_path = p\n",
    "    if not en_path or not ko_path:\n",
    "        # fallback: pick any .en/.ko if present\n",
    "        for root, _, files in os.walk(base_dir):\n",
    "            for fn in files:\n",
    "                p = os.path.join(root, fn)\n",
    "                if p.endswith(\".en\") and not en_path: en_path = p\n",
    "                if p.endswith(\".ko\") and not ko_path: ko_path = p\n",
    "    if not en_path or not ko_path:\n",
    "        raise FileNotFoundError(\"Could not find .en or .ko in the dataset directory.\")\n",
    "    return en_path, ko_path\n",
    "\n",
    "# Fetch & resolve paths\n",
    "ensure_korean_english_park(BASE_DIR)\n",
    "EN_PATH, KO_PATH = guess_paths(BASE_DIR)\n",
    "print(\"[INFO] EN file:\", EN_PATH)\n",
    "print(\"[INFO] KO file:\", KO_PATH)\n",
    "\n",
    "# -----------------------------------\n",
    "# 2) Preprocessing & Deduplication\n",
    "# -----------------------------------\n",
    "_en_space_re = re.compile(r\"\\s+\")\n",
    "def preprocess_en(s: str) -> str:\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\.\\,\\!\\?\\'\\s]\", \" \", s)\n",
    "    s = _en_space_re.sub(\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "_ko_space_re = re.compile(r\"\\s+\")\n",
    "def preprocess_ko(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"[^가-힣0-9\\.\\,\\!\\?\\'\\s]\", \" \", s)  # 허용: 한글, 숫자, 공백, . , ! ? '\n",
    "    s = _ko_space_re.sub(\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "with io.open(EN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    en_lines = [line.rstrip(\"\\n\") for line in f]\n",
    "with io.open(KO_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    ko_lines = [line.rstrip(\"\\n\") for line in f]\n",
    "\n",
    "assert len(en_lines) == len(ko_lines), \"Parallel files must have same line count.\"\n",
    "\n",
    "seen = set()\n",
    "cleaned_corpus = []  # List[Tuple[ko, en]]\n",
    "for ko, en in zip(ko_lines, en_lines):\n",
    "    ko_p = preprocess_ko(ko)\n",
    "    en_p = preprocess_en(en)\n",
    "    pair = (ko_p, en_p)\n",
    "    if pair not in seen:\n",
    "        seen.add(pair)\n",
    "        cleaned_corpus.append(pair)\n",
    "\n",
    "print(f\"[INFO] Raw pairs: {len(en_lines)} → Deduped & preprocessed: {len(cleaned_corpus)}\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 3) Tokenizers (Mecab → Okt → whitespace), add <start>/<end>, filter by len≤40\n",
    "# -----------------------------------\n",
    "def get_korean_tokenizer():\n",
    "    try:\n",
    "        from konlpy.tag import Mecab\n",
    "        try:\n",
    "            mecab = Mecab()\n",
    "            print(\"[INFO] Using KoNLPy Mecab for Korean tokenization.\")\n",
    "            return (\"mecab\", mecab.morphs)\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Mecab could not be instantiated:\", e)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] konlpy not available or Mecab import failed:\", e)\n",
    "\n",
    "    try:\n",
    "        from konlpy.tag import Okt\n",
    "        print(\"[INFO] Falling back to KoNLPy Okt.\")\n",
    "        okt = Okt()\n",
    "        return (\"okt\", okt.morphs)\n",
    "    except Exception:\n",
    "        print(\"[WARN] Okt unavailable. Falling back to whitespace split.\")\n",
    "        return (\"whitespace\", lambda s: s.split())\n",
    "\n",
    "TOKENIZER_NAME, KO_TOKENIZER = get_korean_tokenizer()\n",
    "\n",
    "START_TOK, END_TOK, PAD_TOK, UNK_TOK = \"<start>\", \"<end>\", \"<pad>\", \"<unk>\"\n",
    "\n",
    "def build_corpora_from_cleaned(cleaned: List[Tuple[str, str]], max_len: int = 40):\n",
    "    eng_corpus, kor_corpus = [], []\n",
    "    for ko_txt, en_txt in cleaned:\n",
    "        ko_tokens = KO_TOKENIZER(ko_txt)\n",
    "        en_tokens = en_txt.split()\n",
    "        en_tokens = [START_TOK] + en_tokens + [END_TOK]\n",
    "        if len(ko_tokens) <= max_len and len(en_tokens) <= max_len:\n",
    "            kor_corpus.append(ko_tokens)\n",
    "            eng_corpus.append(en_tokens)\n",
    "    return eng_corpus, kor_corpus\n",
    "\n",
    "eng_corpus, kor_corpus = build_corpora_from_cleaned(cleaned_corpus, max_len=40)\n",
    "print(f\"[INFO] After length filter ≤ 40: {len(kor_corpus)} pairs remain.\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 4) Tokenizer/Vocab builders (≥10k)\n",
    "# -----------------------------------\n",
    "class VocabTokenizer:\n",
    "    def __init__(self, min_freq: int = 1, max_size: int = 12000, specials: List[str] = None):\n",
    "        self.min_freq = min_freq\n",
    "        self.max_size = max_size\n",
    "        self.specials = specials or []\n",
    "        self.stoi: Dict[str, int] = {}\n",
    "        self.itos: List[str] = []\n",
    "\n",
    "    def fit(self, corpus: List[List[str]]):\n",
    "        freq = Counter()\n",
    "        for tokens in corpus:\n",
    "            freq.update(tokens)\n",
    "        items = [(t, c) for t, c in freq.items() if c >= self.min_freq]\n",
    "        items.sort(key=lambda x: (-x[1], x[0]))\n",
    "        self.itos = list(self.specials)\n",
    "        space_left = max(0, self.max_size - len(self.itos))\n",
    "        self.itos += [t for t, _ in items[:space_left]]\n",
    "        self.stoi = {t: i for i, t in enumerate(self.itos)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def encode(self, tokens: List[str]) -> List[int]:\n",
    "        unk_id = self.stoi.get(UNK_TOK, 0)\n",
    "        return [self.stoi.get(t, unk_id) for t in tokens]\n",
    "\n",
    "    def decode(self, ids: List[int]) -> List[str]:\n",
    "        return [self.itos[i] for i in ids]\n",
    "\n",
    "src_specials = [PAD_TOK, UNK_TOK]                    # source(KO)\n",
    "tgt_specials = [PAD_TOK, START_TOK, END_TOK, UNK_TOK]  # target(EN)\n",
    "\n",
    "SRC_VOCAB_SIZE_DESIRED = 12000\n",
    "TGT_VOCAB_SIZE_DESIRED = 12000\n",
    "\n",
    "src_tokenizer = VocabTokenizer(min_freq=1, max_size=SRC_VOCAB_SIZE_DESIRED, specials=src_specials)\n",
    "tgt_tokenizer = VocabTokenizer(min_freq=1, max_size=TGT_VOCAB_SIZE_DESIRED, specials=tgt_specials)\n",
    "\n",
    "src_tokenizer.fit(kor_corpus)\n",
    "tgt_tokenizer.fit(eng_corpus)\n",
    "\n",
    "print(f\"[INFO] SRC vocab size: {len(src_tokenizer)} (desired ≥ 10000)\")\n",
    "print(f\"[INFO] TGT vocab size: {len(tgt_tokenizer)} (desired ≥ 10000)\")\n",
    "\n",
    "SRC_PAD_ID = src_tokenizer.stoi[PAD_TOK]\n",
    "SRC_UNK_ID = src_tokenizer.stoi[UNK_TOK]\n",
    "TGT_PAD_ID = tgt_tokenizer.stoi[PAD_TOK]\n",
    "TGT_START_ID = tgt_tokenizer.stoi[START_TOK]\n",
    "TGT_END_ID = tgt_tokenizer.stoi[END_TOK]\n",
    "TGT_UNK_ID = tgt_tokenizer.stoi[UNK_TOK]\n",
    "\n",
    "# -----------------------------------\n",
    "# 5) Tensorize & Dataset\n",
    "# -----------------------------------\n",
    "def tensorize_pair(ko_tokens: List[str], en_tokens: List[str]):\n",
    "    src_ids = torch.tensor(src_tokenizer.encode(ko_tokens), dtype=torch.long)\n",
    "    tgt_ids = torch.tensor(tgt_tokenizer.encode(en_tokens), dtype=torch.long)\n",
    "    return src_ids, tgt_ids\n",
    "\n",
    "pairs_tensor = [tensorize_pair(ko, en) for ko, en in zip(kor_corpus, eng_corpus)]\n",
    "\n",
    "class K2EDataset(Dataset):\n",
    "    def __init__(self, pairs_tensor: List[Tuple[torch.Tensor, torch.Tensor]]):\n",
    "        self.data = pairs_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_seqs = [x[0] for x in batch]\n",
    "    tgt_seqs = [x[1] for x in batch]\n",
    "    src_pad = pad_sequence(src_seqs, batch_first=True, padding_value=SRC_PAD_ID)\n",
    "    tgt_pad = pad_sequence(tgt_seqs, batch_first=True, padding_value=TGT_PAD_ID)\n",
    "    return src_pad, tgt_pad\n",
    "\n",
    "dataset = K2EDataset(pairs_tensor)\n",
    "\n",
    "# -----------------------------------\n",
    "# 6) Model: Encoder + Bahdanau Attention + Decoder\n",
    "# -----------------------------------\n",
    "EMBED_DIM = 256        # tune as needed\n",
    "HIDDEN_DIM = 512       # tune as needed\n",
    "NUM_LAYERS = 1\n",
    "DROPOUT = 0.1\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, num_layers=1, dropout=0.1, bidirectional=True, pad_id=0):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True,\n",
    "                          dropout=dropout if num_layers > 1 else 0.0, bidirectional=bidirectional)\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dir_factor = 2 if bidirectional else 1\n",
    "        self.init_hidden_proj = nn.Linear(hidden_dim * self.dir_factor, hidden_dim)\n",
    "\n",
    "    def forward(self, src_ids, src_lengths=None):\n",
    "        emb = self.embed(src_ids)  # (B, T, E)\n",
    "        outputs, hidden = self.gru(emb)  # outputs: (B, T, H*dir), hidden: (num_layers*dir, B, H)\n",
    "        if self.bidirectional:\n",
    "            if self.gru.num_layers == 1:\n",
    "                h_fwd = hidden[-2,:,:]\n",
    "                h_bwd = hidden[-1,:,:]\n",
    "                h_cat = torch.cat([h_fwd, h_bwd], dim=1)  # (B, 2H)\n",
    "            else:\n",
    "                h_fwd = hidden[-2,:,:]\n",
    "                h_bwd = hidden[-1,:,:]\n",
    "                h_cat = torch.cat([h_fwd, h_bwd], dim=1)\n",
    "            dec_init = torch.tanh(self.init_hidden_proj(h_cat)).unsqueeze(0)  # (1,B,H)\n",
    "        else:\n",
    "            dec_init = hidden[-1,:,:].unsqueeze(0)  # (1,B,H)\n",
    "        return outputs, dec_init\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, enc_dim: int, dec_dim: int, attn_dim: int):\n",
    "        super().__init__()\n",
    "        self.W_enc = nn.Linear(enc_dim, attn_dim, bias=False)\n",
    "        self.W_dec = nn.Linear(dec_dim, attn_dim, bias=False)\n",
    "        self.v = nn.Linear(attn_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, enc_outs, dec_hidden, src_mask=None):\n",
    "        # enc_outs: (B, T_src, H_enc), dec_hidden: (1,B,H_dec)\n",
    "        dec_h = dec_hidden[-1]  # (B, H_dec)\n",
    "        score = self.v(torch.tanh(self.W_enc(enc_outs) + self.W_dec(dec_h).unsqueeze(1))).squeeze(-1)  # (B,T_src)\n",
    "        if src_mask is not None:\n",
    "            score = score.masked_fill(src_mask == 0, -1e9)\n",
    "        attn = torch.softmax(score, dim=-1)  # (B,T_src)\n",
    "        context = torch.bmm(attn.unsqueeze(1), enc_outs).squeeze(1)  # (B,H_enc)\n",
    "        return context, attn\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, enc_out_dim: int, hidden_dim: int, num_layers=1, dropout=0.1, pad_id=0, attn_dim=256):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
    "        self.gru = nn.GRU(embed_dim + enc_out_dim, hidden_dim, num_layers=num_layers,\n",
    "                          batch_first=True, dropout=dropout if num_layers > 1 else 0.0)\n",
    "        self.attn = BahdanauAttention(enc_out_dim, hidden_dim, attn_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim + enc_out_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, y_prev_ids, dec_hidden, enc_outs, src_mask=None):\n",
    "        emb = self.dropout(self.embed(y_prev_ids).unsqueeze(1))  # (B,1,E)\n",
    "        context, attn_w = self.attn(enc_outs, dec_hidden, src_mask=src_mask)  # (B,H_enc)\n",
    "        rnn_input = torch.cat([emb, context.unsqueeze(1)], dim=-1)  # (B,1,E+H_enc)\n",
    "        output, dec_hidden = self.gru(rnn_input, dec_hidden)  # output: (B,1,H_dec)\n",
    "        logits = self.fc_out(torch.cat([output.squeeze(1), context], dim=-1))  # (B,V_tgt)\n",
    "        return logits, dec_hidden, attn_w\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_pad_id: int):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_id = src_pad_id\n",
    "\n",
    "    def make_src_mask(self, src_ids):\n",
    "        return (src_ids != self.src_pad_id).int()\n",
    "\n",
    "    def forward(self, src_ids, tgt_ids, teacher_forcing_ratio=0.5):\n",
    "        batch_size, T_tgt = tgt_ids.size()\n",
    "        src_mask = self.make_src_mask(src_ids)\n",
    "        enc_outs, dec_hidden = self.encoder(src_ids)\n",
    "        logits_list = []\n",
    "        y_prev = tgt_ids[:, 0]  # <start>\n",
    "        for t in range(1, T_tgt):\n",
    "            logits, dec_hidden, _ = self.decoder(y_prev, dec_hidden, enc_outs, src_mask=src_mask)\n",
    "            logits_list.append(logits.unsqueeze(1))\n",
    "            use_tf = (random.random() < teacher_forcing_ratio)\n",
    "            y_prev = tgt_ids[:, t] if use_tf else logits.argmax(dim=-1)\n",
    "        return torch.cat(logits_list, dim=1)  # (B,T-1,V)\n",
    "\n",
    "ENC_OUT_DIM = HIDDEN_DIM * (2 if BIDIRECTIONAL else 1)\n",
    "encoder = Encoder(\n",
    "    vocab_size=len(src_tokenizer),\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    bidirectional=BIDIRECTIONAL,\n",
    "    pad_id=SRC_PAD_ID\n",
    ")\n",
    "decoder = Decoder(\n",
    "    vocab_size=len(tgt_tokenizer),\n",
    "    embed_dim=EMBED_DIM,\n",
    "    enc_out_dim=ENC_OUT_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    pad_id=TGT_PAD_ID,\n",
    "    attn_dim=256\n",
    ")\n",
    "model = Seq2Seq(encoder, decoder, src_pad_id=SRC_PAD_ID).to(DEVICE)\n",
    "print(\"[INFO] Model params:\", sum(p.numel() for p in model.parameters())/1e6, \"M\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 7) Training config\n",
    "# -----------------------------------\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 6\n",
    "LR = 3e-4\n",
    "CLIP = 1.0\n",
    "TEACHER_FORCING = 0.6  # 약간 올려서 초반 수렴 돕기\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, drop_last=False)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TGT_PAD_ID)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "# -----------------------------------\n",
    "# 8) Inference helpers (🔧 patched to restore mode)\n",
    "# -----------------------------------\n",
    "def preprocess_ko_and_tokenize(s: str) -> List[str]:\n",
    "    return KO_TOKENIZER(preprocess_ko(s))\n",
    "\n",
    "def preprocess_en_and_split(s: str) -> List[str]:\n",
    "    return preprocess_en(s).split()\n",
    "\n",
    "@torch.no_grad()\n",
    "def translate_sentence(model: Seq2Seq, ko_text: str, max_len: int = 40):\n",
    "    # 🔧 keep & restore training/eval state\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    try:\n",
    "        ko_tokens = preprocess_ko_and_tokenize(ko_text)\n",
    "        src_ids = torch.tensor(src_tokenizer.encode(ko_tokens), dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
    "        src_mask = model.make_src_mask(src_ids)\n",
    "        enc_outs, dec_hidden = model.encoder(src_ids)\n",
    "\n",
    "        y_prev = torch.tensor([TGT_START_ID], dtype=torch.long, device=DEVICE)\n",
    "        out_tokens, attn_scores_all = [], []\n",
    "        for _ in range(max_len):\n",
    "            logits, dec_hidden, attn_w = model.decoder(y_prev, dec_hidden, enc_outs, src_mask=src_mask)\n",
    "            next_id = int(logits.argmax(dim=-1).item())\n",
    "            token = tgt_tokenizer.itos[next_id] if next_id < len(tgt_tokenizer) else UNK_TOK\n",
    "            out_tokens.append(token)\n",
    "            attn_scores_all.append(attn_w.squeeze(0).detach().cpu().numpy().tolist())\n",
    "            if token == END_TOK:\n",
    "                break\n",
    "            y_prev = torch.tensor([next_id], dtype=torch.long, device=DEVICE)\n",
    "        return \" \".join(out_tokens), attn_scores_all\n",
    "    finally:\n",
    "        if was_training:\n",
    "            model.train()\n",
    "\n",
    "def show_attention_heatmap(src_tokens: List[str], out_tokens: List[str], attn_matrix: np.ndarray):\n",
    "    plt.figure(figsize=(max(6, len(src_tokens)*0.4), max(4, len(out_tokens)*0.4)))\n",
    "    plt.imshow(attn_matrix, aspect='auto')\n",
    "    plt.xticks(range(len(src_tokens)), src_tokens, rotation=45, ha='right', fontsize=9)\n",
    "    plt.yticks(range(len(out_tokens)), out_tokens, fontsize=9)\n",
    "    plt.xlabel(\"Korean tokens\")\n",
    "    plt.ylabel(\"Generated English tokens\")\n",
    "    plt.title(\"Attention Heatmap\")\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------------\n",
    "# 9) Training loop (prints K1~K4 periodically; 🔧 demo 후 train 복귀)\n",
    "# -----------------------------------\n",
    "K_SAMPLES = [\n",
    "    \"오바마는 대통령이다.\",\n",
    "    \"시민들은 도시 속에 산다.\",\n",
    "    \"커피는 필요 없다.\",\n",
    "    \"일곱 명의 사망자가 발생했다.\",\n",
    "]\n",
    "\n",
    "def train():\n",
    "    step = 0\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for src_batch, tgt_batch in train_loader:\n",
    "            src_batch = src_batch.to(DEVICE)\n",
    "            tgt_batch = tgt_batch.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(src_batch, tgt_batch, teacher_forcing_ratio=TEACHER_FORCING)  # (B,T-1,V)\n",
    "            gold = tgt_batch[:, 1:].contiguous()  # (B,T-1)\n",
    "            loss = criterion(logits.reshape(-1, logits.size(-1)), gold.reshape(-1))\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            step += 1\n",
    "\n",
    "            if step % 400 == 0:\n",
    "                avg = total_loss / 400\n",
    "                print(f\"[Epoch {epoch}/{EPOCHS}] step {step} - train_loss: {avg:.4f}\")\n",
    "                total_loss = 0.0\n",
    "                for i, ks in enumerate(K_SAMPLES, 1):\n",
    "                    out_text, _ = translate_sentence(model, ks, max_len=40)\n",
    "                    print(f\"K{i}) {ks}\\n  → {out_text}\")\n",
    "                # 🔧 ensure back to training mode after demo\n",
    "                model.train()\n",
    "\n",
    "        print(f\"[EPOCH {epoch}] Demo:\")\n",
    "        for i, ks in enumerate(K_SAMPLES, 1):\n",
    "            out_text, _ = translate_sentence(model, ks, max_len=40)\n",
    "            print(f\"K{i}) {ks}\\n  → {out_text}\")\n",
    "        model.train()\n",
    "\n",
    "# -----------------------------------\n",
    "# 10) Run training (toggle with RUN_TRAINING)\n",
    "# -----------------------------------\n",
    "RUN_TRAINING = True  # 필요하면 False로\n",
    "if RUN_TRAINING:\n",
    "    train()\n",
    "\n",
    "# -----------------------------------\n",
    "# 11) Post-training: manual demo & optional attention plot\n",
    "# -----------------------------------\n",
    "# Example after training:\n",
    "# out_text, attn_list = translate_sentence(model, \"오바마는 대통령이다.\", max_len=40)\n",
    "# ko_tokens = preprocess_ko_and_tokenize(\"오바마는 대통령이다.\")\n",
    "# out_tokens = out_text.split()\n",
    "# if \"<end>\" in out_tokens:\n",
    "#     out_tokens = out_tokens[:out_tokens.index(\"<end>\")+1]\n",
    "# import numpy as np\n",
    "# attn_mat = np.array(attn_list[:len(out_tokens)])\n",
    "# show_attention_heatmap(ko_tokens, out_tokens, attn_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6069e4e-0172-40c1-91bc-78ade0a94a70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
