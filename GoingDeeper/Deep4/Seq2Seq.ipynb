{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07ea3556-765c-4b6d-aaff-2356bdcebb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, src):\n",
    "        print(\"입력 Shape:\", src.size())\n",
    "\n",
    "        embedded = self.embedding(src)\n",
    "        print(\"Embedding Layer를 거친 Shape:\", embedded.size())\n",
    "\n",
    "        outputs, (h_0, c_0) = self.rnn(embedded)\n",
    "        print(\"LSTM Layer의 Output Shape:\", outputs.size())\n",
    "        print(\"LSTM Layer의 Hidden State Shape:\", h_0.size())\n",
    "        print(\"LSTM Layer의 Cell State Shape:\", c_0.size())\n",
    "\n",
    "        return outputs, h_0, c_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43e2a820-24f6-4406-9c65-7e2abd2d2353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 30000\n",
      "Embedidng Size: 256\n",
      "LSTM Size: 512\n",
      "Batch Size: 1\n",
      "Sample Sequence Length: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "emb_size = 256\n",
    "lstm_size = 512\n",
    "batch_size = 1\n",
    "sample_seq_len = 3\n",
    "\n",
    "print(\"Vocab Size: {0}\".format(vocab_size))\n",
    "print(\"Embedidng Size: {0}\".format(emb_size))\n",
    "print(\"LSTM Size: {0}\".format(lstm_size))\n",
    "print(\"Batch Size: {0}\".format(batch_size))\n",
    "print(\"Sample Sequence Length: {0}\\n\".format(sample_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bcecdf9-8298-4b8b-a48a-3db9bca77a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 Shape: torch.Size([1, 3])\n",
      "Embedding Layer를 거친 Shape: torch.Size([1, 3, 256])\n",
      "LSTM Layer의 Output Shape: torch.Size([1, 3, 512])\n",
      "LSTM Layer의 Hidden State Shape: torch.Size([1, 1, 512])\n",
      "LSTM Layer의 Cell State Shape: torch.Size([1, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "encoder = Encoder(vocab_size, emb_size, lstm_size)\n",
    "sample_input = torch.randint(0, vocab_size, (batch_size, sample_seq_len))\n",
    "\n",
    "sample_output, hidden, cell = encoder(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df787a41-3cd8-4881-a087-60e0871a754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim + hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell, context):\n",
    "        print(\"입력 Shape:\", x.size())\n",
    "\n",
    "        embedded = self.embedding(x)\n",
    "        print(\"Embedding Layer를 거친 Shape:\", embedded.size())\n",
    "\n",
    "        embedded = torch.cat((embedded, context), dim=2)\n",
    "        print(\"Context Vector가 더해진 Shape:\", embedded.size())\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        print(\"LSTM Layer의 Output Shape:\", output.size())\n",
    "\n",
    "        output = self.fc(output)\n",
    "        print(\"Decoder 최종 Output Shape:\", output.size())\n",
    "\n",
    "        return output, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55140ab5-d065-4db5-a5d5-cf1aa0bca4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 30000\n",
      "Embedidng Size: 256\n",
      "LSTM Size: 512\n",
      "Batch Size: 1\n",
      "Sample Sequence Length: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab Size: {0}\".format(vocab_size))\n",
    "print(\"Embedidng Size: {0}\".format(emb_size))\n",
    "print(\"LSTM Size: {0}\".format(lstm_size))\n",
    "print(\"Batch Size: {0}\".format(batch_size))\n",
    "print(\"Sample Sequence Length: {0}\\n\".format(sample_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20674a7d-b1e4-45a2-aa6d-36fbdb0c40b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 Shape: torch.Size([1, 3])\n",
      "Embedding Layer를 거친 Shape: torch.Size([1, 3, 256])\n",
      "Context Vector가 더해진 Shape: torch.Size([1, 3, 768])\n",
      "LSTM Layer의 Output Shape: torch.Size([1, 3, 512])\n",
      "Decoder 최종 Output Shape: torch.Size([1, 3, 30000])\n"
     ]
    }
   ],
   "source": [
    "decoder_input = torch.randint(0, vocab_size, (batch_size, sample_seq_len))  # (batch_size, seq_length)\n",
    "\n",
    "decoder = Decoder(vocab_size, emb_size, lstm_size)\n",
    "\n",
    "dec_output, hidden, cell = decoder(decoder_input, hidden, cell, sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05ec0b99-ff0a-4e61-9f90-e0abb0110391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden State를 100차원으로 Mapping\n",
      "\n",
      "[ H_encoder ] Shape: torch.Size([1, 10, 512])\n",
      "[ W_encoder X H_encoder ] Shape: torch.Size([1, 10, 100])\n",
      "\n",
      "[ H_decoder ] Shape: torch.Size([1, 512])\n",
      "[ W_decoder X H_decoder ] Shape: torch.Size([1, 1, 100])\n",
      "[ Score_alignment ] Shape: torch.Size([1, 10, 1])\n",
      "\n",
      "최종 Weight:\n",
      " [[0.11019357 0.09919199 0.09694339 0.09879664 0.09815952 0.09675518\n",
      "  0.10399728 0.09687828 0.10417452 0.09490961]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W_decoder = nn.Linear(512, units)  # Decoder hidden state -> units\n",
    "        self.W_encoder = nn.Linear(512, units)  # Encoder hidden state -> units\n",
    "        self.W_combine = nn.Linear(units, 1)   # Alignment score -> scalar weight\n",
    "\n",
    "    def forward(self, H_encoder, H_decoder):\n",
    "        print(\"[ H_encoder ] Shape:\", H_encoder.shape)  # (batch, seq_len, hidden_dim)\n",
    "\n",
    "        H_encoder = self.W_encoder(H_encoder)\n",
    "        print(\"[ W_encoder X H_encoder ] Shape:\", H_encoder.shape)  # (batch, seq_len, units)\n",
    "\n",
    "        print(\"\\n[ H_decoder ] Shape:\", H_decoder.shape)  # (batch, hidden_dim)\n",
    "        H_decoder = H_decoder.unsqueeze(1)  # (batch, 1, hidden_dim)\n",
    "        H_decoder = self.W_decoder(H_decoder)  # (batch, 1, units)\n",
    "\n",
    "        print(\"[ W_decoder X H_decoder ] Shape:\", H_decoder.shape)  # (batch, 1, units)\n",
    "\n",
    "        score = self.W_combine(torch.tanh(H_decoder + H_encoder))  # (batch, seq_len, 1)\n",
    "        print(\"[ Score_alignment ] Shape:\", score.shape)\n",
    "\n",
    "        attention_weights = F.softmax(score, dim=1)  # (batch, seq_len, 1)\n",
    "        print(\"\\n최종 Weight:\\n\", attention_weights.squeeze(-1).detach().numpy())\n",
    "\n",
    "        context_vector = attention_weights * H_encoder  # (batch, seq_len, units) # decoder -> encoder\n",
    "        context_vector = torch.sum(context_vector, dim=1)  # (batch, units)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# 설정\n",
    "W_size = 100\n",
    "print(f\"Hidden State를 {W_size}차원으로 Mapping\\n\")\n",
    "\n",
    "# 모델 생성\n",
    "attention = BahdanauAttention(W_size)\n",
    "\n",
    "# 입력 데이터 (배치 크기 = 1)\n",
    "enc_state = torch.rand((1, 10, 512))  # (batch, seq_len, hidden_dim)\n",
    "dec_state = torch.rand((1, 512))  # (batch, hidden_dim)\n",
    "\n",
    "# 실행\n",
    "_ = attention(enc_state, dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93acd1a2-5b5a-49ac-986a-cedb79ee433f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ H_encoder ] Shape: torch.Size([1, 10, 512])\n",
      "[ W_encoder X H_encoder ] Shape: torch.Size([1, 10, 512])\n",
      "[ Score_alignment ] Shape: torch.Size([1, 10, 1])\n",
      "\n",
      "최종 Weight:\n",
      " [[0.00220163 0.55174404 0.00279592 0.00392605 0.00060809 0.10223878\n",
      "  0.00263917 0.0074951  0.2951521  0.03119915]]\n"
     ]
    }
   ],
   "source": [
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, units):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.W_combine = nn.Linear(units, units)  # Encoder hidden state 변환\n",
    "\n",
    "    def forward(self, H_encoder, H_decoder):\n",
    "        print(\"[ H_encoder ] Shape:\", H_encoder.shape)  # (batch, seq_len, hidden_dim)\n",
    "\n",
    "        WH = self.W_combine(H_encoder)  # (batch, seq_len, hidden_dim)\n",
    "        print(\"[ W_encoder X H_encoder ] Shape:\", WH.shape)\n",
    "\n",
    "        H_decoder = H_decoder.unsqueeze(1)  # (batch, 1, hidden_dim)\n",
    "        alignment = torch.bmm(WH, H_decoder.transpose(1, 2))  # (batch, seq_len, 1)\n",
    "        print(\"[ Score_alignment ] Shape:\", alignment.shape)\n",
    "\n",
    "        attention_weights = F.softmax(alignment, dim=1)  # (batch, seq_len, 1)\n",
    "        print(\"\\n최종 Weight:\\n\", attention_weights.squeeze(-1).detach().numpy())\n",
    "\n",
    "        attention_weights = attention_weights.squeeze(-1)  # (batch, seq_len)\n",
    "        context_vector = torch.bmm(attention_weights.unsqueeze(1), H_encoder)  # (batch, 1, hidden_dim)\n",
    "        context_vector = context_vector.squeeze(1)  # (batch, hidden_dim)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# 설정\n",
    "emb_dim = 512\n",
    "attention = LuongAttention(emb_dim)\n",
    "\n",
    "# 입력 데이터 (배치 크기 = 1)\n",
    "enc_state = torch.rand((1, 10, emb_dim))  # (batch, seq_len, hidden_dim)\n",
    "dec_state = torch.rand((1, emb_dim))  # (batch, hidden_dim)\n",
    "\n",
    "# 실행\n",
    "_ = attention(enc_state, dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017a41a1-f959-44c2-836f-09d6ff1b7d71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
